<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="description" content="description"><meta name="author" content="Jan Lühr"><title>Apache Flink Workshop</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="reveal.js-3.9.2/css/reveal.css" rel="stylesheet"><link href="reveal.js-3.9.2/plugin/title-footer/title-footer.css" rel="stylesheet"><link rel="stylesheet" href="reveal.js-3.9.2/css/theme/anderscore.css" id="theme"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css"><style>/* Stylesheet for CodeRay to match GitHub theme | MIT License | http://foundation.zurb.com */
pre.CodeRay{background:#f7f7f8}
.CodeRay .line-numbers{border-right:1px solid currentColor;opacity:.35;padding:0 .5em 0 0}
.CodeRay span.line-numbers{display:inline-block;margin-right:.75em}
.CodeRay .line-numbers strong{color:#000}
table.CodeRay{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.CodeRay td{vertical-align:top;line-height:inherit}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.code{padding:0 0 0 .75em}
.CodeRay .debug{color:#fff !important;background:#000080 !important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:#000080}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:#008080}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:#008080}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#000}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:#008080}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword {color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:#008080}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}</style><link href="reveal.js-3.9.2/lib/css/zenburn.css" rel="stylesheet"><script>document.write( '<link rel="stylesheet" href="reveal.js-3.9.2/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );</script><script>document.write('<script src="http://' + (location.host || 'localhost').split(':')[0] + ':35729/livereload.js?snipver=1"></' + 'script>')</script></head><body><div class="reveal"><div class="slides"><section id="apache_flink_workshop" data-state="no-title-footer"><h2>Apache Flink Workshop</h2><div class="paragraph center"><p>Jan Lühr</p></div>
<div class="imageblock" style=""><img src="images/anderScore/anderscore-white.jpg" alt="anderscore white" height="100"></div></section>
<section id="vorstellung" class="columns"><h2>Vorstellung</h2><div class="openblock"><div class="content"><div class="paragraph"><p>Jan Lühr (M.Sc. Computer Science)</p></div>
<div class="ulist"><ul><li><p>Fokus</p><div class="ulist"><ul><li><p>IT-Trainer</p></li><li><p>Pragmatic Architect</p></li><li><p>Software Development</p></li><li><p>Build- and Deployment Engineering</p></li><li><p>Network- and Security-Techniques</p></li><li><p>Trainings, Artikel, Vorträge</p></li><li><p>Java, JavaScript, TypeScript, Rust …</p></li></ul></div></li></ul></div></div></div>
<div class="imageblock" style=""><img src="images/anderscore/user-jluehr.png" alt="user jluehr"></div></section>
<section id="unternehmen"><h2>Unternehmen</h2><div class="paragraph"><p>
<head>
<style>
	span.r {
		color:#225c4d;
	}
	ul.bullet li::before{
  		content: "\25CF";
  		color: #225c4d;
  		display: inline-block;
  		margin-right: 20px;
	}
	ul.checkmark li::before{
  		content: "\2714";
  		color: #225c4d;
  		display: inline-block;
  		margin-right: 11px;
	}
	ul.arrow li::before{
  		content: "\2794";
  		color: #225c4d;
  		display: inline-block;
  		margin-right: 11px;
	}
</style>
</head>
<body>
<p>
	<div style="position:relative; top:280px;">
		Individuelle Anwendungsentwicklung - Java Enterprise, Web, Mobile
		<ul class="bullet" style="list-style:none;">
			<li>seit 2005 <span class="r">&#160;&#160;&#160;♦&#160;&#160;&#160;</span> in Köln <span class="r">&#160;&#160;&#160;♦&#160;&#160;&#160;</span> für alle Branchen <span class="r">&#160;&#160;&#160;♦&#160;&#160;&#160;</span> <img style="position:relative; top:17px" src="images/anderscore/goldschmiede.png" height="60"></li>
			<li>nach Aufwand & zum Festpreis</li>
		</ul>
		<div class="fragment">
			<ul class="checkmark" style="list-style:none;">
				<li>Digitalisierung / Prozesse / Integration</li>
				<li>Migration</li>
				<li>Neuentwicklung</li>
				<li>Notfall / kritische Situationen</li>
			</ul><br>
			<ul class="arrow" style="list-style:none;">
				<li>pragmatisch, zielgerichtet, zuverlässig</li>
			</ul>
		</div>
	</div>
</p>
<p class="fragment">
	<img style="overflow:hidden; position:relative; bottom:30px; left:830px;" src="images/anderscore/Bereiche.png" height="600">
</p>
<p>
	<div class="fragment">
		<div style="position:relative; bottom:270px;">
		Kompletter SW Life Cycle<br>
			<ul class="bullet" style="list-style:none;">
				<li>Projektmanagement / agile Methodik</li>
				<li>Anforderungsanalyse</li>
				<li>Architektur & SW-Design</li>
				<li>Implementierung & Testautomation</li>
				<li>Studien & Seminare</li>
			</ul>
		</div>
		<div style="position:relative; left:850px; bottom:330px; color:#225c4d; font-weight:bold">
			... und für Sie? Sprechen Sie uns an!
		</div>
	</div>
</p>
</body>
</p></div></section>
<section id="agenda"><h2>Agenda</h2><div class="olist arabic"><ol class="arabic"><li><p><a href="00-greeting.html">Begrüßung</a></p></li><li><p><a href="01-intro.html">Stateful Stream Processing: Einführung &amp; Grundlagen</a></p></li><li><p><a href="03-architektur.html">Die Architektur von Apache Flink</a></p></li><li><p><a href="04-jobs-im-detail.html">Jobs im Detail</a></p></li><li><p><a href="05-externe-systeme.html">Anbindung externer Systeme</a></p></li><li><p><a href="06-flink-ml.html">Machine Learning mit Flink ML</a></p></li><li><p><a href="07-betrieb.html">Betrieb</a></p></li></ol></div></section>
<section id="inhalt_und_ziel"><h2>Inhalt und Ziel</h2><div class="dlist"><dl><dt class="hdlist1">Workshop</dt><dd><div class="ulist"><ul><li><p>Einführung in grundlegende Flink-Konzepte</p></li><li><p>Überwindung von Einstiegshürden</p></li><li><p>Funktionen und Features werden mit Aufgaben erarbeitet</p></li></ul></div></dd><dt class="hdlist1">Zielgruppe</dt><dd><div class="ulist"><ul><li><p>Software-Entwickler (Java) und Data-Scientists, die Apache Flink für Pipeline-Processing und Predictive Analytics verwenden möchten</p></li></ul></div></dd><dt class="hdlist1">Voraussetzungen</dt><dd><div class="ulist"><ul><li><p>Gute Java Kenntnisse</p></li><li><p>Sicherheit im Umgang mit einer IDE</p></li><li><p>Erfahrung mit Maven</p></li><li><p>Grundkenntnisse in Datentransformationen und Machine Learning</p></li></ul></div></dd></dl></div></section>
<section id="zeitplan"><h2>Zeitplan</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Beginn</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">09:00 Uhr</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Kaffeepause</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">ca. 10:30 Uhr</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Mittagspause</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">12:00 bis 13:00 Uhr</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Ende</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">16:00 Uhr</p></td></tr></table></section>
<section><section id="remote_training"><h2>Remote Training</h2><div class="ulist"><ul><li><p>Video-Konferenz über Zoom</p><div class="ulist"><ul><li><p>Bildschirmfreigabe für Folien</p></li><li><p>Breakout Rooms für Übungen</p></li><li><p>Lautsprecher + Mikrofon benötigt, Kamera empfehlenswert</p></li></ul></div></li><li><p>Entwicklung: Remote Desktop Protocol (RDP)</p><div class="ulist"><ul><li><p>Praktische Übungen</p></li><li><p>Ubuntu VM</p></li><li><p>Aufschaltung über Zoom möglich</p></li><li><p>Kennwort für Benutzer <strong>student</strong>: <code>miro</code></p></li></ul></div></li><li><p>Material auf GitHub</p></li></ul></div></section><section id="vereinbarungen"><h2>Vereinbarungen</h2><div class="ulist"><ul><li><p>Pausen</p><div class="ulist"><ul><li><p>Gemeinsam zu vorgegebenen Zeiten</p></li><li><p>Individuell während der Übungen</p></li></ul></div></li><li><p>Erreichbarkeit Dozent</p><div class="ulist"><ul><li><p>Zoom (Chat, Mikrofon)</p></li><li><p>E-Mail</p></li><li><p>Handy</p></li><li><p>Kamera aus: gerade nicht anwesend bzw. ansprechbar</p></li></ul></div></li><li><p>Regeln</p><div class="ulist"><ul><li><p>Mikrofon möglichst aus (Hintergrundgeräusche)</p></li><li><p>Bei Fragen: "Hand heben" oder Chat</p></li><li><p>Wenn Übung fertig, selbst in Hauptsession zurückkehren</p></li></ul></div></li></ul></div></section></section>
<section id="material"><h2>Material</h2><div class="ulist"><ul><li><p>Flink-Dokumentation<br>
<a href="https://nightlies.apache.org/flink/flink-docs-stable/" class="bare">https://nightlies.apache.org/flink/flink-docs-stable/</a><br></p></li><li><p>"Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019</p></li></ul></div></section>
<section id="vorstellung_2" class="columns"><h2>Vorstellung</h2><div class="openblock"><div class="content"><div class="paragraph heading"><p>Jetzt sind Sie dran!</p></div>
<div class="ulist"><ul><li><p>Name</p></li><li><p>Vorwissen</p></li><li><p>Erwartungen</p></li><li><p>Themenwünsche</p></li></ul></div></div></div>
<div class="imageblock" style="float: right"><img src="images/anybody.jpg" alt="anybody"></div></section>
<section id="ihre_umgebung"><h2>Ihre Umgebung</h2><div class="ulist"><ul><li><p>OpenJDK 11</p></li><li><p>Apache Maven</p></li><li><p>IntelliJ Community Edition</p></li><li><p>Apache Flink</p></li><li><p>Git</p></li><li><p>Docker</p></li></ul></div></section>
<section id="was_ist_flink"><h2>Was ist Flink?</h2><div class="paragraph heading"><p>Was ist Flink?</p></div>
<div class="paragraph center"><p><em>"Apache Flink is an open-source, unified stream-processing and batch-processing framework developed by the Apache Software Foundation."</em></p></div>
<div class="paragraph center"><p>(aus Wikipedia <a href="https://en.wikipedia.org/wiki/Apache_Flink" class="bare">https://en.wikipedia.org/wiki/Apache_Flink</a>)</p></div>
<div class="paragraph center"><p><em>"Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.<br>
 Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale."</em></p></div>
<div class="paragraph center"><p>(aus der Flink-Dokumentation <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/" class="bare">https://nightlies.apache.org/flink/flink-docs-release-1.17/</a>)</p></div></section>
<section id="streamverarbeitung_motivation"><h2>Streamverarbeitung Motivation</h2><div class="paragraph heading center"><p>Wozu Datenstreaming?</p></div></section>
<section id="ausgangspunkt_traditionelle_batch_datenverarbeitung"><h2>Ausgangspunkt: Traditionelle Batch-Datenverarbeitung</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Extract-Transform-Load (ETL) Prozess</p><div class="ulist"><ul><li><p>Daten werden aus transaktionalen Datenbanken extrahiert</p></li><li><p>für die Verarbeitung in geeignete Formate transformiert</p></li><li><p>dann in ein Datenlager geladen</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>regelmäßiges <strong>Batching</strong></p><div class="ulist"><ul><li><p>oft große Verzögerung zwischen Generierung und Auswertung der Daten</p></li><li><p>bei einem Fehler muss der gesamte Prozess neu gestartet werden</p></li></ul></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/data_warehouse_architecture.png" alt="data warehouse architecture"></div>
<div class="paragraph small"><small><em>(Bildquelle: "Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019)</em></small></div></div></td></tr></table></section>
<section id="ansatz_streaming"><h2>Ansatz Streaming</h2><div class="ulist"><ul><li><p>Daten werden als eine kontinuierliche Folge (<strong>Stream</strong>) von Ereignissen modelliert</p></li><li><p>Daten können über Pipelines in Echtzeit transportiert, transformiert und verarbeitet werden</p></li><li><p>&#8594; Ergebnisse mit sehr geringer Verzögerung</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>erfordert <strong>komplexere Systeme</strong></p><div class="ulist"><ul><li><p>Herausforderungen im Bereich der Organisation und Kommunikation der einzelnen Prozesse</p></li><li><p>Zustandshaltung wird teilweise in die Anwendungen verlegt</p></li><li><p>Ziele wie Konsistenz, Performance und Verfügbarkeit sollten möglichst erreicht werden</p></li></ul></div></li><li><p>&#8594; Notwendigkeit für spezialisierte Frameworks wie Flink und Kafka</p></li></ul></div></section>
<section id="stateful_stream_processing"><h2>Stateful Stream Processing</h2><div class="ulist"><ul><li><p>Streaming-Anwendungen können <strong>stateless</strong> (zustandslos) oder <strong>stateful</strong> (zustandsbehaftet) sein</p></li><li><p>Stateless Anwendungen verarbeiten Datensätze unabhängig voneinander</p></li><li><p>Stateful Anwendungen können Datensätze in Abhängigkeit voneinander verarbeiten</p></li><li><p>Für komplexere Aufgaben wird <strong>State</strong> (Zustand) benötigt</p></li></ul></div></section>
<section id="3_use_cases_für_den_streaming_ansatz"><h2>3 Use Cases für den Streaming-Ansatz</h2><div class="ulist"><ul><li><p>Event-driven Application</p></li><li><p>Data Pipelines</p></li><li><p>Data Analytics</p></li></ul></div></section>
<section id="use_case_event_driven_application"><h2>Use Case: Event-driven Application</h2><div class="imageblock" style=""><img src="images/usecases-eventdrivenapps.png" alt="usecases eventdrivenapps"></div>
<div class="paragraph small"><small><em>(Bildquelle: <a href="https://flink.apache.org/use-cases/" class="bare">https://flink.apache.org/use-cases/</a>)</em></small></div>
<div class="ulist"><ul><li><p>Events lösen Berechnungen, Zustandsupdates oder externe Aktionen aus</p></li><li><p>geeignet auch für Microservice-Architektur</p></li><li><p>Beispiele: Real-time Empfehlungen, regelbasiertes Alerting, Fraud Detection</p></li></ul></div></section>
<section id="use_case_data_pipelines"><h2>Use Case: Data Pipelines</h2><div class="imageblock" style=""><img src="images/usecases-datapipelines.png" alt="usecases datapipelines"></div>
<div class="paragraph small"><small><em>(Bildquelle: <a href="https://flink.apache.org/use-cases/" class="bare">https://flink.apache.org/use-cases/</a>)</em></small></div>
<div class="ulist"><ul><li><p>Transformation von Daten</p></li><li><p>Anreicherung von Daten</p></li><li><p>Verarbeitung von vielen Daten in kurzer Zeit</p></li><li><p>Beispiele: Datensynchronisierung, einfaches Monitoring, Search Index Building (E-Commerce)</p></li></ul></div></section>
<section id="use_case_data_analytics"><h2>Use Case: Data Analytics</h2><div class="imageblock" style=""><img src="images/usecases-analytics.png" alt="usecases analytics"></div>
<div class="paragraph small"><small><em>(Bildquelle: <a href="https://flink.apache.org/use-cases/" class="bare">https://flink.apache.org/use-cases/</a>)</em></small></div>
<div class="ulist"><ul><li><p>Extraktion von Information aus Rohdaten</p></li><li><p>Kontinuierliche Datenanalyse</p></li><li><p>Beispiele: Analyse von Nutzerverhalten, Quality Monitoring</p></li></ul></div></section>
<section><section id="flink_eckdaten"><h2>Flink Eckdaten</h2><div class="paragraph heading"><p>Flink</p></div><div class="ulist"><ul><li><p>Ursprung im deutschen Forschungsprojekt 'Stratosphere: Information Management on the Cloud' (2011)</p></li><li><p>seit 2014 Apache Projekt</p></li><li><p>Open-Source, betrieben durch die Apache Flink Community</p></li><li><p>entwickelt in Java und Scala</p></li><li><p>Anwendungen für Flink können in Java, Scala, Python und SQL entwickelt werden</p></li><li><p>findet heute zahlreiche Verwendung v.a. in Cloud-Anwendungen</p><div class="ulist"><ul><li><p>für Beispiele siehe <a href="https://flink.apache.org/powered-by/" class="bare">https://flink.apache.org/powered-by/</a></p></li></ul></div></li></ul></div></section><section id="flink"><h2>Flink..</h2><div class="ulist"><ul><li><p>..beschränkt sich auf die Verarbeitung von Daten</p><div class="ulist"><ul><li><p>Für Sourcing, Transfer und Persistierung sind weitere Lösungen erforderlich (z.B. Apache Kafka; S3)</p></li></ul></div></li><li><p>..nutzt parallelisierte, stateful Streamverarbeitung</p></li><li><p>..wird in der Form eines dedicated Clusters betrieben</p></li><li><p>..ist daher besonders für Anwendungsfälle mit aufwändigen oder komplexen Datenverarbeitungsschritten geeignet</p><div class="ulist"><ul><li><p>für eine einfache Microservice-Architektur ist Apache Kafka ausreichend</p></li></ul></div></li></ul></div></section></section>
<section id="aufgabe_0_lokaler_flink_cluster_setup_1"><h2>Aufgabe 0: Lokaler Flink Cluster Setup (1)</h2><div class="olist arabic"><ol class="arabic"><li><p>Laden Sie sich Flink auf <a href="https://flink.apache.org/downloads/" class="bare">https://flink.apache.org/downloads/</a> in der aktuellen Version herunter</p></li><li><p>Extrahieren Sie das Archiv in einen Ordner</p></li><li><p>Starten Sie den Flink-Cluster (standalone), indem Sie im erstellten Ordner ./bin/start-cluster.sh ausführen</p></li><li><p>Überprüfen Sie, dass der Cluster läuft, indem Sie im Browser die URL <a href="http://localhost:8081" class="bare">http://localhost:8081</a> aufrufen</p></li><li><p>Machen Sie sich etwas mit der angezeigten Web UI vertraut</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>Sehen Sie nach, ob es bereits einen Task Manager gibt</p></li><li><p>Starten Sie einen neuen Task Manager mit ./bin/taskmanager.sh start</p></li><li><p>Verifizieren Sie, dass in der Web UI jetzt ein (weiterer) Task Manager erscheint</p></li></ol></div></li></ol></div></section>
<section id="aufgabe_0_lokaler_flink_cluster_setup_2"><h2>Aufgabe 0: Lokaler Flink Cluster Setup (2)</h2><div class="ulist"><ul><li><p>Im Verzeichnis aufgaben/aufgabe-00 der Schulung (Git Repository) finden Sie eine demo-job.jar</p><div class="olist arabic"><ol class="arabic"><li><p>Kopieren Sie diese in Ihr Flink-Verzeichnis</p></li><li><p>Lassen Sie die JAR auf dem Flink-Cluster als Job laufen, indem Sie folgenden Befehl ausführen:</p><div class="ulist"><ul><li><p>./bin/flink run demo-job.jar</p></li><li><p>Ignorieren Sie etwaige Warnungen in der Konsole mit "illegal reflective access"; diese sind harmlos</p></li></ul></div></li><li><p>Überprüfen Sie in der Konsolenausgabe und in der Web UI, dass der Job erfolgreich läuft</p></li><li><p>Überprüfen Sie, dass Flink im "log"-Verzeichnis in einer .out Datei mit dem Wort "taskexecutor" im Namen Ausgaben der folgenden Art generiert hat:</p></li></ol></div>
<div class="paragraph"><p><span class="blue-font"><em>Abhebung{automat_id=45678, person_name='Lisa C.', betrag_abgehoben=50}</em></span><br></p></div></li></ul></div>
<div class="olist arabic"><ol class="arabic" start="5"><li><p>(Optional) Stoppen Sie den Job und starten Sie ihn erneut über die Web UI</p></li></ol></div></section>
<section id="grundlagen_stream_processing"><h2>Grundlagen Stream-Processing</h2><div class="paragraph heading center"><p>Grundlagen Stream-Processing</p></div></section>
<section id="latenz_und_throughput"><h2>Latenz und Throughput</h2><div class="ulist"><ul><li><p>Um über die Performance von Anwendungen zu sprechen, betrachten wir 2 Metriken:</p><div class="ulist"><ul><li><p><strong>Latenz</strong> (latency) ist die Zeitverzögerung, die zwischen Input eines Datensatzes und Output des auf diesem Datensatz basierenden Resultats entsteht</p><div class="ulist"><ul><li><p>Beispiel im Café: Wie lange muss ein Kunde auf seine Bestellung warten?</p></li></ul></div></li><li><p><strong>Throughput</strong> ist die Anzahl der Input-Datensätze, die in einer gegebenen Zeit (z.B. 1 Sekunde) vollständig verarbeitet wurden</p><div class="ulist"><ul><li><p>Beispiel im Café: Wie viele Bestellungen wurden innerhalb eines Tages erfolgreich abgewickelt?</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="event_time_und_processing_time"><h2>Event Time und Processing Time</h2><div class="ulist"><ul><li><p>Daten sind oft mit einem Zeitpunkt assoziiert</p><div class="ulist"><ul><li><p><strong>processing time</strong>: Zeit der Verarbeitung durch den jeweiligen Prozess</p></li><li><p><strong>event time</strong>: Zeit der ursprünglichen Generierung der Daten</p><div class="ulist"><ul><li><p>muss dem Datensatz als zusätzliche Information angeheftet werden.</p></li></ul></div></li></ul></div></li><li><p>Nutzung von processing time ist einfacher zu konfigurieren und führt zu schnellerer Verarbeitung, da Prozesse nicht auf out-of-order Ereignisse warten müssen</p></li><li><p>event time ist genauer und wird benötigt, wenn</p><div class="ulist"><ul><li><p>die Zeit der Erstellung relevant für die Verarbeitung ist <strong>und</strong></p></li><li><p>die Zeit der Erstellung nicht genau genug mit der Zeit der Verarbeitung übereinstimmt</p></li></ul></div></li></ul></div></section>
<section id="event_time_und_processing_time_beispiel"><h2>Event Time und Processing Time : Beispiel</h2><div class="imageblock" style=""><img src="images/eigene/eventtime-processingtime.svg" alt="eventtime processingtime" height="700"></div>
<div class="ulist"><ul><li><p>Eine Wetterstation schickt minütlich ihre Messdaten an einen Flink-Cluster</p></li><li><p>Aufgrund von einer Störung treffen die Daten von 8:25 und 8:26 verzögert ein</p><div class="ulist"><ul><li><p>&#8594; Processing Time ist nicht durch Event Time determiniert</p></li></ul></div></li></ul></div></section>
<section id="operators_und_operator_state"><h2>Operators und Operator State</h2><div class="ulist"><ul><li><p>Eine Streaming-Anwendungen besteht aus mit einander verbundenen <strong>Operators</strong></p></li><li><p>Einzelne Operators können stateless oder stateful sein</p></li><li><p>Beispiele für stateless Operators:</p><div class="ulist"><ul><li><p>Konvertierung zwischen Datenformaten</p></li><li><p>Filterung</p></li></ul></div></li><li><p>Beispiele für stateful Operators:</p><div class="ulist"><ul><li><p>Aggregierung von Informationen innerhalb eines Zeitraums</p></li><li><p>Berechnung von statistischen Metriken</p></li></ul></div></li></ul></div></section>
<section id="arten_von_operators"><h2>Arten von Operators</h2><div class="ulist"><ul><li><p>Quelle : beschafft Input der Anwendung</p></li><li><p>Senke : erzeugt Output der Anwendung</p></li><li><p>Transformation : Synonym für stateless Operator</p></li><li><p>Rolling Aggregation :</p><div class="ulist"><ul><li><p>stateful, State ist ein einzelner Wert</p></li><li><p>ankommende Datensätze updaten diesen Wert</p><div class="ulist"><ul><li><p>der neue Wert wird jeweils als Output emittiert</p></li></ul></div></li><li><p>Zustandsupdate hängt nicht von der Reihenfolge der Datensätze ab</p></li><li><p>Beispiel : Zählen, Summieren von Werten</p></li></ul></div></li><li><p>Window Operator</p></li></ul></div></section>
<section id="window_operators"><h2>Window Operators</h2><div class="ulist"><ul><li><p>stateful</p></li><li><p>arbeiten mit <strong>Windows</strong> (Zeitfenstern)</p></li><li><p>für jedes Window exisitiert in separater Zustand</p></li><li><p>dies ermöglicht es, auch auf unbeschränkten Streams Operationen zu verwenden, die sonst nur auf beschränkten Streams Sinn ergeben</p><div class="ulist"><ul><li><p>z.B. Median von Zahlen bilden</p></li></ul></div></li><li><p>konkretes Beispiel : Anzahl der Loginversuche nach User über einen Zeitraum von 2 Minuten bestimmen</p></li></ul></div></section>
<section id="arten_von_windows"><h2>Arten von Windows</h2><div class="ulist"><ul><li><p>Zeitbasierte Windows:</p><div class="ulist"><ul><li><p><strong>Tumbling Windows</strong></p><div class="ulist"><ul><li><p>nicht überlappende Windows einer festgelegten zeitlichen Länge</p></li></ul></div></li><li><p><strong>Sliding Windows</strong></p><div class="ulist"><ul><li><p>überlappende Windows einer festgelegten zeitlichen Länge</p></li></ul></div></li></ul></div></li><li><p>Datenbasierte Windows :</p><div class="ulist"><ul><li><p><strong>Counting Windows</strong></p><div class="ulist"><ul><li><p>nicht überlappende Windows, die je eine festgelegte Anzahl von Datensätzen enthalten</p></li></ul></div></li><li><p><strong>Session Windows</strong></p><div class="ulist"><ul><li><p>nicht überlappende Fenster, deren Start und Länge dynamisch nach Aktivität des Streams festgelegt werden</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="zeitbasierte_windows"><h2>Zeitbasierte Windows</h2><div class="imageblock" style=""><img src="images/eigene/windows/tumblingwindowsfixedtime.png" alt="tumblingwindowsfixedtime" width="1500"></div>
<div class="imageblock" style=""><img src="images/eigene/windows/slidingwindows.png" alt="slidingwindows" width="1500"></div></section>
<section id="datenbasierte_windows"><h2>Datenbasierte Windows</h2><div class="imageblock" style=""><img src="images/eigene/windows/countingwindows.png" alt="countingwindows" width="1500"></div>
<div class="imageblock" style=""><img src="images/eigene/windows/sessionwindows.png" alt="sessionwindows" width="1500"></div></section>
<section id="watermarks_motivation"><h2>Watermarks (Motivation)</h2><div class="ulist"><ul><li><p>Probleme bei zeitbasierten <strong>Window Operators</strong> in Kombination mit <strong>event time</strong>:</p><div class="ulist"><ul><li><p>Reihenfolge der Datensätze im Stream muss nicht Reihenfolge der event times entsprechen</p></li><li><p>Es kann immer Datensätze geben, die verspätet erscheinen</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Frage: Wann sollte ein Window Operator ein zeitbasiertes Window für die weitere Bearbeitung schließen und die Ergebnisse weitergeben?</p><div class="ulist"><ul><li><p>In Flink wird wird hierfür ein Mechanismus namens <strong>Watermarks</strong> verwendet</p></li></ul></div></li></ul></div></section>
<section id="watermarks"><h2>Watermarks</h2><div class="ulist"><ul><li><p>Watermarks sind spezielle Datensätze, die einen Timestamp und keine Daten enthalten und zwischen die regulären Datensätze eines Streams gemischt werden</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>Ein Watermark entspricht der Information, dass ab dieser Stelle im Stream keine weiteren Datensätze mit einer Eventzeit zu erwarten sind, die früher als die im Watermark angegebene ist</strong></p><div class="ulist"><ul><li><p>Windowed Operators schließen ein Window, wenn sie dem ersten Watermark begegnen, das später als der späteste Zeitpunkt im Window ist</p></li></ul></div></li><li><p>Die Art des Umgangs mit <strong>zu späten</strong> Datensätzen enthält einen Trade-Off zwischen Latenz und Vollständigkeit</p></li></ul></div></section>
<section id="watermarks_beispiel"><h2>Watermarks (Beispiel)</h2><div class="imageblock" style=""><img src="images/eigene/watermark.png" alt="watermark"></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="paragraph"><p><strong>Beispiel: Bestimmen einer Durchschnittstemperatur im Window</strong></p></div>
<div class="ulist"><ul><li><p>Tolerierte Verspätung : 3 Minuten</p><div class="ulist"><ul><li><p>Window schließt nach Erhalt einer Watermark mit Zeit 13:00 oder später</p><div class="ulist"><ul><li><p>Diese folgt auf (manche) Datensätze mit Zeit 13:03 oder später</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="herausforderung_von_statefulness"><h2>Herausforderung von Statefulness</h2><div class="ulist"><ul><li><p><strong>State Management</strong></p><div class="ulist"><ul><li><p>Effiziente und sichere Verwaltung von State durch das System</p></li></ul></div></li><li><p><strong>State Partitioning</strong></p><div class="ulist"><ul><li><p>Wenn ein Operator parallelisiert wird, muss sein State partitioniert (aufgeteilt) oder repliziert werden</p></li></ul></div></li><li><p><strong>State Recovery</strong></p><div class="ulist"><ul><li><p>Im Fehlerfall sollte der State wiederhergestellt werden können</p></li><li><p>&#8594; Regelmäßige sichere Persistierung des States ("Checkpointing")</p></li></ul></div></li></ul></div></section>
<section id="konsistenz"><h2>Konsistenz</h2><div class="ulist"><ul><li><p>Die Ausführung einer Anwendung kann auf verschiedene Weisen fehlschlagen</p><div class="ulist"><ul><li><p>&#8594; State der Anwendung kann inkonsistent mit den Inputdaten werden</p></li><li><p>&#8594; Output der Anwendung wird verfälscht</p></li></ul></div></li><li><p>Wir möchten einen Recovery-Mechanismus, der im Fehlerfall den State auf einen konsistenten Zustand zurückversetzt</p></li><li><p>Es sollte eine Garantie geben, dass sich der Output auch im Fehlerfall nicht oder zumindest nur in einer bekannten, beschränkten
Weise, ändert</p></li><li><p>&#8594; <strong>Konsistenzgarantien</strong></p></li></ul></div></section>
<section><section id="konsistenzgarantien"><h2>Konsistenzgarantien</h2><div class="paragraph"><p>Was passiert bei einem Fehler?</p></div><div class="ulist"><ul><li><p>At least once</p><div class="ulist"><ul><li><p>Mindestens ein Mal : Risiko von Duplikaten</p></li></ul></div></li><li><p>At most once</p><div class="ulist"><ul><li><p>Maximal ein Mal: Kein Neuversuch beim Fehlschlag</p></li></ul></div></li><li><p>Exactly once</p><div class="ulist"><ul><li><p>Genau ein Mal: In Praxis schwer zu erreichen</p></li></ul></div></li><li><p>End-to-end exactly once</p><div class="ulist"><ul><li><p>Genau einmal unter Einbeziehung aller Systeme: Erfordert zusätzliche Abstimmung von Systemen, die alle exactly once unterstützen</p></li></ul></div></li></ul></div></section><section id="at_least_once_beispiel_print_server" class="center"><h2>At Least Once Beispiel: Print Server</h2><div class="imageblock" style=""><img src="images/printer.png" alt="printer" width="150px"></div>
<div class="paragraph"><p><strong>Situation: Druckfehler / Toner leer: Ausdruck zu hell!</strong></p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Fehler wird behoben</p></li><li><p>Alle Seiten in der Warteschlange werden gedruckt</p></li><li><p>… sehr viel Papier</p></li></ol></div></section><section id="at_most_once_beispiel_print_server" class="center"><h2>At Most Once Beispiel: Print Server</h2><div class="imageblock" style=""><img src="images/printer.png" alt="printer" width="150px"></div>
<div class="paragraph"><p><strong>Situation: Druckfehler / Toner leer: Ausdruck zu hell!</strong></p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Fehler wird behoben</p></li><li><p>Keine Seite in der Warteschlange wird gedruckt</p></li><li><p>… hin und her laufen.</p></li></ol></div></section><section id="exactly_once_beispiel_print_server" class="center"><h2>Exactly Once Beispiel: Print Server</h2><div class="imageblock" style=""><img src="images/printer.png" alt="printer" width="150px"></div>
<div class="paragraph"><p><strong>Situation: Druckfehler / Toner leer: Ausdruck zu hell!</strong></p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Fehler wird behoben</p></li><li><p>Genau die zu hellen Seiten werden gedruckt</p></li><li><p>Perfekt :)</p></li></ol></div></section><section id="exactly_once_beispiel_print_server_2" class="center"><h2>Exactly Once Beispiel: Print Server</h2><div class="imageblock" style=""><img src="images/printer.png" alt="printer" width="150px"></div>
<div class="paragraph"><p><strong>Woher weiß der Drucker, welche Seiten zu hell sind?</strong></p></div>
<div class="ulist"><ul><li><p>Idee: Seitennummer am Bedienfeld eingeben</p></li><li><p>Im Allgemeinen: schwer umsetzbar</p></li></ul></div></section></section>
<section id="datenflussgraphen"><h2>Datenflussgraphen</h2><div class="ulist"><ul><li><p>Jede Streaminganwendung enthält einen oder mehrere <strong>Jobs</strong>, die auf logischer Ebene durch einen Datenflussgraphen (<strong>JobGraph</strong>) beschrieben werden</p></li><li><p>Die Knoten sind die Operatoren der Anwendung</p></li><li><p>gerichtete Kanten zwischen den Knoten beschreiben die Abfolge von Verarbeitungsschritten</p><div class="ulist"><ul><li><p>Wenn es eine Kante von Knoten A zu Knoten B gibt, leitet Knoten A seinen Outputstream an Knoten B als Inputstream weiter</p></li></ul></div></li><li><p>Daten fließen von Quellen über verarbeitende Operatoren zu Senken</p></li></ul></div></section>
<section id="parallele_verarbeitung"><h2>Parallele Verarbeitung</h2><div class="ulist"><ul><li><p>Ein Job kann auf unterschiedliche Weisen parallelisiert werden:</p><div class="ulist"><ul><li><p><strong>Datenparallelität</strong></p><div class="ulist"><ul><li><p>Einzelne Operatoren können ihre ankommenden Daten parallel verarbeiten</p></li><li><p><strong>Parallelismus</strong> des Operators = Anzahl der parallelen Instanzen</p></li></ul></div></li><li><p><strong>Taskparallelität</strong></p><div class="ulist"><ul><li><p>verschiedene Operatoren können parallel arbeiten</p></li></ul></div></li></ul></div></li><li><p>Datensätze werden mit <strong>Keys</strong> versehen und nach Gruppen von Keys partitioniert den parallelen Instanzen eines Operators zugeordnet</p></li><li><p>Bestimmte Operationen erfordern ein Neuzuordnen von Keys (<strong>rebalance</strong>)</p></li></ul></div></section>
<section id="task"><h2>Task</h2><div class="ulist"><ul><li><p>kleinste ausführbare Einheit in einem Job</p></li><li><p>wird von einem einzelnen Thread ausgeführt</p></li><li><p>jede parallele Instanz eines Operators in einem Job wird genau einem Task zugeordnet</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>Verkettung von Operatoren</strong>:</p><div class="ulist"><ul><li><p>ein Task kann auch mehrere verkettete Operatorinstanzen als <strong>Subtasks</strong> hintereinander ausführen</p></li><li><p>Die Verkettung ist eher technisch begründet (Performance), als inhaltlich</p></li></ul></div></li></ul></div></section>
<section id="executiongraph"><h2>ExecutionGraph</h2><div class="ulist"><ul><li><p>Ein JobGraph ist ein <strong>logischer</strong> Graph</p></li><li><p>Um einen Job auszuführen, muss aus dem JobGraph ein <strong>physicher</strong> Graph (ExecutionGraph) erstellt werden</p><div class="ulist"><ul><li><p>Dies ist eine parallelisierte Form des JobGraphs, bei dem die Knoten physische Tasks sind</p></li><li><p>Die Knoten enthalten je eine oder mehrere verkettete Operatorinstanzen von Operatoren im JobGraph</p></li></ul></div></li><li><p>Die Tasks können dann basierend auf dem ExecutionGraph vom Framework organisiert auf der Hardware ausgeführt werden</p></li></ul></div></section>
<section id="jobgraph_beispiel"><h2>JobGraph (Beispiel)</h2><div class="paragraph"><p>&#160;<br>
&#160;<br></p></div>
<div class="imageblock" style=""><img src="images/eigene/dataflow.png" alt="dataflow" width="1500"></div></section>
<section id="executiongraph_beispiel"><h2>ExecutionGraph (Beispiel)</h2><div class="imageblock" style=""><img src="images/eigene/dataflow_parallel.png" alt="dataflow parallel" width="1500"></div>
<div class="ulist"><ul><li><p>Quelle und Verarbeitung haben einen Parallelismus von 2, Senke von 1</p></li><li><p>Nach Filterung werden Datensätze <strong>repartitioniert</strong></p></li></ul></div></section>
<section id="datentransferstrategien"><h2>Datentransferstrategien</h2><div class="ulist"><ul><li><p>Jeder Task muss seinen Output an die Tasks schicken, zu denen im ExecutionGraph direkt eine Kante führt</p></li><li><p>Kanten im JobGraph können auf unterschiedliche Arten in Kanten im ExecutionGraph übersetzt werden und Daten für den Transfer nach unterschiedlichen Strategien aufgeteilt werden:</p><div class="ulist"><ul><li><p>Forward Strategy</p></li><li><p>Broadcast Strategy</p></li><li><p>Key-Based Strategy</p></li><li><p>Random Strategy</p></li></ul></div></li></ul></div></section>
<section id="datentransferstrategien_2"><h2>Datentransferstrategien (2)</h2><div class="ulist"><ul><li><p>Gegeben sei jeweils eine Kante zwischen Operatoren im JobGraph</p></li></ul></div>
<div class="imageblock" style=""><img src="images/eigene/transfer-strategies/statJobGraphLessWhite.svg" alt="statJobGraphLessWhite" height="500"></div>
<div class="ulist"><ul><li><p>Wir nehmen vereinfacht an, dass es keine Verkettung von Subtasks gibt</p><div class="ulist"><ul><li><p>Somit werden aus jedem Operator A im JobGraph <strong>n</strong> Knoten A#1, A#2, .., A#n im ExecutionGraph, wobei <strong>n</strong> die Parallelität von A ist</p></li></ul></div></li></ul></div></section>
<section id="datentransfer_forward_strategy"><h2>Datentransfer: Forward Strategy</h2><div class="ulist"><ul><li><p>Jeder Knoten im ExecutionGraph, der zu A gehört, erhält genau eine Kante zu einem Knoten im ExecutionGraph, der zu B gehört</p></li><li><p>Alle vom ersten Knoten generierten Daten werden zum zweiten Knoten geschickt</p></li><li><p>Hierfür sollten A und B möglichst die gleiche Parallelität haben</p></li></ul></div>
<div class="imageblock" style=""><img src="images/eigene/transfer-strategies/forwardStrategy.svg" alt="forwardStrategy" height="500"></div></section>
<section id="datentransfer_broadcast_strategy"><h2>Datentransfer: Broadcast Strategy</h2><div class="ulist"><ul><li><p>Jeder Knoten im ExecutionGraph, der zu A gehört, erhält Kanten zu jedem Knoten im ExecutionGraph, der zu B gehört</p></li><li><p>Alle vom ersten Knoten generierten Daten werden zu allen damit verbundenen Knoten geschickt</p><div class="ulist"><ul><li><p>Nachteil: teuer, insbesondere kann dies zu erhöhtem Netzwerk Traffic führen</p></li></ul></div></li></ul></div>
<div class="imageblock" style=""><img src="images/eigene/transfer-strategies/broadcastStrategy.svg" alt="broadcastStrategy" height="500"></div></section>
<section id="datentransfer_key_based_strategy"><h2>Datentransfer: Key-based Strategy</h2><div class="ulist"><ul><li><p>Kanten wie bei broadcast</p></li><li><p>jeder Datensatz wird nur zu einem der verbundenen Knoten geschickt</p></li><li><p>jeder Datensatz hat einen deterministisch bestimmten Key, der entscheidet, zu welchem Knoten er geschickt wird</p></li></ul></div>
<div class="imageblock" style=""><img src="images/eigene/transfer-strategies/keybasedStrategy.svg" alt="keybasedStrategy" height="500"></div></section>
<section id="datentransfer_random_strategy"><h2>Datentransfer: Random Strategy</h2><div class="ulist"><ul><li><p>Kanten wie bei broadcast</p></li><li><p>wie bei key-based werden Daten aufgeteilt</p></li><li><p>die Aufteilung geschieht aber zufällig</p></li></ul></div>
<div class="imageblock" style=""><img src="images/eigene/transfer-strategies/randomStrategy.svg" alt="randomStrategy" height="500"></div></section>
<section id="features_von_flink"><h2>Features von Flink</h2><div class="ulist"><ul><li><p>starke Unterstützung für Event Time Processing</p></li><li><p>Exactly-Once Konsistenzgarantien</p></li><li><p>Latenzen im Millisekundenbereich bei einem Throughput von Millionen von Daten pro Sekunde</p></li><li><p>Skalierbarkeit auf tausende Prozessorkerne</p></li><li><p>Geschichtete APIs (Flexibilität oder Einfache Benutzbarkeit)</p></li><li><p>Kann mit allen verbreiteten Speichersystemen arbeiten</p></li><li><p>High-Availability Betrieb mit sehr geringen Downtimes möglich</p></li><li><p>Updates und Migrationen ohne Verlust von State</p></li><li><p>Zahlreiche eingebaute Metriken, selbst definierbare Metriken</p></li><li><p>Auch für Batch-Processing einsetzbar</p></li></ul></div></section>
<section id="aufgabe_1"><h2>Aufgabe 1</h2><div class="ulist"><ul><li><p>In einem System zur Betrugserkennung sollen verdächtige Abhebungen an Geldautomaten erkannt und unterbunden werden.</p></li><li><p>Falls innerhalb einer kurzen Zeit (10 Minuten) eine Abhebung an 2 Automaten erfolgt, die mehr als 100km auseinander liegen, liegt ein
Betrugsfall vor.</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Zeichnen Sie einen logischen Datenflussgraphen</p></li><li><p>Zeichnen Sie ein physisches Äquivalent</p><div class="ulist"><ul><li><p>Aus Performancegründen gibt es 3 parallele Verarbeitungsstränge.</p></li></ul></div></li><li><p>Nach welcher Zeit werden Überweisungen gruppiert? Welche Art von Fenster wird verwendet?</p></li><li><p>Welche Resultgarantie wird benötigt?</p></li></ol></div></section>
<section id="die_architektur_von_apache_flink"><h2>Die Architektur von Apache Flink</h2><div class="paragraph heading center"><p>Die Architektur von Apache Flink</p></div></section>
<section id="komponenten_eines_flink_clusters"><h2>Komponenten eines Flink-Clusters</h2><div class="ulist"><ul><li><p>Es gibt 2 Hauptkomponenten im Cluster: <strong>JobManager</strong> und <strong>TaskManager</strong></p></li><li><p>Nur 1 JobManager pro Cluster wird benötigt</p></li><li><p>Es kann beliebig viele TaskManager im Cluster geben</p></li><li><p>Ein JobManager hat folgende Komponenten:</p><div class="ulist"><ul><li><p>1 <strong>ResourceManager</strong></p></li><li><p>1 <strong>Dispatcher</strong></p></li><li><p>0 oder mehr <strong>JobMaster</strong></p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>JobMaster</strong></p><div class="ulist"><ul><li><p>Kontrolliert die Ausführung eines einzelnen Jobs</p></li><li><p>Jedem laufenden Job ist genau ein JobMaster zugeordnet</p></li></ul></div></li></ul></div></section>
<section id="resourcemanager_und_dispatcher"><h2>ResourceManager und Dispatcher</h2><div class="ulist"><ul><li><p><strong>ResourceManager</strong></p><div class="ulist"><ul><li><p>Zuständig für (De-)Allokierung und Provisioning von Ressourcen für alle Jobs</p></li><li><p>Verwaltet TaskManagers, kann neue erstellen oder vorhandene beenden</p><div class="ulist"><ul><li><p>Für flexibles provisioning und recovery &#8594; availability</p></li><li><p>Dies ist nur in einem managed cluster möglich, nicht im standalone Cluster</p></li></ul></div></li><li><p>Unterschiedliche Implementierung je nach Umgebung: standalone, Kubernetes, YARN etc.</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>Dispatcher</strong></p><div class="ulist"><ul><li><p>Nimmt Anfragen entgegen, neue Jobs zu starten</p></li><li><p>Ist über ein REST-Interface von außerhalb des Clusters aus ansprechbar</p></li><li><p>Verwaltet das Web-Interface (Dashboard)</p></li></ul></div></li></ul></div></section>
<section id="taskmanager"><h2>TaskManager</h2><div class="ulist"><ul><li><p>Führt Tasks aus und leitet Daten (Streams) zwischen den Tasks weiter</p></li><li><p>Wird vom ResourceManager angesprochen, um für einen JobMaster Tasks auszuführen</p></li><li><p>Alle Tasks im TaskManager werden im selben JVM-Prozess ausgeführt</p></li><li><p>Hat eine begrenzte Anzahl von <strong>Task Slots</strong> (wird bei Erstellung des TaskManagers festgelegt)</p></li><li><p>Muss in standalone Clustern manuell erstellt werden</p></li></ul></div></section>
<section id="komponenten_diagramm"><h2>Komponenten (Diagramm)</h2><div class="imageblock" style=""><img src="images/eigene/komponenten.png" alt="komponenten"></div></section>
<section id="ausführen_eines_jobs_mit_mehreren_taskmanagern"><h2>Ausführen eines Jobs mit mehreren TaskManagern</h2><div class="imageblock" style=""><img src="images/eigene/job-execution/jobmehreretaskmanager.svg" alt="jobmehreretaskmanager" height="100%"></div></section>
<section id="high_availability_mode"><h2>High-Availability Mode</h2><div class="ulist"><ul><li><p>Ein Flink-Cluster kann bei Bedarf im <strong>high-availability mode</strong> gestartet werden</p></li><li><p>Hauptidee: mehrere JobManager starten</p><div class="ulist"><ul><li><p>Es gibt dann einen Leader, der alleine alle Aufgaben erledigt</p></li><li><p>Der Rest ist in Standby</p></li><li><p>Bei Crash/Failure des Leaders wird ein neuer Leader unter den verbleibenden JobManagern gewählt</p></li></ul></div></li><li><p>Es muss dafür ein <strong>High-Availability-Service</strong> angegeben sein</p><div class="ulist"><ul><li><p><strong>Zookeeper Quorum</strong> ist eine mit Flink mitgelieferte Möglichkeit</p></li><li><p>Auch über den managed <strong>YARN Cluster</strong> oder <strong>Kubernetes Cluster</strong> möglich</p></li></ul></div></li><li><p><strong>Managed</strong> Cluster können auch TaskManager automatisch (neu-)starten</p></li></ul></div></section>
<section id="task_slots"><h2>Task Slots</h2><div class="ulist"><ul><li><p>Unterteilungseinheit für die Ressourcen eines TaskManagers</p><div class="ulist"><ul><li><p>Jeder Slot in einem TaskManager erhält den gleichen Anteil von seinem Speicher</p></li><li><p>Es gibt keine CPU-Isolierung zwischen den Slots eines TaskManagers</p></li></ul></div></li><li><p>Jeder aktive Task wird genau einem Slot zugeordnet</p></li><li><p>Tasks im gleichen Slot sind nur schwach voneinander isoliert</p></li><li><p>Zur Isolierung der Tasks kann die Anzahl der Slots eines TaskManagers auf 1 gesetzt werden (Tradeoff mit Performance)</p></li></ul></div></section>
<section id="slot_sharing"><h2>Slot Sharing</h2><div class="ulist"><ul><li><p>Konfigurierbar, aktiviert per default</p></li><li><p>Wenn deaktiviert, dann kann ein Slot nur einen Task gleichzeitig enthalten</p></li><li><p>Wenn aktiviert, dann können dem gleichen Slot mehrere Tasks zugeordnet werden (<strong>task group</strong>)</p><div class="ulist"><ul><li><p>Diese werden in einem Threadpool organisiert und können parallel ausgeführt werden</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Regeln für die Zuordnung von Tasks zu Slots:</p><div class="ulist"><ul><li><p>Tasks im gleichen Slot müssen zum gleichen Job gehören</p></li><li><p>Ein Slot darf nicht verschiedene parallele Instanzen des gleichen Operators enthalten</p></li></ul></div></li><li><p>&#8594; Somit benötigt ein Job zur Ausführung mindestens so viele Task Slots wie der maximale Parallelismus seiner Operatoren ( = Parallelismus des Jobs )</p></li></ul></div></section>
<section id="slot_sharing_2"><h2>Slot Sharing (2)</h2><div class="ulist"><ul><li><p>Wenn Slot Sharing aktiviert ist, dann ist in der Regel die Anzahl der Slots für einen Job sogar gleich seinem Parallelismus</p><div class="ulist"><ul><li><p>Die Slots enthalten dann jeweils einen kompletten parallelen Slice des Jobs</p></li><li><p>Somit leben Instanzen einer einzelnen parallelen Pipeline des Jobs immer auf der gleichen Node</p><div class="ulist"><ul><li><p>&#8594; Es müssen weniger Daten über das Netzwerk übertragen werden</p></li></ul></div></li></ul></div></li><li><p>Ohne Slot Sharing gibt es den Nachteil, dass alle Tasks die gleiche Menge an Ressourcen erhalten, auch wenn sie unterschiedlich viel benötigen</p><div class="ulist"><ul><li><p>Somit erhalten ressourcenintensive Tasks weniger Ressourcen</p></li></ul></div></li><li><p>Ohne Slot Sharing sind Tasks voneinander stärker isoliert</p><div class="ulist"><ul><li><p>Nur 1 Slot pro TaskManager ohne Slot Sharing garantiert nur 1 Task pro TaskManager</p></li></ul></div></li></ul></div></section>
<section id="ausführen_einer_flinkanwendung"><h2>Ausführen einer Flinkanwendung</h2><div class="ulist"><ul><li><p>Flinkanwendungen können während ihrer Ausführung einen oder mehrere Jobs zur Ausführung an einen Flink-Cluster übergeben</p></li><li><p>Vor Ausführung des Jobs ordnet der JobMaster dem JobGraph einen ExecutionGraph zu</p></li><li><p>Die Tasks im ExecutionGraph können dann geeignet auf verfügbare Task Slots verteilt werden</p></li><li><p>Wenn nicht genügend Slots zur Verfügung stehen, dann schlägt die Ausführung des Jobs fehl</p><div class="ulist"><ul><li><p>Je nach Konfiguration kann der ResourceManager auch automatisch neue Slots provisionieren, indem neue TaskManager gestartet werden</p></li></ul></div></li></ul></div></section>
<section id="beispiel_ausführen_eines_jobs_jobgraph_und_executiongraph"><h2>Beispiel Ausführen eines Jobs : JobGraph und ExecutionGraph</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:28.5714%"><col style="width:71.4286%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p><strong>Beispiel Job:</strong></p></div>
<div class="ulist"><ul><li><p>2 Quellen <strong>Q1</strong> und <strong>Q2</strong></p></li><li><p>Ein Operator <strong>Op</strong></p></li><li><p>Eine Senke <strong>S</strong></p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/eigene/job-execution/jobGraphExecutionGraph.png" alt="jobGraphExecutionGraph"></div></div></td></tr></table></section>
<section id="beispiel_ausführen_eines_jobs_zuordnung_auf_task_slots"><h2>Beispiel Ausführen eines Jobs : Zuordnung auf Task Slots</h2><div class="imageblock" style=""><img src="images/eigene/job-execution/jobtaskmananager.svg" alt="jobtaskmananager" height="1100"></div></section>
<section id="deployment_modes"><h2>Deployment Modes</h2><div class="ulist"><ul><li><p>Es gibt 2 mögliche <strong>Deployment Modes</strong> für Flink-Jobs:</p><div class="ulist"><ul><li><p><strong>Session Mode</strong></p><div class="ulist"><ul><li><p>Verwendet bereits existierenden Cluster zur Ausführung des Jobs</p></li><li><p>Anwendung wird im Client ausgeführt, um einen JobGraph zu erstellen, der dann an den Flink-Cluster geschickt wird</p></li><li><p>Ggf. werden vorher noch Dependencies im Client heruntergeladen</p></li></ul></div></li><li><p><strong>Application Mode</strong></p><div class="ulist"><ul><li><p>Erstellt für jede Anwendung einen neuen Flink Cluster</p></li><li><p>Anwendung wird vom JobManager serverseitig ausgeführt</p></li><li><p>Dependencies der Anwendung sind Teil des Clusters und müssen bei Neustart der Jobs nicht neu verteilt werden</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datentransfer_zwischen_subtasks"><h2>Datentransfer zwischen Subtasks</h2><div class="ulist"><ul><li><p>Gegeben ein Subtask <em>A</em> eines Jobs, der seinen Output an einen Subtask <em>B</em> weiterleitet</p><div class="ulist"><ul><li><p>Dann hat <em>A</em> für diesen Kanal eine <strong>Output Buffer Queue</strong> und <em>B</em> eine <strong>Input Buffer Queue</strong></p></li><li><p>Wenn <em>A</em> und <em>B</em> vom gleichen TaskManager verwaltet werden, dann liegen die Buffer im gemeinsam genutzten Speicher</p><div class="ulist"><ul><li><p>Andernfalls werden vom TaskManager verwaltete Netzwerkbuffer verwendet</p></li></ul></div></li><li><p>Je 2 TaskManager erhalten eine ständige TCP-Verbindung, jede Remoteverbindung zwischen ihren <strong>Tasks</strong> erhält einen TCP-Kanal</p><div class="ulist"><ul><li><p>Daten von allen <strong>Subtasks</strong> von 2 Tasks werden unter Verwendung von Multiplexing über den gleichen TCP-Kanal übertragen</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datentransfer_zwischen_subtasks_grafik"><h2>Datentransfer zwischen Subtasks (Grafik)</h2><div class="imageblock" style=""><img src="images/flink-network-stack2.png" alt="flink network stack2" width="1300" height="600"></div>
<div class="paragraph small center"><small><em>(Bildquelle: <a href="https://flink.apache.org/2019/06/05/a-deep-dive-into-flinks-network-stack/" class="bare">https://flink.apache.org/2019/06/05/a-deep-dive-into-flinks-network-stack/</a>)</em></small></div></section>
<section id="deployment_von_subtasks"><h2>Deployment von Subtasks</h2><div class="ulist"><ul><li><p>Die möglichen <strong>Lokalisierungen</strong> von verbundenen <strong>Subtasks</strong> eines Jobs in abnehmender Verbindungsgeschwindigkeit:</p><div class="ulist"><ul><li><p>Im <strong>gleichen Task</strong> (verkettet)</p></li><li><p>In unterschiedlichen Tasks im <strong>gleichen TaskManager</strong></p></li><li><p>In unterschiedlichen TaskManagern auf der <strong>gleichen Node</strong></p></li><li><p>Auf <strong>unterschiedlichen Nodes</strong></p></li></ul></div></li><li><p>Abgesehen von der Verkettung, sind die anderen Möglichkeiten nicht direkt vom Entwickler einer Flink-Anwendung kontrollierbar</p><div class="ulist"><ul><li><p>Die Zuordnung von Tasks auf TaskManager wird automatisch vom <strong>Scheduler</strong> des Flink-Clusters vorgenommen</p></li></ul></div></li></ul></div></section>
<section id="datentransfer_credit_based_flow_control"><h2>Datentransfer: Credit-Based Flow Control</h2><div class="ulist"><ul><li><p>Jeder Subtask verwaltet einen <strong>Pool</strong> von <strong>Floating Buffers</strong></p></li><li><p>Zusätzlich hat jede Input Buffer Queue dieses Subtasks auch noch <strong>exklusive Buffer</strong></p></li><li><p>Die Floating Buffers können bei Bedarf in exklusive Buffer umgewandelt werden</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Jeder Kanal zwischen 2 Subtasks hat eine Anzahl von <strong>Credits</strong></p><div class="ulist"><ul><li><p>entspricht der Anzahl der verfügbaren exklusiven Buffer der Queue des Empfängers</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>Ziel</strong>: Jeder Kanal soll nicht mehr oder weniger exklusive Buffer haben, als er momentan benötigt</p><div class="ulist"><ul><li><p>&#8594; die vorhandenen Ressourcen werden optimal genutzt</p></li></ul></div></li></ul></div></section>
<section id="datentransfer_credit_based_flow_control_2"><h2>Datentransfer: Credit-Based Flow Control (2)</h2><div class="imageblock" style=""><img src="images/flink-network-stack4.png" alt="flink network stack4" height="420"></div>
<div class="paragraph center small"><small><em>(Bildquelle: <a href="https://flink.apache.org/2019/06/05/a-deep-dive-into-flinks-network-stack/" class="bare">https://flink.apache.org/2019/06/05/a-deep-dive-into-flinks-network-stack/</a>)</em></small></div>
<div class="ulist"><ul><li><p>Credit-Based Flow Control:</p><div class="ulist"><ul><li><p>Buffer werden nur verschickt, wenn der Kanal genug Credits aufweist</p></li><li><p>Ankommende Buffer reduzieren die Anzahl der Credits</p></li><li><p>Sender schicken Empfängern mit den Buffern auch die Anzahl der noch wartenden Buffer (Größe ihres <strong>Backlogs</strong>)</p></li><li><p>Nach Erhalt der Buffer fragt der Empfänger ggf. weitere Buffer aus dem Pool an, um die Credits an die Größe des Backlogs anzugleichen</p></li></ul></div></li></ul></div></section>
<section id="event_time_verarbeitung_watermark_propagation"><h2>Event-Time Verarbeitung : Watermark Propagation</h2><div class="ulist"><ul><li><p>Operatoren geben Watermarks von ihren Inputs an ihre Outputs weiter (<strong>propagation</strong>)</p></li><li><p>Ein Operator kann erst dann keine Ereignisse mit früherer <strong>event time</strong> als eine gegebene mehr erwarten, wenn dies für <strong>alle seine Inputs</strong> der Fall ist</p></li><li><p>Mechanismus für <strong>watermark propagation</strong>:</p><div class="ulist"><ul><li><p>Jeder Operator speichert für jeden seiner Inputs einen Timestamp</p><div class="ulist"><ul><li><p>Entspricht der <strong>letzten Watermark</strong>, die auf diesem Input angekommen ist</p></li></ul></div></li><li><p>Zusätzlich hat der Operator selbst auch noch einen Timestamp</p><div class="ulist"><ul><li><p>Entspricht dem <strong>Minimum</strong> der Timestamps seiner Inputs</p></li></ul></div></li><li><p>Immer wenn sich der Timestamp des Operators erhöht, emittiert dieser in seinen Outputs eine Watermark mit diesem Timestamp</p></li></ul></div></li></ul></div></section>
<section id="event_time_verarbeitung_watermark_propagation_grafik"><h2>Event-Time Verarbeitung : Watermark Propagation (Grafik)</h2><div class="imageblock" style=""><img src="images/watermarks-propagation.jpg" alt="watermarks propagation" width="1300" height="600"></div>
<div class="paragraph small center"><small><em>(Bildquelle: "Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019)</em></small></div></section>
<section id="event_time_verarbeitung_watermark_propagation_mit_window_operators"><h2>Event-Time Verarbeitung : Watermark Propagation mit Window Operators</h2><div class="ulist"><ul><li><p>Sonderregeln für <strong>event-time Window Operators</strong>:</p><div class="ulist"><ul><li><p>Timestamp des Operators entscheidet, wann ein Window geschlossen wird</p></li><li><p>Watermarks werden nur bei Schließen des Windows zusammen mit dem Output emittiert</p></li></ul></div></li></ul></div></section>
<section id="event_time_verarbeitung_idle_stream_problem"><h2>Event-Time Verarbeitung : Idle Stream Problem</h2><div class="ulist"><ul><li><p><strong>Idle stream problem</strong> bei Watermark Propagation:</p><div class="ulist"><ul><li><p>Wenn <strong>einer der Inputs</strong> eines Operators <strong>keine Aktivität</strong> hat:</p><div class="ulist"><ul><li><p>Dann wird der Timestamp dieses Inputs nicht mehr erhöht</p></li><li><p>Daher wird auch der Timestamp des Operators nicht erhöht</p></li><li><p>&#8594; Der Operator emittiert keine Watermarks mehr</p></li><li><p>&#8594; Window Operator schließt seine Windows nicht</p></li><li><p>&#8594; Pipeline ist lahmgelegt</p></li></ul></div></li></ul></div></li><li><p><strong>Lösungen</strong>:</p><div class="ulist"><ul><li><p>Partitionen besser <strong>balancieren</strong>, sodass keine leer oder dünn besetzt sind</p></li><li><p><strong>Keep-alive events</strong></p><div class="ulist"><ul><li><p>Auf jedem Input regelmäßig spezielle Ereignisse schicken</p></li></ul></div></li><li><p><strong>Idleness detection</strong></p><div class="ulist"><ul><li><p>Flink kann konfiguriert werden, Inaktivität eines Inputs zu erkennen und dieses nicht mehr für die Timestamps zu berücksichtigen</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="event_time_verarbeitung_watermark_alignment"><h2>Event-Time Verarbeitung : Watermark Alignment</h2><div class="ulist"><ul><li><p>Problemstellung:</p><div class="ulist"><ul><li><p><strong>event-time</strong> Window Operators, die einen <strong>Join</strong> von Inputs aus <strong>mehreren Quellen</strong> vornehmen, benötigen <strong>zeitlich auf einander abgestimmte</strong> Daten</p><div class="ulist"><ul><li><p>Beispiel : zeitlich korrelierte Messdaten, korreliertes Benutzerverhalten</p></li></ul></div></li><li><p>Wenn die Beschaffung oder Verarbeitung der Daten unterschiedlich lange dauert, können Daten mit ähnlicher <strong>event time</strong>, aber unterschiedlicher <strong>processing time</strong>
ankommen</p></li><li><p>&#8594; Windows verpassen Datensätze oder müssen sehr lange offen bleiben, um auf verspätete Datensätze zu warten</p></li></ul></div></li><li><p>Ansatz <strong>Watermark Alignment</strong>:</p><div class="ulist"><ul><li><p>Quellen können zu einer Gruppe gehören</p></li><li><p>Quellen in der gleichen Gruppe werden zeitlich aufeinander abgestimmt</p></li><li><p>"Schnellere" Quellen der Gruppe halten ggf. an, um auf "langsamere" zu warten</p></li></ul></div></li></ul></div></section>
<section id="event_time_verarbeitung_watermark_alignment_2"><h2>Event-Time Verarbeitung : Watermark Alignment (2)</h2><div class="imageblock" style=""><img src="images/eigene/watermark-alignment.png" alt="watermark alignment" height="720"></div>
<div class="ulist"><ul><li><p>Jede Gruppe speichert den bisher <strong>maximalen</strong> Timestamp von <strong>Watermarks</strong> aus ihren Quellen</p></li><li><p>Quellen der Gruppe <strong>pausieren</strong>, solange der Timestamp ihres nächsten Datensatzes größer als die <strong>Summe</strong> aus Timestamp der Gruppe plus einem gegebenen <strong><em>Maximalen Drift</em></strong> ist</p></li></ul></div></section>
<section id="event_time_verarbeitung_watermarkstrategy"><h2>Event-Time Verarbeitung : WatermarkStrategy</h2><div class="ulist"><ul><li><p>Watermarks werden in Flink über eine <strong>WatermarkStrategy</strong> konfiguriert</p><div class="ulist"><ul><li><p>Jeder Operator kann eine Strategy definieren, bevorzugt sollten dies aber nur die <strong>Quellen</strong> sein</p></li><li><p>Diese besteht aus einem <strong>TimestampAssigner</strong> und einem <strong>WatermarkGenerator</strong></p><div class="ulist"><ul><li><p>TimestampAssigner ist optional, um Datensätzen Timestamps (neu) zuzuordnen</p></li></ul></div></li></ul></div></li><li><p>Timestamps sind in Flink Long-Zahlenwerte in Millisekunden-Genauigkeit</p></li><li><p>Am Anfang jedes Streams mit Watermarks kommt ein Watermark mit Timestamp <em>Long.MIN_VALUE</em></p></li></ul></div></section>
<section id="event_time_verarbeitung_watermarkgenerator"><h2>Event-Time Verarbeitung : WatermarkGenerator</h2><div class="ulist"><ul><li><p><strong>Patterns</strong> für WatermarkGenerators:</p><div class="ulist"><ul><li><p>Ein <strong>Periodic WatermarkGenerator</strong> erstellt in gleichmäßigen zeitlichen Abständen Watermarks</p><div class="ulist"><ul><li><p>die im Watermark verwendete Zeit kann z.B. auf dem zuletzt verarbeiteten Datensatz oder auf der aktuellen Systemzeit (processing time) basieren</p></li></ul></div></li><li><p>Ein <strong>Punctuated WatermarkGenerator</strong> erstellt ein Watermark immer dann, wenn ein Datensatz verarbeitet wurde, der ein bestimmtes Kriterium erfüllt</p><div class="ulist"><ul><li><p>erfordert, dass der Inputstream entsprechend markierte Datensätze enthält</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="zustandshaltung_in_flink"><h2>Zustandshaltung in Flink</h2><div class="ulist"><ul><li><p>In Flink wird der State getrennt für jeden Operator verwaltet, um erhöhte Performance und Isolation zu gewährleisten</p></li><li><p>Jeglicher State eines Jobs ist assoziiert mit einem seiner Operatoren</p></li><li><p>Der State eines Jobs entspricht der Menge der States aller Operatoren</p></li><li><p>In Flink wird der State eines Jobs als Einheit betrachtet, der nur insgesamt gespeichert oder geladen werden kann.</p><div class="ulist"><ul><li><p>Damit kann erreicht werden, dass die States aller Operatoren aufeinander abgestimmt (konsistent) bleiben</p></li></ul></div></li></ul></div></section>
<section id="state_scopes"><h2>State Scopes</h2><div class="ulist"><ul><li><p>Es gibt 2 mögliche <strong>Scopes</strong> für State:</p><div class="ulist"><ul><li><p><strong>Operator State</strong> gehört zu einer parallelen <strong>Instanz</strong> des Operators (Subtask)</p></li><li><p><strong>Keyed State</strong> gehört zu einem <strong>Key</strong> des Inputstreams des Operators</p><div class="ulist"><ul><li><p>entspricht insgesamt einer Key-Value Map für diesen Operator</p></li></ul></div></li></ul></div></li></ul></div>
<table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/operator-state.jpg" alt="operator state" height="550"></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/keyed-state.jpg" alt="keyed state" height="550"></div></div></td></tr></table></section>
<section id="raw_state_und_managed_state"><h2>Raw State und Managed State</h2><div class="ulist"><ul><li><p>State kann physisch in 2 Formen existieren (konfigurierbar) :</p><div class="ulist"><ul><li><p><strong>Raw State</strong> wird nur von den Tasks in ihren eigenen Datenstrukturen erhalten</p><div class="ulist"><ul><li><p>beschränkte Recovery- und Scaling-Funktionen</p></li></ul></div></li><li><p><strong>Managed State</strong> (default) wird von der Flinklaufzeit verwaltet von einem TaskManager in speziellen Datenstrukturen gespeichert</p><div class="ulist"><ul><li><p>empfohlen, da es Flink mehr Optimierungs- und Recoveryoptionen gibt</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="state_backends"><h2>State Backends</h2><div class="ulist"><ul><li><p>Für die Speicherung von Managed State muss ein <strong>State Backend</strong> gewählt werden</p></li><li><p>Dies kann global für den Cluster oder individuell für einen Job konfiguriert werden</p></li><li><p>Per Default unterstützt Flink 2 Arten von Backends:</p><div class="ulist"><ul><li><p><strong>HashMapStateBackend</strong></p></li><li><p><strong>EmbeddedRocksDBStateBackend</strong></p></li></ul></div></li></ul></div></section>
<section id="state_backends_2"><h2>State Backends (2)</h2><div class="ulist"><ul><li><p><strong>HashMapStateBackend</strong></p><div class="ulist"><ul><li><p>Speichert State als Objekte im Java Heap (also im Arbeitsspeicher) des TaskManagers</p></li><li><p>Hohe Performance</p></li><li><p>Speicherplatz ist für die meisten Anwendungen ausreichend</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>EmbeddedRocksDBStateBackend</strong></p><div class="ulist"><ul><li><p>Speichert State serialisiert in einer RocksDB Datenbank in einem lokalen Verzeichnis im permanenten Speicher des TaskManagers</p></li><li><p>Etwas geringere Performance</p></li><li><p>Für sehr großen State geeignet</p></li><li><p>Geeignet für high availability, da inkrementelle Snapshots unterstützt werden</p></li></ul></div></li></ul></div></section>
<section id="state_typen"><h2>State Typen</h2><div class="ulist"><ul><li><p>Je nach Scope (Operator, Keyed) eines States sind unterschiedliche Typen auswählbar</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Für Keyed State und Operator State:</p><div class="ulist"><ul><li><p><strong>List State</strong> : Liste von Werten des gleichen Typs</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Nur für Keyed State:</p><div class="ulist"><ul><li><p><strong>Value State</strong> : einzelner Wert</p></li><li><p><strong>Map State</strong> : Key-Value Map von Werten</p></li><li><p><strong>AggregatingState</strong> und <strong>ReducingState</strong> :</p><div class="ulist"><ul><li><p>Einzelner Wert</p></li><li><p>Hinzufügen von Datensätzen aktualisiert den State (Beispiel: Bilden einer Summe)</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="state_patterns"><h2>State Patterns</h2><div class="ulist"><ul><li><p>Zusätzlich werden folgende <strong>State Patterns</strong> von Flink unterstützt:</p><div class="ulist"><ul><li><p><strong>Broadcast State</strong> :</p><div class="ulist"><ul><li><p>Alle Operatorinstanzen teilen sich den State (Keyed oder Operator Scope)</p></li><li><p>Ein Upstream Operator broadcasted den State an alle Instanzen seiner Downstream Operatoren</p></li><li><p>Es gibt keine Kommunikation zwischen den Tasks zum Abgleich des Broadcast States</p></li><li><p>Nur für In-Memory State Backend verfügbar</p></li></ul></div></li><li><p><strong>Union List State</strong> :</p><div class="ulist"><ul><li><p>Wie List State, aber anderes Verhalten bei Scaling und Recovery (s.u.)</p></li><li><p>Sollte nicht für große States verwendet werden</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="stateverteilung_nach_operator_scaling"><h2>Stateverteilung nach Operator Scaling</h2><div class="ulist"><ul><li><p><strong>Operator Rescaling</strong> : Veränderung der Parallelität eines Operators</p><div class="ulist"><ul><li><p>Für stateful Operatoren muss der State umverteilt werden</p></li><li><p>Wird in Flink von Savepoints unterstüzt</p><div class="ulist"><ul><li><p>Zum Rescaling wird ein Savepoint erstellt und der Job einfach vom Savepoint aus mit veränderter Parallelität neu gestartet</p></li></ul></div></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Wie wird der State nach Rescaling auf die neuen Operatorinstanzen verteilt?</p><div class="ulist"><ul><li><p>Bei <strong>Keyed State</strong>:</p><div class="ulist"><ul><li><p>Die möglichen Keys werden auf die neuen Instanzen verteilt und der State jedes Keys kann der zugehörigen Instanz zugeordnet werden</p></li><li><p>Für verbesserte Effizienz fasst Flink die Keys zu <strong>Key Groups</strong> zusammen und verteilt eigentlich diese</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="stateverteilung_nach_operator_scaling_keyed_grafik"><h2>Stateverteilung nach Operator Scaling : Keyed (Grafik)</h2><div class="imageblock" style=""><img src="images/opscaling-keyed.jpg" alt="opscaling keyed" width="1300" height="600"></div>
<div class="paragraph small center"><small><em>(Bildquelle: "Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019)</em></small></div></section>
<section id="stateverteilung_nach_operator_scaling_2"><h2>Stateverteilung nach Operator Scaling (2)</h2><div class="ulist"><ul><li><p>Wie wird der State nach Rescaling auf die neuen Operatorinstanzen verteilt?</p><div class="ulist"><ul><li><p>Bei <strong>Operator List State</strong>:</p><div class="ulist"><ul><li><p>Die Einträge der Listen aller ehemaligen Instanzen werden gleichmäßig auf die neuen Instanzen verteilt</p></li><li><p>Dies impliziert, dass die Listeneinträge voneinander unabhängig und die Operatorinstanzen in ihrer Funktion nicht unterscheidbar sind</p></li></ul></div></li><li><p>Bei <strong>Operator Union List State</strong>:</p><div class="ulist"><ul><li><p><strong>Jede</strong> Operatorinstanz erhält zunächst <strong>alle</strong> gespeicherten Listen</p></li><li><p>Daraufhin entscheidet jede Instanz, welche <strong>Teilmenge</strong> der Einträge sie behalten will</p></li><li><p>Hierfür sollten die Instanzen unterscheidbar sein bzw. verschiedene Rollen haben (z.B. individuelle Partitionierung)</p></li></ul></div></li><li><p>Bei <strong>Operator Broadcast State</strong>:</p><div class="ulist"><ul><li><p>Alle neuen Instanzen erhalten den (gleichen) State der alten Instanzen</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="stateverteilung_nach_operator_scaling_list_grafik"><h2>Stateverteilung nach Operator Scaling : List (Grafik)</h2><div class="imageblock" style=""><img src="images/opscaling-list.jpg" alt="opscaling list" width="1300" height="400"></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="paragraph small center"><small><em>(Bildquelle: "Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019)</em></small></div></section>
<section id="stateverteilung_nach_operator_scaling_union_grafik"><h2>Stateverteilung nach Operator Scaling : Union (Grafik)</h2><div class="imageblock" style=""><img src="images/opscaling-union.jpg" alt="opscaling union" width="1300" height="400"></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="paragraph small center"><small><em>(Bildquelle: "Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019)</em></small></div></section>
<section id="stateverteilung_nach_operator_scaling_broadcast_grafik"><h2>Stateverteilung nach Operator Scaling : Broadcast (Grafik)</h2><div class="imageblock" style=""><img src="images/opscaling-broadcast.jpg" alt="opscaling broadcast" width="1300" height="400"></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="paragraph"><p><em>(Bildquelle: "Stream Processing with Apache Flink" (F. Hueske, V. Kalavri), 1. Ed., 2019)</em></p></div></section>
<section id="checkpoints_und_savepoints"><h2>Checkpoints und Savepoints</h2><div class="ulist"><ul><li><p><strong>Checkpoints</strong></p><div class="ulist"><ul><li><p>Flink kann in regelmäßigen Abständen den State eines Jobs in sog. Checkpoints persistieren</p></li><li><p>Es wird (per default) nur der eigentliche State der Operatoren, nicht der Inhalt von Buffern gespeichert</p></li><li><p>Dies sind (per default) in sich konsistente Abbilder des Zustands aller Operatoren im Job (<strong>self-consistent snapshot</strong>)</p></li></ul></div></li><li><p><strong>Recovery</strong></p><div class="ulist"><ul><li><p>im Fehlerfall wird der Job automatisch mit dem im letzten Checkpoint gespeicherten State neu gestartet</p></li></ul></div></li><li><p><strong>Savepoints</strong></p><div class="ulist"><ul><li><p>ein anderes Beispiel von self-consistent snapshots</p></li><li><p>werden nur auf manuellen Befehl hin angelegt oder geladen</p></li></ul></div></li></ul></div></section>
<section id="self_consistent_snapshot"><h2>Self-consistent Snapshot</h2><div class="ulist"><ul><li><p>Ein <strong>snapshot</strong> (Momentaufnahme) des States eines Jobs ist <strong>self-consistent</strong>, wenn folgende Bedingung erfüllt ist:</p><div class="ulist"><ul><li><p>Es gibt einen <strong>Offset</strong> des Inputstreams, sodass gilt:</p><div class="ulist"><ul><li><p>Jeder Operator hat alle Datensätze im Inputstream, und daraus generierte Datensätze, bis zu diesem Offset vollständig bearbeitetet</p></li><li><p>Kein Operator hat einen Datensatz des Inputstreams mit einem größeren Offset bearbeitet</p></li></ul></div></li><li><p>Der Offset wird ebenfalls mit dem Snapshot gespeichert</p></li></ul></div></li></ul></div></section>
<section id="interne_und_end_to_end_konsistenz"><h2>Interne und End-To-End-Konsistenz</h2><div class="ulist"><ul><li><p><strong>Interne Konsistenz</strong>:</p><div class="ulist"><ul><li><p>Wie oft durchläuft jeder Inputdatensatz (und daraus generierte Datensätze) physisch den Jobgraphen?</p><div class="ulist"><ul><li><p>Hierfür kann Flink starke Garantien bieten</p></li></ul></div></li></ul></div></li><li><p><strong>End-To-End Konsistenz</strong>:</p><div class="ulist"><ul><li><p>Wie oft werden die aus einem Inputdatensatz generierten Ergebnisse von den <strong>externen</strong> Senken tatsächlich verarbeitet?</p><div class="ulist"><ul><li><p>Hierfür kann Flink nur starke Garantien bieten, falls das externe System kompatibel ist</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="interne_konsistenz_in_flink"><h2>Interne Konsistenz in Flink</h2><div class="ulist"><ul><li><p>Nach <strong>Laden</strong> eines Flink-Jobs mit einem <strong>self-consistent snapshot</strong> seines States zum Offset <span class="blue-font"><strong>n</strong></span>:</p><div class="ulist"><ul><li><p>die Verarbeitung beginnt bei einem Offset <span class="orange-font"><strong>m</strong></span> &gt; <span class="blue-font"><strong>n</strong></span></p></li><li><p>&#8594; Es gibt keine falschen Ergebnisse durch nur teilweise verarbeitete Daten</p></li><li><p>&#8594; Interne Konsistenz ist immer (mindestens) <strong>at-most-once</strong></p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>Ideale</strong> Wahl: <span class="orange-font"><strong>m</strong></span> = <span class="blue-font"><strong>n</strong></span>+1</p><div class="ulist"><ul><li><p>&#8594; Interne Konsistenz <strong>exactly once</strong></p></li><li><p>Erfordert Zurückrollbarkeit der Quelle</p></li></ul></div></li></ul></div></section>
<section id="end_to_end_konsistenz_mit_flink"><h2>End-To-End Konsistenz mit Flink</h2><div class="ulist"><ul><li><p>Zurückrollbare Quellen (<strong>at-least-once</strong> Semantik):</p><div class="ulist"><ul><li><p>Beispiel : Datei (filepointer), Kafka Topic (partition offsets)</p></li><li><p>Gegenbeispiel : Netzwerksocket ohne Caching</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Damit keine Ausgaben zweimal passieren (<strong>end-to-end at-most-once</strong>), muss auch der Outputstream der Senken effektiv zurückrollbar sein (schwierig)</p><div class="ulist"><ul><li><p>Beispiel : Kafka Topic (mit transaktionalen commits)</p></li><li><p>Gegenbeispiel : jeder externe Konsument ohne spezielle dahingehende Funktionalität</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>&#8594; Wenn beides gegeben ist, kann also mit Flink sogar <strong>end-to-end exactly-once</strong> garantiert werden</p></li></ul></div></section>
<section id="checkpointing_algorithmus"><h2>Checkpointing Algorithmus</h2><div class="ulist"><ul><li><p>Um einen Checkpoint zu erstellen, sollte nicht der gesamte Job angehalten werden müssen</p></li><li><p>Für einen Checkpoint werden an den Quellen spezielle Datensätze namens <strong>Barriers</strong> hinzugefügt,
die ähnlich wie Watermarks keine Daten, sondern nur die <strong>ID des Checkpoints</strong> enthalten</p><div class="ulist"><ul><li><p>Diese zeigen an, dass <strong>alle späteren Datensätze nicht mehr zu diesem Checkpoint</strong> gehören</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="imageblock" style=""><img src="images/stream_barriers.svg" alt="stream barriers" height="350px"></div>
<div class="paragraph small center"><small>(Bildquelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/stateful-stream-processing/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/stateful-stream-processing/</a>)</small></div></section>
<section id="checkpointing_algorithmus_2"><h2>Checkpointing Algorithmus (2)</h2><div class="ulist"><ul><li><p>Es gibt für jeden Job einen <strong>Checkpoint Coordinator</strong> als Teil des JobManagers</p></li><li><p>Checkpointing Algorithmus:</p><div class="ulist"><ul><li><p>Barriers werden an den <strong>Quellen</strong> hinzugefügt</p></li><li><p>Sobald ein <strong>Operator</strong> Barriers von allen seinen Inputs erhalten hat, persistiert er seinen State und emittiert Barriers in seinen Outputs</p></li><li><p>Sobald eine <strong>Senke</strong> Barriers von allen ihren Inputs erhalten hat, meldet sie dies an den Checkpoint Coordinator</p></li><li><p>Wenn das für alle Senken im Job passiert ist, beschließt der Checkpoint Coordinator den Checkpoint</p></li><li><p>&#8594; <strong>self-consistent snapshot</strong> zum letzten Offset vor der Barrier</p></li></ul></div></li></ul></div></section>
<section id="checkpointing_algorithmus_aligned_checkpointing"><h2>Checkpointing Algorithmus : Aligned Checkpointing</h2><div class="ulist"><ul><li><p>Wenn ein Operator <strong>mehrere Inputs</strong> hat, dann muss er nach Erhalt einer Barrier auf einem Inputkanal die Verarbeitung auf diesem Kanal <strong>anhalten</strong>, bis er
auch auf allen anderen Inputs Barriers erhalten hat</p><div class="ulist"><ul><li><p>Da der gespeicherte State nur die Verarbeitung bis zu den Barriers repräsentieren soll</p></li><li><p>&#8594; Die Inputs sind dann aufeinander abgestimmt (<strong>aligned</strong>)</p></li></ul></div></li><li><p>Dies führt zu einem <strong>aligned checkpoint</strong> und ist das Defaultverhalten von Flink</p></li></ul></div></section>
<section id="checkpoint_alghorithmus_aligned_checkpointing_grafik"><h2>Checkpoint Alghorithmus : Aligned Checkpointing (Grafik)</h2><div class="imageblock" style=""><img src="images/from-aligned-to-unaligned-checkpoints-part-1-3.png" alt="from aligned to unaligned checkpoints part 1 3" width="2000"></div>
<div class="paragraph small center"><small><em>(Bildquelle: <a href="https://flink.apache.org/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1-checkpoints-alignment-and-backpressure/" class="bare">https://flink.apache.org/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1-checkpoints-alignment-and-backpressure/</a>)</em></small></div></section>
<section id="checkpointing_algorithmus_aligned_checkpointing_nachteile"><h2>Checkpointing Algorithmus : Aligned Checkpointing (Nachteile)</h2><div class="ulist"><ul><li><p>aligned checkpoints sind oft eine sinnvolle Wahl, aber nicht immer</p></li><li><p>Mögliche Nachteile :</p><div class="ulist"><ul><li><p>Blockieren der Operatoren lässt CPU-Ressourcen des Clusters ungenutzt und kann zu Latenzen führen</p></li><li><p>Erstellen eines Checkpoints <strong>kann</strong> sehr lange dauern</p><div class="ulist"><ul><li><p>&#8594; Seltenere Checkpoints</p></li></ul></div></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Eine Alternative sind <strong>unaligned checkpoints</strong></p></li></ul></div></section>
<section id="checkpointing_algorithmus_unaligned_checkpointing"><h2>Checkpointing Algorithmus : Unaligned Checkpointing</h2><div class="ulist"><ul><li><p>Für <strong>unaligned checkpoints</strong> wird nicht nur der State des Operators, sondern auch die Daten in seinen <strong>Buffern</strong> gespeichert (<strong>in-flight data</strong>)</p></li><li><p>Unaligned Checkpointing Algorithmus:</p><div class="ulist"><ul><li><p>Wenn ein Operator <strong>das erste Mal</strong> auf einem seiner <strong>Inputbuffer</strong> eine <strong>Barrier</strong> erhält, dann:</p><div class="ulist"><ul><li><p>Fügt er sofort eine Barrier ans Ende seiner <strong>Outputbuffer</strong> an</p></li><li><p>Persistiert er seinen State</p></li><li><p>Markiert er alle hinter der neuen Barrier liegenden Buffer zur Persistierung</p></li></ul></div></li></ul></div></li><li><p>Konsistenzgarantien sind die gleichen wie für aligned checkpoints</p></li></ul></div></section>
<section id="checkpointing_algorithmus_unaligned_checkpointing_grafik"><h2>Checkpointing Algorithmus : Unaligned Checkpointing (Grafik)</h2><div class="imageblock" style=""><img src="images/stream_unaligning.svg" alt="stream unaligning" width="2000"></div>
<div class="paragraph small center"><small><em>(Bildquelle: <a href="https://flink.apache.org/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1-checkpoints-alignment-and-backpressure/" class="bare">https://flink.apache.org/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1-checkpoints-alignment-and-backpressure/</a>)</em></small></div></section>
<section id="checkpointing_algorithmus_aligned_vs_unaligned_checkpointing"><h2>Checkpointing Algorithmus : Aligned vs Unaligned Checkpointing</h2><div class="ulist"><ul><li><p><strong>Vorteile</strong> von <strong>unaligned checkpoints</strong>:</p><div class="ulist"><ul><li><p>Barriers erreichen Senken schneller</p><div class="ulist"><ul><li><p>&#8594; Häufigere Checkpoints möglich</p></li></ul></div></li><li><p>Zeit für Checkpointing ist von der End-To-End-Latenz des Jobs unabhängig</p></li></ul></div></li><li><p><strong>Einschränkungen</strong> von <strong>unaligned checkpoints</strong>:</p><div class="ulist"><ul><li><p>mehr IO nötig für Speichern der Buffer, größerer Speicherbedarf</p><div class="ulist"><ul><li><p>Checkpointing ist nur dann schneller, wenn IO kein Bottleneck ist</p></li></ul></div></li><li><p>Laden dauert länger und erfordert Wiederverarbeitung der gespeicherten Buffer</p></li><li><p>Laden von Watermarks ist leicht unterschiedlich</p><div class="ulist"><ul><li><p>für Details siehe <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/state/checkpointing_under_backpressure/" class="bare">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/state/checkpointing_under_backpressure/</a></p></li></ul></div></li><li><p>es können nicht mehrere Checkpoints gleichzeitig erstellt werden</p></li></ul></div></li></ul></div></section>
<section id="recovery_von_checkpoints_vorgang"><h2>Recovery von Checkpoints (Vorgang)</h2><div class="imageblock" style=""><img src="images/from-aligned-to-unaligned-checkpoints-part-1-4.png" alt="from aligned to unaligned checkpoints part 1 4" width="2000"></div>
<div class="paragraph small center"><small><em>(Bildquelle: <a href="https://flink.apache.org/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1-checkpoints-alignment-and-backpressure/" class="bare">https://flink.apache.org/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1-checkpoints-alignment-and-backpressure/</a>)</em></small></div>
<div class="ulist"><ul><li><p>Für unaligned Checkpointing kommt noch der Schritt hinzu, die gespeicherten Buffer zu laden</p></li></ul></div></section>
<section id="incremental_checkpointing"><h2>Incremental Checkpointing</h2><div class="ulist"><ul><li><p>Idee von <strong>incremental checkpointing</strong>:</p><div class="ulist"><ul><li><p>Nur die Veränderungen im Vergleich zum letzten Checkpoint speichern</p></li></ul></div></li><li><p>Nicht alle State Backends unterstützen dieses Feature (per Default nur <strong>RockDB</strong>)</p></li><li><p>Vorteile:</p><div class="ulist"><ul><li><p>Weniger Speicherbedarf bei <strong>großem State</strong></p></li><li><p>Recovery kann schneller sein (weniger <strong>CPU</strong>- und <strong>IO</strong>-Ressourcen benötigt)</p></li></ul></div></li><li><p>Nachteile:</p><div class="ulist"><ul><li><p>Recovery kann auch länger dauern, da ggf. eine History von mehreren Checkpoints geladen werden muss (&#8594; <strong>Netzwerklast</strong>)</p><div class="ulist"><ul><li><p>RockDB verwendet <strong>Compaction</strong>, um inkrementelle Checkpoints zusammenzufassen und die Größe der History zu beschränken</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="weitere_optionen_für_checkpointing"><h2>Weitere Optionen für Checkpointing</h2><div class="ulist"><ul><li><p><strong>at-least-once Checkpointing</strong></p><div class="ulist"><ul><li><p>Entspricht unaligned checkpointing ohne Speicherung der Buffer</p><div class="ulist"><ul><li><p>&#8594; Schnell, aber keine at-most-once Konsistenz</p></li></ul></div></li><li><p>Interessant, falls sehr niedrige Latenzen wichtig sind</p></li></ul></div></li><li><p><strong>concurrent checkpointing</strong></p><div class="ulist"><ul><li><p>Es können mehrere Checkpoints gleichzeitig erstellt werden</p></li><li><p>Keine Probleme mit Konsistenz, allerdings kann es zu erhöhten Latenzen führen</p></li><li><p>Interessant, wenn <strong>aligned</strong> Checkpoints mit hoher Frequenz benötigt werden</p></li></ul></div></li><li><p><strong>externalized checkpoints</strong></p><div class="ulist"><ul><li><p>Checkpoints werden in externen Speicher persistiert</p></li><li><p>Ermöglicht es, Checkpoints zu erhalten, auch nachdem ein Job <strong>abgebrochen</strong> wurde</p></li><li><p>Jobs können dann manuell mit diesen Checkpointdaten neugestartet werden</p></li></ul></div></li></ul></div></section>
<section id="savepoints"><h2>Savepoints</h2><div class="ulist"><ul><li><p>Savepoints haben die Funktion eines <strong>Backups</strong>, um die Konsistenz eines Jobs nach <strong>geplanten Downtimes</strong> (Updates, Rescaling, Migration &#8230;&#8203;) zu erhalten</p></li><li><p>Erstellen, Laden und Löschen muss manuell ausgelöst werden</p></li><li><p>Werden nach dem gleichen Algorithmus wie <strong>aligned</strong> Checkpoints erstellt</p></li><li><p>Enthalten zusätzliche <strong>Metainformationen</strong></p></li><li><p>Portabler und <strong>flexibler</strong> als Checkpoints, dafür langsamer zu laden</p><div class="ulist"><ul><li><p>Können mit veränderten Versionen des gleichen Jobs verwendet werden, solange die States kompatibel sind</p></li><li><p>Können nach Update von Flink oder Änderung des State Backends weiter verwendet werden</p></li></ul></div></li></ul></div></section>
<section id="jobs_im_detail"><h2>Jobs im Detail</h2><div class="paragraph heading center"><p>Jobs im Detail</p></div></section>
<section id="übersicht_der_apis_für_flink_anwendungen"><h2>Übersicht der APIs für Flink-Anwendungen</h2><div class="ulist"><ul><li><p><strong>DataStream API</strong> :</p><div class="ulist"><ul><li><p>Flexible API, die mit Streams und Windows arbeitet</p></li><li><p>Ermöglicht sowohl low-level Stream-Processing Operationen (über ProcessFunctions) als auch high-level Operationen</p></li><li><p>für Java (oder Scala); Python hat eine eigene Version "PyFlink DataStream API"</p></li></ul></div></li><li><p><strong>Table API</strong> :</p><div class="ulist"><ul><li><p>high-level API, die mit dynamischen Tabellen arbeitet</p></li><li><p>für Java, Python</p></li></ul></div></li><li><p><strong>Flink SQL</strong> :</p><div class="ulist"><ul><li><p>Sehr high-level API, die mit SQL-Anfragen arbeitet, die Streams ähnlich wie Tabellen in RDBs behandeln</p></li><li><p>SQL-Unterstützung basiert auf Apache Calcite</p></li><li><p>unterstützt nur eine Teilmenge der SQL-Statements</p></li><li><p>Interaktion mit Cluster über "SQL Client" Anwendung von Flink</p></li></ul></div></li><li><p><strong>DataSet API</strong> (veraltet)</p></li></ul></div></section>
<section id="datastream_api"><h2>DataStream API</h2><div class="paragraph"><p>&#8594; In diesem Kapitel arbeiten wir ab jetzt mit der <strong>Data Stream API</strong></p></div></section>
<section id="struktur_einer_datastream_anwendung_in_java"><h2>Struktur einer DataStream-Anwendung (in Java)</h2><div class="ulist"><ul><li><p>Eine ausführbare <strong>Flink-Anwendung</strong> kann einen oder mehrere <strong>Jobs</strong> enhthalten</p></li><li><p>Für jeden auszuführenden Job:</p><div class="ulist"><ul><li><p>Definition eines <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/api/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html">StreamExecutionEnvironment</a></strong></p></li><li><p>Operatoren werden dem StreamExecutionEnvironment in der Reihenfolge ihrer Anwendung hinzugefügt</p></li><li><p>Auf dem StreamExecutionEnvironment wird die Methode <em>execute</em> ausgeführt</p><div class="ulist"><ul><li><p>Dies übergibt den Job zur Ausführung an einen JobManager</p></li></ul></div></li></ul></div></li><li><p>Beispiel:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">HelloFlink</span> {
    <span class="directive">public</span> <span class="directive">static</span> <span class="type">void</span> main(<span class="predefined-type">String</span><span class="type">[]</span> args) <span class="directive">throws</span> <span class="exception">Exception</span> {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream&lt;<span class="predefined-type">String</span>&gt; dataStream = env.fromElements(
                <span class="string"><span class="delimiter">&quot;</span><span class="content">Hello</span><span class="delimiter">&quot;</span></span>,
                <span class="string"><span class="delimiter">&quot;</span><span class="content">World</span><span class="delimiter">&quot;</span></span>);
        dataStream.print();
        env.execute();
    }
}</code></pre></section>
<section id="streamexecutionenvironment"><h2>StreamExecutionEnvironment</h2><div class="paragraph"><p>Erstellung eines <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/api/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html">StreamExecutionEnvironment</a></strong> über statische Factory-Methoden:</p></div>
<div class="ulist"><ul><li><p><em>getExecutionEnvironment()</em> :</p><div class="ulist"><ul><li><p>erstellt Umgebung zur Ausführung im Flink-Cluster</p></li><li><p>wenn die Anwendung lokal als normale Java-Anwendung (z.B. in der IDE) gestartet wird, wird statt dessen eine lokale Umgebung in nur einer JVM erstellt (für Tests geeignet)</p><div class="ulist"><ul><li><p>JobManager und TaskManager erhalten dann jeweils einen Thread</p></li></ul></div></li></ul></div></li><li><p><em>createLocalEnvironment()</em> :</p><div class="ulist"><ul><li><p>erzwingt lokale Ausführung</p></li></ul></div></li><li><p><em>createRemoteEnvironment(String host, int port, String&#8230;&#8203; jarFiles)</em> :</p><div class="ulist"><ul><li><p>versucht Ausführung in einem Flink-Cluster an dem angegebenen Ort, ggf. mit Dependencies</p></li></ul></div></li><li><p>es gibt jeweils Varianten, die noch eine <strong>Configuration</strong> mitgeben können</p></li><li><p>Wenn man die Table API verwendet, muss statt dessen ein <strong>TableEnvironment</strong> erstellt werden</p></li></ul></div></section>
<section id="aufgabe_0_teil_2"><h2>Aufgabe 0, Teil 2</h2><div class="paragraph heading"><p>Erster Blick auf einen Flinkjob in Java</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Downloaded Sie sich den Sourcecode von Aufgabe 0 und öffnen Sie das Projekt in IntelliJ</p></li><li><p>Bauen Sie das Projekt mit "mvn clean install"</p></li><li><p>Führen Sie die Klasse Aufgabe0 über die IDE mit "Run" aus, um die Flinkanwendung lokal zu starten</p><div class="ulist"><ul><li><p>Stellen Sie zunächst in der Run Configuration folgendes ein:</p><div class="ulist"><ul><li><p>die Option "Add dependencies with 'provided' scope to classpath"</p></li><li><p>Java Version 11</p></li></ul></div></li></ul></div></li><li><p>Sehen Sie sich den Log in der Konsolenausgabe gründlich an</p></li></ol></div></section>
<section id="aufgabe_0_teil_3"><h2>Aufgabe 0, Teil 3</h2><div class="olist arabic"><ol class="arabic"><li><p>Erstellen Sie ein eigenes Maven-Projekt über die Konsole, indem Sie in einem von Ihnen erstellen Ordner den folgenden Befehl ausführen:</p></li></ol></div>
<pre class="CodeRay listingblock"><code>mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink \
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.17.1 \
      -DgroupId=flinkSchulung  \
      -DartifactId=flinkDemo \
      -Dversion=1.0-SNAPSHOT \
      -DinteractiveMode=false</code></pre>
<div class="olist arabic"><ol class="arabic" start="2"><li><p>Öffnen Sie das erstellte Projekt in IntelliJ und stellen Sie sicher, dass Sie es mit Maven bauen können</p><div class="ulist"><ul><li><p>Sie können das Projekt als Basis für weitere Aufgaben verwenden</p></li></ul></div></li></ol></div></section>
<section id="datastream_api_datentypen"><h2>DataStream API : Datentypen</h2><div class="ulist"><ul><li><p>Welche (Java) <strong>Datentypen</strong> können mit Flink <strong>gestreamt</strong> und für <strong>State</strong> verwendet werden ?</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Flink hat einen <strong>nativen Serialisierer</strong>, der folgende Typen unterstützt:</p><div class="ulist"><ul><li><p><strong>Primitive Typen</strong> und String, Date, BigDecimal, BigInteger, void</p></li><li><p>Aus solchen Typen (rekursiv) <strong>zusammengesetzte</strong> Typen:</p><div class="ulist"><ul><li><p>Arrays, Lists, Maps, Tuples</p></li><li><p>Plain Old Java Objects (POJOs, eingeschränkt)</p></li></ul></div></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong>Tuples</strong> sind ein einfacher Wrapper von Flink für  zusammengesetzte Typen</p><div class="ulist"><ul><li><p>Verwendung z.B. <em>Tuple2&lt;String, Integer&gt;</em></p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Der native Serialisierer wird bei passenden Datentypen <strong>per Default</strong> von Flink verwendet und bietet die <strong>beste Performance</strong></p></li></ul></div></section>
<section id="datastream_api_datentypen_2"><h2>DataStream API : Datentypen (2)</h2><div class="ulist"><ul><li><p><strong>Regeln</strong> für nativ serialisierbare <strong>POJOs</strong> :</p><div class="ulist"><ul><li><p>Top-Level Klasse mit public access</p></li><li><p>hat einen public no-argument Konstruktor</p></li><li><p>nicht-statische Felder haben nativ serialisierbare Typen und public access oder public Getter und Setter</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Jede andere Klasse, die das <strong>Serializable</strong>-Interface implementiert:</p><div class="ulist"><ul><li><p>Wird per default als <strong><em>generischer Typ</em></strong> mit dem <strong>Kryo</strong>-Framework serialisert</p></li><li><p>Kryo ist flexibel, aber langsam</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Auch <strong>Apache Avro</strong> wird unterstützt und kann nach Einbinden einer geeigneten Dependency verwendet werden</p></li><li><p>Darüber hinaus lassen sich auch <strong>eigene Serialisierer</strong> definieren</p></li></ul></div></section>
<section id="executionconfig"><h2>ExecutionConfig</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/api/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html">StreamExecutionEnvironment</a></strong> kann über seine <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/ExecutionConfig.html">ExecutionConfig</a></strong> konfiguriert werden</p><div class="ulist"><ul><li><p>Beispiel:</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
ExecutionConfig config = env.getConfig();
config.setAutoWatermarkInterval(<span class="integer">100</span>);
config.setRestartStrategy(RestartStrategies.noRestart());</code></pre></section>
<section id="datastreamt"><h2>DataStream&lt;T&gt;</h2><div class="ulist"><ul><li><p>Die Klasse <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html">DataStream&lt;T&gt;</a></strong> ist die zentrale Abstraktion der DataStream API</p><div class="ulist"><ul><li><p>steht für einen Stream, der Datensätze vom Typ <strong>T</strong> enthält</p></li></ul></div></li><li><p>DataStreams können aus einer <strong>Source</strong> (Quelle) abgeleitet werden</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">String</span>&gt; helloStream = env.fromElements(<span class="string"><span class="delimiter">&quot;</span><span class="content">Hello</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">World</span><span class="delimiter">&quot;</span></span>);</code></pre>
<div class="ulist"><ul><li><p><strong>Operatoren</strong> können mittels einer Fluent API zu Streams hinzugefügt und konfiguriert werden</p><div class="ulist"><ul><li><p>der ursprüngliche Stream ist der Input des Operators, der zurückgegebene Stream der Output</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">String</span>&gt; lowercaseStream = helloStream.map(<span class="predefined-type">String</span>::toLowerCase);</code></pre>
<div class="ulist"><ul><li><p>mit der <em>sinkTo</em> Methode kann der DataStream mit einer <strong>Sink</strong> (Senke) verknüpft werden</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">lowercaseStream.sinkTo(FileSink.forRowFormat(<span class="keyword">new</span> Path(<span class="string"><span class="delimiter">&quot;</span><span class="content">path</span><span class="delimiter">&quot;</span></span>), <span class="keyword">new</span> SimpleStringEncoder&lt;<span class="predefined-type">String</span>&gt;(<span class="string"><span class="delimiter">&quot;</span><span class="content">UTF-8</span><span class="delimiter">&quot;</span></span>))
                               .build());</code></pre></section>
<section id="datastream_api_source"><h2>DataStream API : Source</h2><div class="ulist"><ul><li><p>Erstellung einer <strong>Source</strong> mit einem StreamExecutionEnvironment <em>env</em>:</p><div class="ulist"><ul><li><p>Stream aus einzelnen Elementen oder einer Collection:</p><div class="ulist"><ul><li><p><em>env.fromElements(element1, element2, ..)</em></p></li><li><p><em>env.fromCollection(collection)</em></p></li></ul></div></li><li><p>Stream aus Socket mit Textdaten :</p><div class="ulist"><ul><li><p><em>env.socketTextStream(..)</em></p></li></ul></div></li><li><p>Generische Quelle :</p><div class="ulist"><ul><li><p>Interface <strong>SourceFunktion&lt;T&gt;</strong> implementieren</p></li><li><p>hinzufügen mit <em>env.addSource(sourceFunction)</em></p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_source_aus_datei"><h2>DataStream API : Source aus Datei</h2><div class="ulist"><ul><li><p>Beispiel für Stream aus einer Datei:</p><div class="ulist"><ul><li><p>benötigt Dependency <strong>flink-connector-files</strong></p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">String</span>&gt; inputLines = FileSource.forRecordStreamFormat(
                                                <span class="keyword">new</span> TextLineInputFormat(),
                                                <span class="keyword">new</span> Path(<span class="string"><span class="delimiter">&quot;</span><span class="content">path</span><span class="delimiter">&quot;</span></span>))
                                          .build()</code></pre>
<div class="ulist"><ul><li><p>liest Textdatei zeilenweise als Stream von Strings</p></li><li><p>Kurze Variante ohne zusätzliche Dependency (deprecated):</p></li></ul></div>
<pre class="CodeRay listingblock"><code>DataStream&lt;String&gt; inputLines = env.readTextFile("path");</code></pre></section>
<section id="verwendung_von_datageneratorsource_und_kafkasource_beispiel"><h2>Verwendung von DataGeneratorSource und KafkaSource (Beispiel)</h2><div class="ulist"><ul><li><p>Erstellung einer <strong>DataGeneratorSource</strong> für einen automatisch generierten Stream:</p><div class="ulist"><ul><li><p>erfordert die Dependency <strong>flink-connector-datagen</strong></p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataGeneratorSource&lt;<span class="predefined-type">String</span>&gt; source =
        <span class="keyword">new</span> DataGeneratorSource&lt;&gt;(
                        index -&gt; <span class="string"><span class="delimiter">&quot;</span><span class="content">Record#</span><span class="delimiter">&quot;</span></span> + index,
                        numRecords,
                        RateLimiterStrategy.perSecond(<span class="integer">1</span>),
                        <span class="predefined-type">Types</span>.STRING);</code></pre>
<div class="ulist"><ul><li><p>Erstellung einer <strong>KafkaSource</strong> aus einem Kafka Topic:</p><div class="ulist"><ul><li><p>erfordert die Dependency <strong>flink-connector-kafka</strong></p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaSource&lt;<span class="predefined-type">String</span>&gt; source = KafkaSource.&lt;<span class="predefined-type">String</span>&gt;builder()
        .setProperties(config)
        .setTopics(<span class="string"><span class="delimiter">&quot;</span><span class="content">topic1</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">topic2</span><span class="delimiter">&quot;</span></span>)
        .setValueOnlyDeserializer(<span class="keyword">new</span> SimpleStringSchema())
        .build();</code></pre></section>
<section id="datastream_api_sinks"><h2>DataStream API : Sinks</h2><div class="ulist"><ul><li><p><strong>Sinks</strong> mit einem Stream verknüpfen:</p><div class="ulist"><ul><li><p>Konsolenausgabe:</p><div class="ulist"><ul><li><p>stream.print()</p></li><li><p>stream.printToErr()</p></li></ul></div></li><li><p>Dateiausgabe (deprecated zugunsten von flink-connector-files):</p><div class="ulist"><ul><li><p>stream.writeAsText(&lt;Path&gt;, &lt;FileSystem.WriteMode&gt;)</p></li><li><p>stream.writeAsCsv(&lt;Path&gt;)</p></li><li><p>stream.writeUsingOutputFormat(&lt;OutputFormat&gt;)</p></li></ul></div></li><li><p>Socket :</p><div class="ulist"><ul><li><p>stream.writeToSocket(&lt;hostName&gt;, &lt;port&gt;, &lt;SerializationSchema&gt;)</p></li></ul></div></li><li><p>Generische Sink :</p><div class="ulist"><ul><li><p>Interface <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/connector/sink2/Sink.html">Sink&lt;T&gt;</a></strong> (flexibler) bzw. <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/sink/SinkFunction.html">SinkFunction&lt;T&gt;</a></strong> implementieren</p></li><li><p>hinzufügen mit <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#sinkTo-org.apache.flink.api.connector.sink.Sink-">stream.sinkTo(sink)</a> bzw. <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#addSink-org.apache.flink.streaming.api.functions.sink.SinkFunction-">stream.addSink(sinkFunction)</a></p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_beispiel_custom_sinkfunction"><h2>DataStream API : Beispiel Custom SinkFunction</h2><div class="ulist"><ul><li><p>Beispiel für eine SinkFunction, die Datensätze in Batches an ein (nicht gezeigtes) externes System schickt:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">BufferingSink</span>&lt;T&gt;
        <span class="directive">implements</span> SinkFunction&lt;T&gt;{

    <span class="directive">private</span> <span class="directive">final</span> <span class="type">int</span> threshold;

    <span class="directive">private</span> <span class="predefined-type">List</span>&lt;T&gt; bufferedElements = <span class="keyword">new</span> <span class="predefined-type">ArrayList</span>&lt;&gt;();

    <span class="directive">public</span> BufferingSink(<span class="type">int</span> threshold) {
        <span class="local-variable">this</span>.threshold = threshold;
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> invoke(T value, <span class="predefined-type">Context</span> context) <span class="directive">throws</span> <span class="exception">Exception</span> {
        bufferedElements.add(value);
        <span class="keyword">if</span> (bufferedElements.size() &gt;= threshold) {
            <span class="keyword">for</span> (T element : bufferedElements) {
                <span class="comment">// send it to the sink</span>
            }
            bufferedElements.clear();
        }
    }
}</code></pre>
<div class="paragraph center small"><small>(Beispiel angepasst aus <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/fault-tolerance/state/" class="bare">https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/fault-tolerance/state/</a>)</small></div></section>
<section id="datastream_api_verwendung_von_kafkasink_beispiel"><h2>DataStream API : Verwendung von KafkaSink (Beispiel)</h2><div class="ulist"><ul><li><p>Festlegung der Serialisierung des Outputs der Senke:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaRecordSerializationSchema&lt;MyClass&gt; serializer =
    KafkaRecordSerializationSchema.&lt;MyClass&gt;builder()
                                  .setTopic(<span class="string"><span class="delimiter">&quot;</span><span class="content">topic1</span><span class="delimiter">&quot;</span></span>)
                                  .setValueSerializationSchema(<span class="keyword">new</span> JsonSerializationSchema&lt;&gt;())
                                  .build();</code></pre>
<div class="ulist"><ul><li><p>Erstellung der Konfiguration für den Kafka Producer (nicht gezeigt)</p></li><li><p>Erstellung der Senke :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaSink&lt;MyClass&gt; kafkaSink = KafkaSink.&lt;MyClass&gt;builder()
                                        .setKafkaProducerConfig(config)
                                        .setRecordSerializer(serializer)
                                        .setDeliveryGuarantee(DeliveryGuarantee.NONE)
                                        .build();</code></pre>
<div class="ulist"><ul><li><p>Hinzufügen zum Job :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">stream.sinkTo(kafkaSink);</code></pre></section>
<section id="datastream_api_einfache_transformationen"><h2>DataStream API : Einfache Transformationen</h2><div class="ulist"><ul><li><p>Wir sehen uns zunächst an, wie sich einfache <strong>stateless</strong> Operatoren erstellen lassen</p></li><li><p>Die Syntax in der DataStream API ist teilweise angelehnt an die Java Stream API</p></li><li><p>gegeben sei jeweils ein Stream "stream" vom Typ DataStream&lt;MyClass&gt; für eine serialisierbare Klasse MyClass</p></li><li><p><strong>stream.map(mapFunction)</strong></p><div class="ulist"><ul><li><p>erstellt aus jedem einzelnen Element in einem InputStream ein neues Element im Outputstream nach einer angegebenen Vorschrift</p></li><li><p>mapFunction ist vom Typ <strong>MapFunction&lt;MyClass, OtherClass&gt;</strong> und kann durch einen lambda-Ausdruck gegeben werden (ähnliches gilt für die folgenden Transformationen)</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">Double</span>&gt; doublesStream = integerStream.map(
        x -&gt; <span class="predefined-type">Double</span>.valueOf(x) / <span class="integer">2</span>
);</code></pre></section>
<section id="datastream_api_filter"><h2>DataStream API : Filter</h2><div class="ulist"><ul><li><p><strong>stream.filter(filter)</strong></p><div class="ulist"><ul><li><p>gibt Teilstream der Elemente im Stream aus, für die der Filter true ergibt</p></li><li><p>filter ist vom Typ <strong>FilterFunction&lt;MyClass&gt;</strong></p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">Integer</span>&gt; evenNumbersStream = integerStream.filter(
        x -&gt; x % <span class="integer">2</span> == <span class="integer">0</span>
);</code></pre></section>
<section id="datastream_api_flatmap"><h2>DataStream API : FlatMap</h2><div class="ulist"><ul><li><p><strong>stream.flatMap(flatMapFunction)</strong></p><div class="ulist"><ul><li><p>erstellt aus jedem Streamelement einen Stream von neuen Elementen, die jeweils dem Outputstream angehängt werden</p></li><li><p>flatMapFunction ist vom Typ <strong>FlatMapFunction&lt;MyClass, OtherClass&gt;</strong> und implementiert die Methode flatMap(MyClass, Collector&lt;OtherClass&gt;)</p><div class="ulist"><ul><li><p>mit collector.collect(item) können Daten zum Ausgabestream hinzugefügt werden</p></li></ul></div></li><li><p>es müssen nicht zu jedem Input Outputelemente generiert werden</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">String</span>&gt; stringStream = collectionStream.flatMap(
(<span class="predefined-type">Collection</span>&lt;<span class="predefined-type">String</span>&gt; collection, Collector&lt;<span class="predefined-type">String</span>&gt; collector) -&gt; {
        <span class="keyword">for</span> (<span class="predefined-type">String</span> item : collection) {
            collector.collect(item);
        }
}).returns(<span class="predefined-type">Types</span>.STRING);</code></pre>
<div class="ulist"><ul><li><p><strong>Der Typ des Outputstreams muss bei Verwendung eines lambda-Ausdrucks durch anschließende Anwendung der <em>returns(TypeInformation&lt;Type&gt;)</em> Methode explizit angegeben werden</strong></p></li></ul></div></section>
<section id="datastream_api_keyby"><h2>DataStream API : KeyBy</h2><div class="ulist"><ul><li><p><strong>stream.keyBy(keySelector)</strong></p><div class="ulist"><ul><li><p>erzeugt einen partitionierbaren <strong>KeyedStream&lt;MyClass, KeyType&gt;</strong></p><div class="ulist"><ul><li><p>KeyedStream&lt;MyClass, KeyType&gt; extends DataStream&lt;MyClass&gt;</p></li></ul></div></li><li><p>keySelector ordnet Datensätzen einen Key vom Typ KeyType zu</p></li><li><p>ermöglicht mehr Optionen für parallel processing</p></li><li><p>manche Operatoren sind nur auf KeyedStreams anwendbar</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KeyedStream&lt;<span class="predefined-type">Polygon</span>, <span class="predefined-type">Color</span>&gt; keyedPolygonStream = polygonStream.keyBy(
        polygon -&gt; polygon.getColor()
);</code></pre></section>
<section id="datastream_api_reduce"><h2>DataStream API : Reduce</h2><div class="ulist"><ul><li><p><strong>keyedStream.reduce(reduceFunction)</strong></p><div class="ulist"><ul><li><p>erstellt eine <strong>Rolling Aggregation (stateful)</strong>, bei der Input- und Outputtyp identisch sind</p></li><li><p>mit jedem neuen Datensatz erfährt das Aggregat für den Key dieses Datensatzes ein Update und wird ausgegeben</p></li><li><p>wird auf einen <strong>KeyedStream&lt;MyClass&gt;</strong> angewendet und produziert einen <strong>DataStream&lt;MyClass&gt;</strong></p></li><li><p>benötigt eine <strong>ReduceFuntion&lt;MyClass&gt;</strong></p></li><li><p>die ReduceFunction sollte symmetrisch in ihren beiden Eingaben sein, da diese nicht unterscheidbar sind</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">Integer</span>&gt; sumsByKey = numberStream.reduce((number1, number2) -&gt; number1 + number2);</code></pre>
<div class="ulist"><ul><li><p>im Beispiel entsteht ein Stream aus Zahlen ohne Keys</p><div class="ulist"><ul><li><p>Falls die Keys später benutzt werden sollen, müssen sie aus den Outputdatensätzen selber extrahierbar sein</p></li></ul></div></li></ul></div></section>
<section id="aufgabe_2"><h2>Aufgabe 2</h2><div class="olist arabic"><ol class="arabic"><li><p>Erstellen Sie mit Java einen Flink Job, der folgendes tut:</p><div class="ulist"><ul><li><p>Nimmt eine Menge von Textzeilen als Quelle (im Code als Konstante definieren oder aus Datei auslesen)</p></li><li><p>Schreibt einen Stream von Zeilen in eine Datei</p><div class="ulist"><ul><li><p>Für jedes Wort in einer der Inputzeilen gibt es eine Ausgabezeile</p></li><li><p>Die Ausgabezeilen haben das Format "Wortname : Anzahl"</p></li><li><p>wobei "Anzahl" die Anzahl der bisher verarbeiteten Vorkommen des Wortes angibt</p></li></ul></div></li><li><p>Inputzeilen, die mit dem Zeichen "#" anfangen, werden allerdings nicht berücksichtigt</p></li></ul></div></li></ol></div>
<div class="olist arabic"><ol class="arabic" start="2"><li><p>Testen Sie ihren Job, indem Sie ihn auf einem Flink-Cluster ausführen, und verifizieren Sie die Ausgabe</p></li></ol></div></section>
<section id="aufgabe_2_hinweise"><h2>Aufgabe 2 (Hinweise)</h2><div class="ulist"><ul><li><p>Bauen Sie Ihren Job mit Maven als JAR, um ihn auf dem Cluster auszuführen zu können (siehe Aufgabe 0)</p></li><li><p>Um die Anzahl der Vorkommen von einem Wort mit einem reduce zu zählen, kann ein zusammengesetzter Datentyp verwendet werden, der neben dem Wort auch einen Zähler enthält</p></li></ul></div></section>
<section id="datastream_api_festlegen_des_parallelismus"><h2>DataStream API : Festlegen des Parallelismus</h2><div class="ulist"><ul><li><p>Setzen des <strong>Parallelismus</strong> für einen <strong>Operator</strong> :</p><div class="ulist"><ul><li><p>wende die Methode <strong><em>setParallelism(<span class="blue-font">parallelism</span>)</em></strong> auf einen Stream nach Anwenden des Operators an</p><div class="ulist"><ul><li><p><span class="blue-font">parallelism</span> ist hier eine positive ganze Zahl (1 für nicht-parallel)</p></li></ul></div></li></ul></div></li><li><p>Setzen des Parallelismus für einen <strong>Job</strong> :</p><div class="ulist"><ul><li><p>wende anfänglich die Methode <strong><em>setParallelism(<span class="blue-font">parallelism</span>)</em></strong> auf das StreamExecutionEnvironment an</p></li></ul></div></li><li><p>Setzen des Parallelismus für eine <strong>Anwendung</strong> :</p><div class="ulist"><ul><li><p>führe den Befehl "flink run" mit Parameter "-p <span class="blue-font">parallelism</span>" aus</p></li></ul></div></li><li><p>Setzen des Parallelismus für alle Jobs auf einen <strong>Cluster</strong> :</p><div class="ulist"><ul><li><p>Setzen des Parameters <em>parallelism.default</em> in conf/flink-conf.yaml (Default : 1)</p></li></ul></div></li><li><p><strong>Speziellere Einstellungen überschreiben (wenn vorhanden) immer die allgemeineren Defaults</strong></p></li></ul></div></section>
<section id="datastream_api_operator_chaining"><h2>DataStream API : Operator Chaining</h2><div class="ulist"><ul><li><p>Flink fasst per Default <strong>automatisch</strong> Operatoren mit einer <strong>Forward</strong>-Verbindung zu einer <strong>Kette</strong> zusammen (<strong>operator chaining</strong>)</p></li><li><p>Über die API lässt sich dieses Verhalten folgendermaßen einschränken:</p><div class="ulist"><ul><li><p>Anwendung der Methode <strong><em>disableOperatorChaining()</em></strong> auf dem StreamExecutionEnvironment deaktiviert operator chaining komplett</p></li><li><p>Anwendung der Methode <strong><em>disableChaining()</em></strong> auf einen Stream nach Anwendung eines Operators deaktiviert operator chaining für diesen Operator</p><div class="ulist"><ul><li><p>&#8594; dieser Operator erhält immer einen eigenen Task</p></li></ul></div></li><li><p>Anwendung von <strong><em>startNewChain()</em></strong> nach einem Operator verhindert, dass dieser mit seinen <strong>upstream</strong> Operatoren verkettet wird</p><div class="ulist"><ul><li><p>statt dessen wird er nur (soweit möglich) mit seinen <strong>downstream</strong> Operatoren verkettet</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="aufgabe_3_1"><h2>Aufgabe 3 (1)</h2><div class="olist arabic"><ol class="arabic"><li><p>Starten Sie Ihren Job aus Aufgabe 2 noch einmal und sehen Sie sich in der Web UI (Dashboard) an, welche Operatoren von Flink verkettet wurden und mit welchem Parallelismus die
Tasks ausgeführt wurden</p><div class="ulist"><ul><li><p>Warum wurde die Verkettung von Flink in der Weise gewählt?</p></li></ul></div></li><li><p>Starten Sie den Job erneut, aber mit einem Parallelismus von 4</p><div class="ulist"><ul><li><p>Sie erhalten wahrscheinlich eine Fehlermeldung über nicht vorhandene Ressourcen, da Ihr Cluster nicht automatisch weitere TaskManager starten kann</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>Stoppen Sie alle vorhandenen TaskManager über die Konsole</p></li><li><p>Setzen Sie in der Datei conf/flink-conf.yaml den Parameter <em>taskmanager.numberOfTaskSlots</em> auf 2</p></li><li><p>Starten Sie 2 neue TaskManager</p></li><li><p>Verifizieren Sie im Dashboard, dass es nun genau 2 TaskManager mit je 2 Slots gibt</p></li><li><p>Starten Sie den Job erneut</p></li></ol></div></li></ul></div></li><li><p>Sehen Sie sich den Job wiederum im Dashboard an</p></li></ol></div></section>
<section id="aufgabe_3_2"><h2>Aufgabe 3 (2)</h2><div class="olist arabic"><ol class="arabic" start="4"><li><p>Sehen Sie sich Art und Reihenfolge der Ausgaben an</p></li><li><p>Bewirken Sie durch geeignete Zuordnung von Keys, dass Wörter mit gleichen Anfangsbuchstaben von der gleichen Sink-Instanz bearbeitet werden</p><div class="ulist"><ul><li><p>Führen Sie den Job erneut aus und sehen Sie sich die Ausgabe an</p></li></ul></div></li><li><p>Modifizieren Sie nun den Job, sodass die Sink mit Parallelismus 1 arbeitet und führen ihn erneut (mit Parallelismus 4) aus</p><div class="ulist"><ul><li><p>Vergleichen Sie wieder die Ausgabe</p></li></ul></div></li><li><p>Probieren Sie aus, wie sich die Anzeige ihres JobGraph im Dashboard verändert, wenn Sie den Parallelismus eines der Operatoren auf 2 setzen oder
das <strong>operator chaining</strong> eines Operators über die API unterbinden</p></li></ol></div></section>
<section id="datastream_api_processfunction"><h2>DataStream API : ProcessFunction</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/ProcessFunction.html">ProcessFunctions</a></strong> sind ein Grundbaustein der DataStream API, die es ermöglichen, <strong>beliebige</strong> Operatoren auf <strong>niedriger Abstraktionsebene</strong> zu implementieren</p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/KeyedProcessFunction.html">KeyedProcessFunction</a></strong> sind ein Spezialfall für KeyedStreams</p></li></ul></div></li><li><p>Anwenden einer ProcessFunction auf einen Stream geschieht über die Methode <strong><em><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#process-org.apache.flink.streaming.api.functions.ProcessFunction-">process(processFunction)</a></em></strong></p></li><li><p>Die folgende Methode wird für jeden Datensatz einmal aufgerufen:</p></li></ul></div>
<pre class="CodeRay listingblock"><code>public abstract void processElement(I input, Context context, Collector&lt;O&gt; output)</code></pre>
<div class="ulist"><ul><li><p>Ähnlich wie bei flatMap können mit dem <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/util/Collector.html">Collector</a></strong> auch mehrere Elemente emittiert werden</p></li><li><p>zusätzlich steht ein <strong>Context</strong> zu Verfügung, der u.a. Zugriff auf Timestamp und Key (für KeyedProcessFunction) des Elements ermöglicht</p></li></ul></div></section>
<section id="datastream_api_processfunction_2"><h2>DataStream API : ProcessFunction (2)</h2><div class="ulist"><ul><li><p>Beispiel: Ein Filter als ProcessFunction:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">MyLengthFilterFunction</span> <span class="directive">extends</span> ProcessFunction&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; {

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> processElement(<span class="predefined-type">String</span> input, <span class="predefined-type">Context</span> context, Collector&lt;<span class="predefined-type">String</span>&gt; collector)  {
        <span class="keyword">if</span> (input.length() &lt; <span class="integer">5</span>) {
            collector.collect(input);
        }
    }
}</code></pre>
<div class="ulist"><ul><li><p>Anwendung auf einen Stream:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
DataStream&lt;<span class="predefined-type">String</span>&gt; myStream = env.fromElements(<span class="string"><span class="delimiter">&quot;</span><span class="content">Test</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">TestLang</span><span class="delimiter">&quot;</span></span>);
myStream.process(<span class="keyword">new</span> MyLengthFilterFunction())
                .print();  <span class="comment">// &quot;Test&quot;</span></code></pre></section>
<section id="datastream_api_stateful_processfunction"><h2>DataStream API : Stateful ProcessFunction</h2><div class="ulist"><ul><li><p>Um in einer ProcessFunction einen <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/State.html">State</a></strong> nutzen zu können, müssen wir die <strong><em>open</em></strong> Methode überschreiben, um den State initial zu <strong>registrieren</strong></p><div class="ulist"><ul><li><p>Diese kommt aus dem allgemeineren <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/functions/RichFunction.html">RichFunction</a> Interface</p></li></ul></div></li><li><p>Beispiel einer KeyedProcessFunction mit <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/ValueState.html">ValueState</a>, die Datensätze nach Keys zählt und mit Zähler ausgibt (Aggregation):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">CountByKeyFunction</span>&lt;K, I&gt; <span class="directive">extends</span> KeyedProcessFunction&lt;K,I, Tuple2&lt;I, <span class="predefined-type">Long</span>&gt;&gt; {

    <span class="directive">private</span> ValueState&lt;<span class="predefined-type">Long</span>&gt; state;

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> open(<span class="predefined-type">Configuration</span> parameters)  {
        state = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string"><span class="delimiter">&quot;</span><span class="content">countState</span><span class="delimiter">&quot;</span></span>, <span class="predefined-type">Long</span>.class));
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> processElement(I input, <span class="predefined-type">Context</span> context, Collector&lt;Tuple2&lt;I, <span class="predefined-type">Long</span>&gt;&gt; collector) <span class="directive">throws</span> <span class="exception">Exception</span> {
        <span class="predefined-type">Long</span> value = state.value();
        <span class="predefined-type">Long</span> newValue = value == <span class="predefined-constant">null</span> ? <span class="integer">1</span> : value + <span class="integer">1</span>;
        state.update(newValue);
        collector.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input, newValue));
    }
}</code></pre></section>
<section id="datastream_api_processfunction_state_typen"><h2>DataStream API : ProcessFunction State Typen</h2><div class="ulist"><ul><li><p>Neben <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/ValueState.html">ValueState&lt;T&gt;</a></strong> können wir in einer ProcessFunction auch die anderen im letzten Kapitel dargestellten Arten von State verwenden:</p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/ListState.html">ListState&lt;T&gt;</a></strong></p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/ReducingState.html">ReducingState&lt;T&gt;</a></strong> ist eine Variation, bei der mit jedem neuen Eintrag automatisch eine ReduceFunction angewendet wird, um einen einzelnen Ergebniswert zu updaten</p></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/AggregatingState.html">AggregatingState&lt;IN,OUT&gt;</a></strong> ist ähnlich, aber ermöglicht allgemeinere Rolling Aggregations</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/MapState.html">MapState&lt;UK, UV&gt;</a></strong></p></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/BroadcastState.html">BroadcastState&lt;K,V&gt;</a></strong></p><div class="ulist"><ul><li><p>für einen <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/BroadcastStream.html">BroadcastStream</a></strong>, der wiederum mit der <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#broadcast--"><em>broadcast</em></a></strong> Methode eines DataStreams erzeugt wurde</p></li></ul></div></li><li><p><strong>Union List State</strong> hat keinen eigenen Typ, sondern wird als <strong>Pattern</strong> für vereinigte Streams (<em>union</em> Operation, s.u.) mit ListStates verwendet</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_distributionstransformationen"><h2>DataStream API : Distributionstransformationen</h2><div class="ulist"><ul><li><p><strong>Datenaustauschstrategien</strong> sind in Flink über sog. <strong>Distributionstransformationen</strong> konfigurierbar</p><div class="ulist"><ul><li><p>die Einstellung wird in den meisten Fällen besser von Flink automatisch gehandhabt</p></li></ul></div></li><li><p>Die folgenden Methoden können auf DataStreams <strong>nach Anwendung eines Operators</strong> aufgerufen werden, um zu <strong>kontrollieren, wie dieser Operator seinen Output auf seine downstream Operatoren verteilt</strong>:</p><div class="ulist"><ul><li><p><strong>shuffle()</strong> : entspricht random Strategie</p></li><li><p><strong>rebalance()</strong> : round-robin (gleichmäßig)</p></li><li><p><strong>rescale()</strong> : round-robin, aber jede Operatorinstanz schickt ihre Daten nur an eine gewisse Teilmenge der Instanzen von downstream Operatoren</p><div class="ulist"><ul><li><p>die Teilmengen werden von Flink so gewählt, dass sich eine effiziente Verarbeitung gibt</p></li><li><p>für den Fall gedacht, dass downstream Operatoren einen höheren Parallelismus haben (am besten ein Vielfaches)</p></li></ul></div></li><li><p><strong>broadcast()</strong> : entspricht broadcast Strategie</p></li><li><p><strong>global()</strong> : sendet alle Outputs an nur den ersten downstream Task</p></li><li><p><strong>partitionCustom(partitioner, keySelector)</strong> : partitioniert Output nach vorgegebenen Schema</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_distributionstransformation_partitioncustom"><h2>DataStream API : Distributionstransformation partitionCustom</h2><div class="ulist"><ul><li><p><strong>partitionCustom</strong></p><div class="ulist"><ul><li><p>ähnlich wie keyBy</p><div class="ulist"><ul><li><p>erfordert Angabe eines <strong>KeySelector</strong>, um einen Key aus einem Datensatz zu generieren</p></li><li><p>zusätzlich ein <strong>Partitioner</strong>, um aus einem Key und einer Anzahl von Partitionen eine Partitionsnummer zu bestimmen</p></li></ul></div></li><li><p>Unterschiede zu keyBy</p><div class="ulist"><ul><li><p>low-level</p></li><li><p>physische, aber keine logische Partitionierung : produziert aus einem DataStream wieder einen DataStream, keinen KeyedStream</p></li><li><p>erlaubt Nummer des Empfängers eines Datensatzes über den Key direkter zu steuern</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_multistream_transformationen_und_side_output"><h2>DataStream API : Multistream Transformationen und Side Output</h2><div class="ulist"><ul><li><p>Bisher haben wir nur gelernt, wie wir eine lineare <strong>Pipeline</strong> als Flink-Job umsetzen können</p></li><li><p>Um beliebige Job-Graphen zu realisieren, benötigen wir eine Möglichkeit, für einen Operator <strong>mehrere andere Operatoren</strong> als Quelle seines <strong>Inputs</strong> oder Ziel seines <strong>Outputs</strong> festzulegen</p></li><li><p>Hierfür stellt die API die Features <strong>Multistream Transformationen</strong> (mehrere Inputs) und <strong>Side Output</strong> (mehrere Outputs) zu Verfügung</p></li></ul></div></section>
<section id="datastream_api_multistream_transformationen"><h2>DataStream API : Multistream Transformationen</h2><div class="ulist"><ul><li><p>Der Fall, dass ein Operator <strong>mehr als einen Inputstream</strong> benötigt, wird in der API so gehandhabt, dass die Inputstreams zunächst mit gewissen Transformationen zu einem Stream <strong>zusammengefasst</strong> werden</p></li><li><p>Hierfür stehen mehrere Transformationen auf DataStreams zur Verfügung:</p><div class="ulist"><ul><li><p><strong>stream.union(otherstream1, otherstream2, ..)</strong></p><div class="ulist"><ul><li><p>leitet alle Elemente in den vereinigten DataStreams in einen einzelnen DataStream (Resultat der Operation) weiter</p></li><li><p>Die Typen der Streams müssen identisch sein</p></li><li><p>ein Stream darf mehrfach vorkommen; in dem Fall kommen die Elemente dieses Streams im Resultatstream mehrfach vor</p></li><li><p>Über die Reihenfolge der Elemente aus verschiedenen Inputstreams im Resultat gibt es keine Garantien</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_multistream_transformationen_2"><h2>DataStream API : Multistream Transformationen (2)</h2><div class="ulist"><ul><li><p><strong>stream.connect(otherStream)</strong></p><div class="ulist"><ul><li><p>ermöglicht das Kombinieren von Streams mit unterschiedlichen Typen</p></li><li><p>liefert aus einem <strong>DataStream&lt;S&gt;</strong> und einem <strong>DataStream&lt;T&gt;</strong> ein Object vom Typ <strong>ConnectedStreams&lt;S,T&gt;</strong></p></li><li><p>dies ist selbst <strong>kein</strong> DataStream, ermöglicht aber die Anwendung der Methoden <em>map</em>, <em>flatMap</em>, <em>keyBy</em> und <em>process</em></p></li><li><p>diese Methoden funktionieren wie die von DataStream, benötigen aber spezielle Funktionsobjekte, die Elemente von beiden Typen verarbeiten können</p></li><li><p>Beispiel :</p><div class="ulist"><ul><li><p><em>map</em> auf einem <strong>ConnectedStream&lt;S,T&gt;</strong> benötigt eine <strong>CoMapFunction&lt;S,T,U&gt;</strong> und erzeugt einen <strong>DataStream&lt;U&gt;</strong></p></li></ul></div></li></ul></div></li><li><p><strong>stream.join(..)</strong> und <strong>stream.cogroup(..)</strong> kombinieren zusätzlich Windows (dazu später mehr)</p></li></ul></div></section>
<section id="datastream_api_multistream_transformationen_beispiel"><h2>DataStream API : Multistream Transformationen Beispiel</h2><div class="ulist"><ul><li><p>Beispiel für einen Operator mit 2 Inputs:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">GenerateLogFunction</span> <span class="directive">implements</span> CoMapFunction&lt;OrderEvent, UserUpdateEvent, <span class="predefined-type">String</span>&gt; {

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="predefined-type">String</span> map1(OrderEvent event) {
        <span class="keyword">return</span> MyUtils.generateLogForOrderEvent(event);
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="predefined-type">String</span> map2(UserUpdateEvent event)  {
        <span class="keyword">return</span> MyUtils.generateLogForUserUpdateEvent(event);
    }
}

<span class="comment">// in main Method:</span>

DataStream&lt;OrderEvent&gt; orders = env.fromSource(ordersSource);
DataStream&lt;UserUpdateEvent&gt; updates = env.fromSource(updatesSource);
ConnectedStreams&lt;OrderEvent, UserUpdateEvent&gt; ordersAndUpdates = orders.connect(updates);

ordersAndUpdates.map(<span class="keyword">new</span> LoggingCoMapFunction())
                .sinkTo(<span class="keyword">new</span> LoggingSink());</code></pre></section>
<section id="datastream_api_side_output"><h2>DataStream API : Side Output</h2><div class="ulist"><ul><li><p><strong>Side Outputs</strong> ermöglichen es, den Output von einem Stream <strong>an mehrere verschiedene downstream Operatoren</strong> weiterzuleiten</p></li><li><p>Ein Operator kann beliebig viele Side Output Streams generieren und sie mit Daten von beliebigen Datentypen befüllen</p><div class="ulist"><ul><li><p>jeder Side Output Stream erhält einen <strong>identifizierenden Tag</strong></p></li></ul></div></li><li><p>Side Output kann <strong>nur</strong> unter Verwendung der <strong>Process Function API</strong> generiert werden</p></li></ul></div></section>
<section id="datastream_api_side_output_beispiel"><h2>DataStream API : Side Output Beispiel</h2><div class="ulist"><ul><li><p>Beispiel eines Operators mit 2 Outputs:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">SideOutputExample</span> {
    <span class="directive">private</span> <span class="directive">static</span> <span class="directive">final</span> OutputTag&lt;MyEvent&gt; rejectedEventsTag = <span class="keyword">new</span> OutputTag&lt;MyEvent&gt;(<span class="string"><span class="delimiter">&quot;</span><span class="content">rejected</span><span class="delimiter">&quot;</span></span>) {};

    <span class="directive">public</span> <span class="directive">static</span> <span class="type">void</span> main(<span class="predefined-type">String</span><span class="type">[]</span> args) {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream&lt;MyEvent&gt; allEvents = env.fromSource(MySourceUtil.createEventSource()); <span class="comment">// Erstellung der Quelle nicht gezeigt</span>
        DataStream&lt;MyEvent&gt; acceptedEvents = allEvents.process(<span class="keyword">new</span> MyAuditingFunction(rejectedEventsTag));
        DataStream&lt;MyEvent&gt; rejectedEvents = acceptedEvents.getSideOutput(rejectedEventsTag);
        acceptedEvents.sinkTo(<span class="keyword">new</span> AcceptedEventsSink());  <span class="comment">// Akzeptierte Events verarbeiten</span>
        rejectedEvents.sinkTo(<span class="keyword">new</span> RejectedEventsSink());  <span class="comment">// Abgelehnte Events Loggen</span>
        env.execute();
    }
    <span class="directive">private</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">MyAuditingFunction</span> <span class="directive">extends</span> ProcessFunction&lt;MyEvent, MyEvent&gt; {
        <span class="directive">private</span> OutputTag&lt;MyEvent&gt; outputTag;

        <span class="directive">public</span> OrderProcessFunction(OutputTag&lt;MyEvent&gt; outputTag) {
            <span class="local-variable">this</span>.outputTag = outputTag;
        }

        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="type">void</span> processElement(MyEvent event, <span class="predefined-type">Context</span> context, Collector&lt;MyEvent&gt; collector)  {
            <span class="keyword">if</span> (MyUtils.checkEvent(event)) {
                collector.collect(event);  <span class="comment">// Event akzeptieren</span>
            } <span class="keyword">else</span> {
                context.output(outputTag, event);  <span class="comment">// Event ablehnen</span>
            }
        }
    }
}</code></pre></section>
<section id="datastream_api_iterate"><h2>DataStream API : Iterate</h2><div class="ulist"><ul><li><p>Es folgt eine <strong>optionale</strong> Betrachtung des Features <strong>iterative Streams</strong></p><div class="ulist"><ul><li><p>Feature ist ab der Version Flink 1.19 deprecated</p></li></ul></div></li><li><p>Anwendung eines <strong>iterativen Algorithmus</strong> auf einen Stream:</p><div class="ulist"><ul><li><p>Es wird wiederholt eine Folge von Transformationen auf Elemente des Streams angewendet, bis das Resultat eine bestimmte Bedingung erfüllt</p></li></ul></div></li><li><p>Ein <strong>IterativeStream&lt;T&gt;</strong> wird durch Anwendung der <strong><em>iterate</em></strong> Methode auf einem <strong>DataStream&lt;T&gt;</strong> erzeugt</p></li><li><p>Dann können auf diesen Stream <strong>Transformationen</strong> und <strong>Filter</strong> angewendet werden, die bei jeder Iteration passieren sollen</p></li><li><p>Der resultierende Stream wird an die Iteration über <strong>iterativeStream.closeWith(transformedStream)</strong> zurückgegeben</p></li><li><p>Der resultierende Stream emittiert nach jeder Iteration seinen Output an seine downstream Operatoren</p></li></ul></div></section>
<section id="datastream_api_iterate_beispiel"><h2>DataStream API : Iterate (Beispiel)</h2><div class="ulist"><ul><li><p>Beispiel Iterate:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">IterativeStream&lt;<span class="predefined-type">Long</span>&gt; iteration = initialStream.iterate();  <span class="comment">// Erstellung eines iterativen Streams</span>
DataStream&lt;<span class="predefined-type">Long</span>&gt; iterationBody = iteration.map (iterativeFunction);  <span class="comment">// Iterationsschritt</span>
DataStream&lt;<span class="predefined-type">Long</span>&gt; feedback = iterationBody.filter(value -&gt; value &gt; <span class="integer">0</span>); <span class="comment">// Definition der Bedingung, bei der nicht abgebrochen wird</span>
iteration.closeWith(feedback); <span class="comment">// Einstellen des Streams der Daten, die weiter bearbeitet werden sollen</span>
DataStream&lt;<span class="predefined-type">Long</span>&gt; output = iterationBody.filter(value -&gt; value &lt;= <span class="integer">0</span>); <span class="comment">// Output extrahieren, wenn Abbruchbedingung erfüllt</span></code></pre>
<div class="ulist"><ul><li><p>Entspricht konzeptuell einer Schleife der Form (Pseudocode) :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">value = initialStream.getNext();
<span class="keyword">do</span> {
    value = iterativeFunction(value);
    <span class="keyword">if</span> (value &lt;= <span class="integer">0</span>) {
        output.add(value);
     }
   }
<span class="keyword">while</span> (value &gt; <span class="integer">0</span>);</code></pre></section>
<section id="aufgabe_4_1"><h2>Aufgabe 4 (1)</h2><div class="ulist"><ul><li><p>Wir erstellen einen Flink Job, der einen <strong>Low-Level Join</strong> von 2 Inputquellen ausführt, um Daten <strong>anzureichern</strong> und dann 2 verschiedene Outputs zu generieren</p><div class="ulist"><ul><li><p>Objekte:</p><div class="ulist"><ul><li><p>Kunden, die einen Namen und eine ID haben</p></li><li><p>Transaktion eines Kunden mit Kunden-ID und einem Zahlwert</p></li></ul></div></li></ul></div></li></ul></div>
<div class="olist arabic"><ol class="arabic" start="1"><li><p>Generieren Sie 3 Inputstreams :</p><div class="ulist"><ul><li><p>Einer enthält die Kunden</p></li><li><p>Zwei Streams enthalten Transaktionen</p><div class="ulist"><ul><li><p>Dies sollten <strong>KeyedStreams</strong> mit der ID als Key werden</p></li></ul></div></li></ul></div></li><li><p>Die letzten beiden Streams stellen äquivalente Inputs aus verschiedenen Quellen dar und sollten mit <strong>union</strong> kombiniert werden</p></li></ol></div></section>
<section id="aufgabe_4_2"><h2>Aufgabe 4 (2)</h2><div class="olist arabic"><ol class="arabic" start="3"><li><p>Definieren Sie dann mit der ProcessFunction API einen <strong>stateful</strong> Operator, der auf der <strong>Kombination</strong> von den Kunden- und Transaktionsstreams agiert</p><div class="ulist"><ul><li><p>die Klasse des Operators sollte <strong>KeyedCoProcessFunction</strong> erweitern</p><div class="ulist"><ul><li><p>Der Operator sollte einen <strong>KeyedState</strong> haben, um die <strong>Zuordnungen</strong> von ID zu Namen zu speichern, wenn ein Datensatz aus dem Kundenstream gelesen wird</p></li></ul></div></li><li><p>Datensätze aus dem Kundenstream erzeugen kein Output</p><div class="ulist"><ul><li><p>für jede erhaltene <strong>Transaktion</strong> sollen 2 Outputs generiert werden:</p></li></ul></div></li><li><p>Ein Output der Art "ID:Betrag"</p></li><li><p>ein Output der Art "Name:Betrag"</p><div class="ulist"><ul><li><p>wenn noch kein Name zu dieser ID gespeichert wurde, zeige einen Platzhalter</p></li></ul></div></li><li><p>diese Outputs können Sie z.B. in 2 verschiedene Dateien schreiben lassen</p></li></ul></div></li><li><p>Testen Sie ihren Job, indem Sie ihn auf einem Cluster ausführen, und verifizieren Sie die Ausgabe</p></li></ol></div></section>
<section id="datastream_api_watermark_strategy"><h2>DataStream API : Watermark Strategy</h2><div class="ulist"><ul><li><p>Bei der Einbindung einer <strong>Source</strong> in das StreamExecutionEnvironment, lässt sich eine <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/WatermarkStrategy.html">WatermarkStrategy</a></strong> angeben:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStreamSource&lt;<span class="predefined-type">String</span>&gt; stream = env.fromSource(source, &lt;watermark strategy&gt;, <span class="string"><span class="delimiter">&quot;</span><span class="content">SourceName</span><span class="delimiter">&quot;</span></span>);</code></pre>
<div class="ulist"><ul><li><p>alternativ kann auch nach der Anwendung eines beliebigen <strong>Operators</strong> mit der Methode <em><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#assignTimestampsAndWatermarks-org.apache.flink.api.common.eventtime.WatermarkStrategy-">assignTimestampsAndWatermarks</a></em> eine WatermarkStrategy übergeben,
die dann auf den Output des Operators angewendet wird</p></li><li><p>Beispiel der Erstellung einer WatermarkStrategy basierend auf einem <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/WatermarkGenerator.html">WatermarkGenerator</a></strong> und einem <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/TimestampAssigner.html">TimestampAssigner</a></strong>:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">WatermarkStrategy&lt;T&gt; myStrategy =
        WatermarkStrategy&lt;T&gt;.forGenerator(context -&gt; <span class="keyword">new</span> MyWatermarkGenerator())
                            .withTimestampAssigner(context -&gt; <span class="keyword">new</span> MyTimeStampAssigner());</code></pre></section>
<section id="datastream_api_watermarkgenerator"><h2>DataStream API : WatermarkGenerator</h2><div class="ulist"><ul><li><p>Um einen eigenen <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/WatermarkGenerator.html">WatermarkGenerator&lt;T&gt;</a></strong> zu erstellen, sind folgende Methoden implementieren:</p><div class="ulist"><ul><li><p><em>void onEvent(T event, long eventTimestamp, WatermarkOutput output)</em></p></li><li><p><em>void onPeriodicEmit(WatermarkOutput output)</em></p><div class="ulist"><ul><li><p>diese Methode wird von Flink in regelmäßigen Abständen aufgerufen</p></li><li><p>Die Länge des periodischen Intervalls ist über die <strong>ExecutionConfig</strong> des StreamExecutionEnvironment einstellbar (Default: 200ms)</p></li></ul></div></li></ul></div></li><li><p>Diese beiden Methoden ermöglichen es, Watermarks als Reaktion auf bestimmte Ereignisse (<strong>punctuated</strong>) oder in regelmäßigen zeitlichen Abständen (<strong>periodic</strong>) zu erstellen</p></li><li><p>innerhalb dieser Methoden kann eine Watermark auf folgenden Befehl emittiert werden :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">output.emitWatermark(<span class="keyword">new</span> Watermark(&lt;time in millis&gt;));</code></pre></section>
<section id="datastream_api_beispiel_boundedoutofordernessgenerator"><h2>DataStream API : Beispiel BoundedOutOfOrdernessGenerator</h2><div class="ulist"><ul><li><p><strong>Beispiel <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/BoundedOutOfOrdernessWatermarks.html">BoundedOutOfOrdernessGenerator</a></strong> (vereinfacht):</p><div class="ulist"><ul><li><p>Aktueller Timestamp als State</p></li><li><p>OnEvent den aktuellen Timestamp aktualisieren</p></li><li><p>OnPeriodicEmit eine Watermark mit dem gespeicherten Timestamp plus Puffer für zu späte Ereignisse emittieren</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">BoundedOutOfOrdernessGenerator</span> <span class="directive">implements</span> WatermarkGenerator&lt;MyEvent&gt; {

    <span class="directive">private</span> <span class="directive">final</span> <span class="type">long</span> maxOutOfOrderness = <span class="integer">3500</span>; <span class="comment">// 3.5 seconds</span>

    <span class="directive">private</span> <span class="type">long</span> currentMaxTimestamp;

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> onEvent(MyEvent event, <span class="type">long</span> eventTimestamp, WatermarkOutput output) {
        currentMaxTimestamp = <span class="predefined-type">Math</span>.max(currentMaxTimestamp, eventTimestamp);
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> onPeriodicEmit(WatermarkOutput output) {
        <span class="comment">// emit the watermark as current highest timestamp minus the out-of-orderness bound</span>
        output.emitWatermark(<span class="keyword">new</span> Watermark(currentMaxTimestamp - maxOutOfOrderness - <span class="integer">1</span>));
    }
}</code></pre>
<div class="paragraph center small"><small><em>(Code aus: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/</a>)</em></small></div></section>
<section id="datastream_api_timestampassigner"><h2>DataStream API : TimestampAssigner</h2><div class="ulist"><ul><li><p>Um einen eigenen <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/TimestampAssigner.html">TimestampAssigner&lt;T&gt;</a></strong> zu erstellen, ist folgende Methode zu implementieren:</p><div class="ulist"><ul><li><p><em>long extractTimestamp(T element, long recordTimestamp)</em></p><div class="ulist"><ul><li><p><em>recordTimestamp</em> enthält den ggf. bereits vorhandenen Timestamp</p></li><li><p>Returnwert ist der neu zu vergebende Timestamp</p></li></ul></div></li><li><p>Diese Methode wird dann von Flink auf jeden Datensatz einmal angewendet</p></li></ul></div></li><li><p>Beispiel für TimestampAssigner:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">MyTimestampAssigner</span> <span class="directive">implements</span> TimestampAssigner&lt;MyEvent&gt; {

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">long</span> extractTimestamp(T element, <span class="type">long</span> recordTimestamp) {
        <span class="keyword">if</span> (recordTimestamp != TimestampAssigner.NO_TIMESTAMP) {
            <span class="keyword">return</span> recordTimestamp; <span class="comment">// Timestamp wurde schon vergeben -&gt; nehme diesen</span>
        }
        <span class="keyword">return</span> MyUtil.extractTimestamp(element); <span class="comment">// Extrahiere Timestamp neu aus dem Datensatz</span>
    }
}</code></pre></section>
<section id="datastream_api_vordefinierte_watermarkstrategy"><h2>DataStream API : Vordefinierte WatermarkStrategy</h2><div class="ulist"><ul><li><p><strong><em>WatermarkStrategy.noWatermarks()</em></strong> :</p><div class="ulist"><ul><li><p>es werden keine Watermarks erstellt</p></li></ul></div></li><li><p><strong><em>WatermarkStrategy.forBoundedOutOfOrderness(maxOutOfOrderness)</em></strong> :</p><div class="ulist"><ul><li><p>Verwendet BoundedOutOfOrdernessGenerator (s.o.) :</p><div class="ulist"><ul><li><p>es werden periodisch Watermarks emittiert, die als Timestamp den maximalen Timestamp unter den bisher gesehenen Streamelementen plus <em>maxOutOfOrderness</em> haben</p></li><li><p><em>maxOutOfOrderness</em> gibt eine Toleranz an, wie lange (event time) auf verspätete Datensätze gewartet werden soll</p></li></ul></div></li></ul></div></li><li><p><strong><em>WatermarkStrategy.forMonotonousTimestamps()</em></strong> :</p><div class="ulist"><ul><li><p>das gleiche wie WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofMillis(0L))</p></li><li><p>für den Fall gedacht, wenn die Datensätze garantiert in-order ankommen</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_konfiguration_von_watermarkstrategy"><h2>DataStream API : Konfiguration von Watermarkstrategy</h2><div class="ulist"><ul><li><p>Eine erstellte <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/eventtime/WatermarkStrategy.html">WatermarkStrategy</a> <em>strategy</em> kann noch zusätzlich <strong>konfiguriert</strong> werden:</p><div class="ulist"><ul><li><p><strong><em>strategy.withIdleness(&lt;duration&gt;)</em></strong></p><div class="ulist"><ul><li><p>erreicht, dass sich der Stream selbst als <strong>idle</strong> deklariert, wenn für die angegebene Zeitlänge keine neuen Datensätze erschienen sind</p></li><li><p>verhindert, dass Watermark Propagation downstream blockiert wird (vgl. <strong>idle stream problem</strong>)</p></li></ul></div></li><li><p><strong><em>strategy.withWatermarkAligment("alignment-groupname", &lt;maxDrift&gt;, &lt;updateFrequency&gt;)</em></strong></p><div class="ulist"><ul><li><p>stellt Watermark Alignment ein (vgl. vorheriges Kapitel)</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_operators"><h2>DataStream API : Window Operators</h2><div class="ulist"><ul><li><p>Bei der Definition eines <strong>Window Operators</strong> ist Folgendes anzugeben:</p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/WindowAssigner.html">Window Assigner</a></strong> : legt fest, wie Elemente im Stream den Fenstern zugeordnet werden</p></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/windowing/WindowFunction.html">Window Function</a></strong> : eine Operator-Funktion, die mit windowed Streams arbeiten kann</p></li><li><p>Es muss entschieden werden, ob <strong>Keys</strong> verwendet werden sollen</p><div class="ulist"><ul><li><p>Falls ja, werden die Datensätze in einem Window für jeden Key separat bearbeitet (<strong>Keyed Windows</strong>)</p></li></ul></div></li></ul></div></li><li><p><strong>Optional</strong> können weitere <strong>Komponenten</strong> angegeben werden:</p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.html">Trigger</a></strong> : enthält eine Bedingung dafür, wann eine Bearbeitung eines Windows (Generierung von Output) stattfinden soll</p><div class="ulist"><ul><li><p>wenn nicht angegeben, wird der Default des WindowAssigners genommen</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/evictors/Evictor.html">Evictor</a></strong> : kann Datensätze vor der abschließenden Bearbeitung eines Windows entfernen</p></li><li><p>Eine <strong>Konfiguration</strong> für den Umgang mit <strong>verspäteten</strong> Datensätzen</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_operators_2"><h2>DataStream API : Window Operators (2)</h2><div class="ulist"><ul><li><p>Definition eines Window Operator im Code:</p></li><li><p>Mit Keyed Windows</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">stream
      .keyBy(keyAssigner)            <span class="comment">// required</span>
      .window(windowAssigner)        <span class="comment">// required</span>
      .trigger(trigger)              <span class="comment">// optional (else default trigger)</span>
      .evictor(evictor)              <span class="comment">// optional (else no evictor)</span>
      .allowedLateness(lateness)     <span class="comment">// optional (else zero)</span>
      .sideOutputLateData(outputTag) <span class="comment">// optional (else no side output for late data)</span>
       apply(windowFunction)         <span class="comment">// required; alternativ : .aggregate(aggregateFunction) oder .reduce(reduceFunction)</span>
      .getSideOutput(outputTag)      <span class="comment">// optional</span>
       <span class="comment">// .. Output verwenden</span></code></pre>
<div class="ulist"><ul><li><p>Non-Keyed Windows</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">stream
       .windowAll(windowAssigner)      <span class="comment">//  required</span>
       .trigger(trigger)               <span class="comment">//  optional (else default trigger)</span>
       .evictor(evictor)               <span class="comment">//  optional (else no evictor)</span>
       .allowedLateness(lateness)      <span class="comment">//  optional (else zero)</span>
       .sideOutputLateData(outputTag)  <span class="comment">//  optional: (else no side output for late data)</span>
       .apply(allWindowFunction)       <span class="comment">//  required; alternativ : .aggregate(aggregateFunction) oder .reduce(reduceFunction)</span>
       .getSideOutput(outputTag)       <span class="comment">//  optional</span>
        <span class="comment">// .. Output verwenden</span></code></pre>
<div class="ulist small"><ul><li><p>siehe auch <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a></p></li></ul></div></section>
<section id="datastream_api_window_assigners"><h2>DataStream API : Window Assigners</h2><div class="ulist"><ul><li><p>Ein <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/WindowAssigner.html">WindowAssigner</a> kann individuell definiert werden, indem die abstrakte Klasse * <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/WindowAssigner.html">WindowAssigner&lt;T,W extends Window&gt;</a>* implementiert wird</p></li><li><p>für die häufigsten Use-Cases gibt es <strong>vordefinierte Assigner</strong>:</p><div class="ulist"><ul><li><p>Tumbling Windows</p><div class="ulist"><ul><li><p>Parameter : Window Size</p></li><li><p>relevante Klassen: <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/TumblingEventTimeWindows.html">TumblingEventTimeWindows</a></strong>, <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/TumblingProcessingTimeWindows.html">TumblingProcessingTimeWindows</a></strong></p></li></ul></div></li><li><p>Sliding Windows</p><div class="ulist"><ul><li><p>Parameter : Window Size, Slide (Slide legt fest, in welchem Zeitintervall ein neues Window gestartet wird)</p></li><li><p>relevante Klassen: <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/SlidingEventTimeWindows.html">SlidingEventTimeWindows</a></strong>, <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/SlidingProcessingTimeWindows.html">SlidingProcessingTimeWindows</a></strong></p></li></ul></div></li><li><p>optional kann man bei beiden noch ein Offset (konstante zeitliche Verschiebung der Fenster) angeben</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_assigners_2"><h2>DataStream API : Window Assigners (2)</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Keyed Tumbling Window (Beispiel)</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Keyed Sliding Window (Beispiel)</p></li></ul></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/tumbling-windows.svg" alt="tumbling windows" height="600"></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/sliding-windows.svg" alt="sliding windows" height="600"></div></div></td></tr></table>
<div class="paragraph center small"><small><em>(Bildquelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a>)</em></small></div></section>
<section id="datastream_api_window_assigners_3"><h2>DataStream API : Window Assigners (3)</h2><div class="ulist"><ul><li><p><strong>Session Windows</strong></p><div class="ulist"><ul><li><p>Ein Window schließt, wenn es eine gewisse Zeit lang (<em>session gap</em>) keine neuen Elemente im Stream gesehen hat</p></li><li><p>Mit dem ersten danach folgenden Element beginnt ein neues Window</p></li><li><p><em>session gap</em> kann statisch (konstante Zeitlänge) gewählt werden, oder basierend auf jedem ankommenden Element im Stream dynamisch neu berechnet werden (über einen sog. <em>session gap extractor</em>)</p></li><li><p>relevante Klassen <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.3/api/java/org/apache/flink/streaming/api/windowing/assigners/EventTimeSessionWindows.html">EventTimeSessionWindows</a></strong>, <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.3/api/java/index.html?org/apache/flink/streaming/api/windowing/assigners/ProcessingTimeSessionWindows.html">ProcessingTimeSessionWindows</a></strong></p></li></ul></div></li><li><p><strong>Global Windows</strong> ordnen alle Element mit dem gleichen Key dem gleichen Fenster zu</p><div class="ulist"><ul><li><p>&#8594; Verhalten des Fensters wird über den angegebenen Trigger gesteuert</p></li><li><p>dieser sollte explizit angegeben werden, da der default "never trigger" ist</p></li><li><p>relevante Klasse: <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/GlobalWindows.html">GlobalWindows</a></strong></p></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_assigners_4"><h2>DataStream API : Window Assigners (4)</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Keyed Session Window</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Keyed Global Window</p></li></ul></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/session-windows.svg" alt="session windows" width="2000" height="600"></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/non-windowed.svg" alt="non windowed" width="2000" height="600"></div></div></td></tr></table>
<div class="paragraph center small"><small><em>(Bildquelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a>)</em></small></div></section>
<section id="datastream_api_window_assigners_5"><h2>DataStream API : Window Assigners (5)</h2><div class="ulist"><ul><li><p>Codebeispiele zum Einstellen der WindowAssigner:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">input1
    .keyBy(&lt;key selector&gt;)
    .window(TumblingEventTimeWindows.of(<span class="predefined-type">Time</span>.seconds(<span class="integer">5</span>)))
    .&lt;windowed transformation&gt;(&lt;window function&gt;);

input2
    .keyBy(&lt;key selector&gt;)
    .window(TumblingProcessingTimeWindows.of(<span class="predefined-type">Time</span>.seconds(<span class="integer">5</span>)))
    .&lt;windowed transformation&gt;(&lt;window function&gt;);

input3
    .keyBy(&lt;key selector&gt;)
    .window(EventTimeSessionWindows.withGap(<span class="predefined-type">Time</span>.minutes(<span class="integer">10</span>)))
    .&lt;windowed transformation&gt;(&lt;window function&gt;);

input4
    .keyBy(&lt;key selector&gt;)
    .window(GlobalWindows.create())
    .&lt;windowed transformation&gt;(&lt;window function&gt;);</code></pre>
<div class="paragraph center small"><small><em>(Beispiele aus: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a>)</em></small></div></section>
<section id="datastream_api_window_functions"><h2>DataStream API : Window Functions</h2><div class="ulist"><ul><li><p>Es gibt 3 vordefinierte <strong>abstrakte Basisklassen</strong> für <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/windowing/WindowFunction.html">Window Functions</a></strong></p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/functions/ReduceFunction.html">ReduceFunction</a></strong> wird hier genau so verwendet, wie schon vorher gesehen, nur wird jeweils pro Window aggregiert</p></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/table/functions/AggregateFunction.html">AggregateFunction</a></strong> analog für allgemeine (windowed) Rolling Aggregations</p><div class="ulist"><ul><li><p>ein Akkumulator wird mit <em>createAccumulator</em> erstellt</p></li><li><p>ankommende Datensätze können mit <em>add</em> mit dem Akkumulator verrechnet werden</p></li><li><p>in der Methode <em>getResult</em> wird aus dem Akkumulator das Resultat des Windows berechnet</p></li><li><p>zusätzlich muss eine <em>merge</em> Operation für 2 Akkumulatorinstanzen definiert werden</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.9/api/java/org/apache/flink/streaming/api/functions/windowing/ProcessWindowFunction.html">ProcessWindowFunction</a></strong> (s.u.)</p></li></ul></div></li><li><p><strong>ReduceFunction</strong> und <strong>AggregateFunction</strong> emittieren ihren aggregierten Wert automatisch bei <strong>Triggeraufrufen</strong> (Default für die vordefinierten WindowAssigner : bei Schließen eines Windows)</p></li></ul></div></section>
<section id="datastream_api_beispiel_aggregatefunction"><h2>DataStream API : Beispiel AggregateFunction</h2><div class="ulist"><ul><li><p>Beispiel : AggregateFunction, die Werte addiert und zählt, um einen Durchschnitt zu berechnen</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">private</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">AverageAggregate</span>
    <span class="directive">implements</span> AggregateFunction&lt;Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt;, Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt;, <span class="predefined-type">Double</span>&gt; {

      <span class="annotation">@Override</span>
      <span class="directive">public</span> Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; createAccumulator() {
        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="integer">0L</span>, <span class="integer">0L</span>);
      }

      <span class="annotation">@Override</span>
      <span class="directive">public</span> Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; add(Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt; value, Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; accumulator) {
        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="integer">1L</span>);
      }

      <span class="annotation">@Override</span>
      <span class="directive">public</span> <span class="predefined-type">Double</span> getResult(Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; accumulator) {
        <span class="keyword">return</span> ((<span class="type">double</span>) accumulator.f0) / accumulator.f1;
      }

      <span class="annotation">@Override</span>
      <span class="directive">public</span> Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; merge(Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; a, Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">Long</span>&gt; b) {
        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);
      }
}</code></pre>
<div class="paragraph center small"><small>(Codebeispiel aus <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a>)</small></div></section>
<section id="datastream_api_beispiel_aggregatefunction_2"><h2>DataStream API : Beispiel AggregateFunction (2)</h2><div class="ulist"><ul><li><p>Beispiel Anwendung der AggregateFunction auf der letzten Folie:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="comment">// in der main Methode</span>

DataStream&lt;Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt;&gt; input = env.fromSource(<span class="keyword">new</span> MySource());

DataStream&lt;<span class="predefined-type">Double</span>&gt; averagesBySecond =   <span class="comment">// Stream von Durchschnittswerten</span>
input
    .keyBy(pair -&gt; MyUtils.generateKey(pair.f1))  <span class="comment">// Durchschnitt nach Keys bilden</span>
    .window(TumblingProcessingTimeWindows.of(TumblingProcessingTimeWindows.of(<span class="predefined-type">Time</span>.seconds(<span class="integer">1</span>)))  <span class="comment">// 1 Output pro Key pro Sekunde</span>
    .aggregate(<span class="keyword">new</span> AverageAggregate());
<span class="comment">// (weitere Verarbeitung der Durchschnittswerte)</span></code></pre>
<div class="paragraph center small"><small>(Codebeispiel erweitert aus <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a>)</small></div></section>
<section id="datastream_api_processwindowfunction"><h2>DataStream API : ProcessWindowFunction</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.9/api/java/org/apache/flink/streaming/api/functions/windowing/ProcessWindowFunction.html">ProcessWindowFunction</a></strong> ist eine allgemeiner Prototyp für WindowFunctions</p><div class="ulist"><ul><li><p><em>process</em> Methode wird <strong>bei einem Trigger</strong> eines Windows (meist bei Schließen) aufgerufen und erhält ein Iterable mit allen Datensätzen innerhalb des Windows</p><div class="ulist"><ul><li><p>&#8594; benötigt Buffering, ineffizient für reine Aggregierungen</p></li></ul></div></li><li><p><em>process</em> hat über den <strong>Context</strong> Zugriff auf die <strong>Metataden</strong> des aktuellen Fensters und auf das zuletzt gesehene <strong>Watermark</strong></p></li><li><p>kann <strong>Window State</strong> haben</p><div class="ulist"><ul><li><p>Window State wird für jedes Window separat verwaltet</p></li><li><p>Window State sollte in der <em>clear()</em> Methode bereinigt werden</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_processwindowfunction_beispiel"><h2>DataStream API : ProcessWindowFunction Beispiel</h2><div class="ulist"><ul><li><p>Beispiel ProcessWindowFunction für Medianberechnung:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">MedianProcessWindowFunction</span>&lt;W <span class="directive">extends</span> <span class="predefined-type">Window</span>&gt;
            <span class="directive">extends</span> ProcessWindowFunction&lt;KeyedDouble, KeyedDouble, <span class="predefined-type">Integer</span>, W&gt; {

        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="type">void</span> process(<span class="predefined-type">Integer</span> key, <span class="predefined-type">Context</span> context,
                            <span class="predefined-type">Iterable</span>&lt;KeyedDouble&gt; input, Collector&lt;KeyedDouble&gt; output)  {

            <span class="predefined-type">List</span>&lt;<span class="predefined-type">Double</span>&gt; values = <span class="keyword">new</span> <span class="predefined-type">ArrayList</span>();
            input.forEach(keyedDouble -&gt; values.add(keyedDouble.getValue()));
            <span class="type">double</span> median = MyMathUtils.computeMedian(values);

            output.collect(<span class="keyword">new</span> KeyedDouble(key, median));
        }
}

<span class="comment">// in main Methode</span>
DataStream&lt;<span class="predefined-type">Double</span>&gt; input = env.fromSource(source, getWatermarkStrategy(), <span class="string"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>); <span class="comment">// Source nicht gezeigt</span>
KeyedStream&lt;KeyedDouble, <span class="predefined-type">Integer</span>&gt; keyedStream = assignKeys(stream);  <span class="comment">// Key Assigner nicht gezeigt</span>

DataStream&lt;KeyedDouble&gt; medianStream =
                keyedStream.window(TumblingEventTimeWindows.of(<span class="predefined-type">Time</span>.seconds(<span class="integer">1</span>)))  <span class="comment">// 1 Output pro Key und Sekunde</span>
                           .process(<span class="keyword">new</span> MedianProcessWindowFunction&lt;&gt;());
<span class="comment">// (weitere Verarbeitung)</span></code></pre></section>
<section id="datastream_api_kombination_von_aggregierung_und_processwindowfunction"><h2>DataStream API : Kombination von Aggregierung und ProcessWindowFunction</h2><div class="ulist"><ul><li><p><strong>Aggregierungen</strong> und <strong>ProcessWindowFunctions</strong> können auch <strong>kombiniert</strong> werden</p><div class="ulist"><ul><li><p>hierfür wird bei Aufruf von <em>reduce</em> oder <em>aggregate</em> <strong>zusätzlich</strong> zu einer ReduceFunction oder AggregateFunction noch eine ProcessWindowFunction als weiterer Parameter übergeben</p></li><li><p><em>process</em> Methode der ProcessWindowFunction erhält dann ein Iterable mit nur einem Element, nämlich dem Resultat der Aggregierung</p><div class="ulist"><ul><li><p>&#8594; ProcessWindowFunction leistet eine <strong>Nachbearbeitung</strong></p></li><li><p>&#8594; Window-Metadaten können zusätzlich das Resultat beeinflussen</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_aggregierung_mit_processwindowfunction_beispiel"><h2>DataStream API : Aggregierung mit ProcessWindowFunction Beispiel</h2><div class="ulist"><ul><li><p>Beispiel: Aggregieren von Counts von Datensätzen pro Window mit Window-Metadaten</p><div class="ulist"><ul><li><p>Gegeben: CountingAggregator zählt einfach nur die Datensätze pro Key und Window</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">private</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">EnrichingProcessWindowFunction</span>&lt;T, W <span class="directive">extends</span> <span class="predefined-type">Window</span>&gt;
            <span class="directive">extends</span> ProcessWindowFunction&lt;T, Tuple2&lt;T, <span class="predefined-type">String</span>&gt;, <span class="predefined-type">Long</span>, W&gt; {

        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="type">void</span> process(<span class="predefined-type">Long</span> key, <span class="predefined-type">Context</span> context,
                            <span class="predefined-type">Iterable</span>&lt;T&gt; input, Collector&lt;Tuple2&lt;T, <span class="predefined-type">String</span>&gt;&gt; output)  {
            <span class="keyword">for</span> (T element : input) {
                output.collect(<span class="keyword">new</span> Tuple2&lt;T, <span class="predefined-type">String</span>&gt;(element, <span class="string"><span class="delimiter">&quot;</span><span class="content">Window : </span><span class="delimiter">&quot;</span></span> + context.window() + <span class="string"><span class="delimiter">&quot;</span><span class="content"> Key : </span><span class="delimiter">&quot;</span></span> + key));
            }
        }
}

<span class="comment">// in main:</span>
KeyedStream&lt;MyEvent, <span class="predefined-type">Long</span>&gt; events = env.fromSource(mySource)
                                       .keyBy(myKeyAssigner);

DataStream&lt;Tuple2&lt;<span class="predefined-type">Long</span>, <span class="predefined-type">String</span>&gt;&gt; countsWithMetaData =
events.window(TumblingEventTimeWindows.of(<span class="predefined-type">Time</span>.seconds(<span class="integer">1</span>)))
      .aggregate(<span class="keyword">new</span> CountingAggregate&lt;MyEvent&gt;(), <span class="keyword">new</span> EnrichingProcessWindowFunction&lt;&gt;());</code></pre></section>
<section id="datastream_api_trigger"><h2>DataStream API : Trigger</h2><div class="ulist"><ul><li><p>durch Erweitern der abstrakten Klasse <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.html">Trigger&lt;T, W extends Window&gt;</a></strong> können eigene <strong>Trigger</strong> definiert werden</p></li><li><p>Trigger kann in seiner Methode <em>onElement</em> auf jedes ankommende Element reagieren</p></li><li><p>zusätzlich kann auf registrierte <strong>Timer</strong> (s.u.) mit <em>onEventTime</em> oder <em>onProcessingTime</em> reagiert werden</p></li><li><p>diese 3 Methoden geben jeweils einen <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/TriggerResult.html">TriggerResult</a></strong> zurück, das ein Enum mit 4 möglichen Werten ist:</p><div class="ulist"><ul><li><p><strong>CONTINUE</strong> : es passiert nichts</p></li><li><p><strong>FIRE</strong> : Rufe die im Operator eingestellte Window Function auf</p><div class="ulist"><ul><li><p>ReduceFunction und Aggregatefunction emittieren ihr derzeitiges Resultat</p></li><li><p>ProcessWindowFunction ruft <em>process</em> Methode auf</p></li></ul></div></li><li><p><strong>PURGE</strong> : Entferne alle Elemente aus dem Window(buffer)</p></li><li><p><strong>FIRE_AND_PURGE</strong> : erst FIRE, dann PURGE</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_trigger_2"><h2>DataStream API : Trigger (2)</h2><div class="ulist"><ul><li><p>Trigger können Windows nicht schließen (hierfür ist nur WindowAssigner zuständig)</p><div class="ulist"><ul><li><p>mit PURGE kann aber das Verhalten von späteren Triggeraufrufen verändert werden</p></li></ul></div></li><li><p>FIRE kann mehrmals für das gleiche Window passieren, wenn entsprechend konfiguriert</p></li><li><p>Triggermethoden haben Zugriff auf Window-Metadaten und können Timer registrieren</p></li><li><p>Trigger können über Verwendung ihres <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.TriggerContext.html">TriggerContext</a></strong> einen <strong>State</strong> (Window bzw. Window/Key Scope) registrieren und verwenden</p><div class="ulist"><ul><li><p>in diesem Fall muss der State in der <em>clear</em> Methode bereinigt werden, und in <em>onMerge</em> berücksichtigt werden</p><div class="ulist"><ul><li><p><em>onMerge</em> wird vom Window Assigner aufgerufen, wenn 2 Windows zusammengelegt werden (passiert v.a. bei Session Windows)</p></li></ul></div></li></ul></div></li><li><p><strong>Stateful</strong> Trigger können komplexe Logik enthalten und sind genauso mächtig wie ProcessWindowFunctions</p></li></ul></div></section>
<section id="datastream_api_trigger_3"><h2>DataStream API : Trigger (3)</h2><div class="ulist"><ul><li><p>wenn kein komplexes Verhalten nötig ist, sind die default Trigger von WindowAssignern meist ausreichend</p></li><li><p>Weitere <strong>vordefinierte</strong> Arten von Triggern sind:</p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/EventTimeTrigger.html">EventTimeTrigger</a></strong> und <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/ProcessingTimeTrigger.html">ProcessingTimeTrigger</a></strong></p><div class="ulist"><ul><li><p>feuern basierend auf dem Fortschritt von Watermarks bzw. Systemzeit</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/CountTrigger.html">CountTrigger</a></strong></p><div class="ulist"><ul><li><p>feuern basierend auf der Anzahl von verarbeiteten Elementen</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/PurgingTrigger.html">PurgingTrigger</a></strong></p><div class="ulist"><ul><li><p>nimmt einen anderen Trigger und ersetzt i.W. alle FIRE Aktionen durch FIRE_AND_PURGE</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_trigger_beispiel"><h2>DataStream API : Trigger Beispiel</h2><div class="ulist"><ul><li><p>Beispiel: Implementierung von <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/CountTrigger.html">CountTrigger</a> mit <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/state/ReducingState.html">ReducingState</a> (unvollständig):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">CountTrigger</span>&lt;W <span class="directive">extends</span> <span class="predefined-type">Window</span>&gt; <span class="directive">extends</span> Trigger&lt;<span class="predefined-type">Object</span>, W&gt; {

    <span class="directive">private</span> <span class="directive">final</span> <span class="type">long</span> maxCount;
    <span class="directive">private</span> <span class="directive">final</span> ReducingStateDescriptor&lt;<span class="predefined-type">Long</span>&gt; stateDesc =
            <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(<span class="string"><span class="delimiter">&quot;</span><span class="content">count</span><span class="delimiter">&quot;</span></span>, <span class="keyword">new</span> Sum(), LongSerializer.INSTANCE);

    <span class="directive">private</span> CountTrigger(<span class="type">long</span> maxCount) {
        <span class="local-variable">this</span>.maxCount = maxCount;
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> TriggerResult onElement(<span class="predefined-type">Object</span> element, <span class="type">long</span> timestamp, W window, TriggerContext ctx)
            <span class="directive">throws</span> <span class="exception">Exception</span> {
        ReducingState&lt;<span class="predefined-type">Long</span>&gt; count = ctx.getPartitionedState(stateDesc);
        count.add(<span class="integer">1L</span>);
        <span class="keyword">if</span> (count.get() &gt;= maxCount) {
            count.clear();
            <span class="keyword">return</span> TriggerResult.FIRE;
        }
        <span class="keyword">return</span> TriggerResult.CONTINUE;
    }

    <span class="directive">private</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">Sum</span> <span class="directive">implements</span> ReduceFunction&lt;<span class="predefined-type">Long</span>&gt; {
        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="predefined-type">Long</span> reduce(<span class="predefined-type">Long</span> value1, <span class="predefined-type">Long</span> value2) <span class="directive">throws</span> <span class="exception">Exception</span> {
            <span class="keyword">return</span> value1 + value2;
        }
    }
}</code></pre></section>
<section id="datastream_api_timer"><h2>DataStream API : Timer</h2><div class="ulist"><ul><li><p><strong>Timer</strong> sind ein <strong>flexibles, low-level Feature</strong>, um zeitbasierte Logik zu implementieren</p><div class="ulist"><ul><li><p>Wenn eine gegebene Anforderung bereits nur mit <strong>Windows</strong> alleine umgesetzt werden kann, ist dies in der Regel zu bevorzugen</p></li></ul></div></li><li><p>Timer sind Teil des <strong>State</strong> eines Operators und werden in Checkpoints persistiert</p></li><li><p>sowohl <strong>ProcessFunctions</strong> als auch <strong>Trigger</strong> erhalten in ihren Hauptmethoden Zugriff auf einen <strong>Context</strong>, über den ein <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/TimerService.html">TimerService</a></strong> erhalten werden kann</p></li><li><p>TimerService enthält 2 Methoden, um einen <strong>Timer</strong> zu <strong>registrieren</strong>:</p><div class="ulist"><ul><li><p><em>registerEventTimeTimer(long time)</em></p></li><li><p><em>registerProcessingTimeTimer(long time)</em></p><div class="ulist"><ul><li><p>die Timer werden ausgelöst, wenn event time bzw. processing time den angegebenen <strong>timestamp</strong> überschreiten</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_timer_2"><h2>DataStream API : Timer (2)</h2><div class="ulist"><ul><li><p>Durch Überschreiben gewisser Methoden kann die ProcessFunction bzw. der Trigger auf das Auslösen eines Timers <strong>reagieren</strong>:</p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/ProcessFunction.html">ProcessFunction</a>:</p><div class="ulist"><ul><li><p><em>onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</em></p></li><li><p>mit dem Collector kann wie in <em>processElement</em> Output emittiert werden</p></li></ul></div></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.html">Trigger</a>:</p><div class="ulist"><ul><li><p><em>onEventTime(long time, W window, TriggerContext ctx)</em></p></li><li><p><em>onProcessingTime(long time, W window, TriggerContext ctx)</em></p></li></ul></div></li></ul></div></li><li><p><strong>Timer</strong> können im TimerService auch wieder <strong>entfernt</strong> werden :</p><div class="ulist"><ul><li><p><em>deleteEventTimeTimer(long time)</em> bzw. <em>deleteProcessingTimeTimer(long time)</em> löschen den Timer zum angegebenen Timestamp</p></li><li><p>dies ist möglich, da es zu einem Timestamp und einem Key immer nur einen Timer geben darf</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_timer_beispiel"><h2>DataStream API : Timer Beispiel</h2><div class="ulist"><ul><li><p>Beispiel : Timer-basierter Operator, der nach 60 Sekunden Inaktivität (für einen String Key) die Anzahl der Datensätze dieser Session emittiert:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">CountWithTimeoutFunction</span>
        <span class="directive">extends</span> KeyedProcessFunction&lt;<span class="predefined-type">String</span>, Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt;, Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt;&gt; {

    <span class="directive">private</span> ValueState&lt;CountWithTimestamp&gt; state;
    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> open(OpenContext openContext) <span class="directive">throws</span> <span class="exception">Exception</span> {
        state = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string"><span class="delimiter">&quot;</span><span class="content">myState</span><span class="delimiter">&quot;</span></span>, CountWithTimestamp.class));
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> processElement(Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; value, <span class="predefined-type">Context</span> ctx, Collector&lt;Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt;&gt; out) {
        <span class="predefined-type">Long</span> currentCount = state.value() == <span class="predefined-constant">null</span> ? <span class="integer">0</span> : state.value().count;  <span class="comment">// retrieve the current count</span>
        state.update(<span class="keyword">new</span> CountWithTimestamp(value.f0, currentCount + <span class="integer">1</span>, ctx.timestamp()));  <span class="comment">// write the new state back</span>
        ctx.timerService().registerEventTimeTimer(current.lastModified + <span class="integer">60000</span>);  <span class="comment">// schedule the next timer</span>
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> onTimer(<span class="type">long</span> timestamp, OnTimerContext ctx, Collector&lt;Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt;&gt; out) {
        CountWithTimestamp result = state.value();  <span class="comment">// get the state for the key that scheduled the timer</span>
        <span class="keyword">if</span> (timestamp == result.lastModified + <span class="integer">60000</span>) {   <span class="comment">// check if this is an outdated timer or the latest timer</span>
            out.collect(<span class="keyword">new</span> Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>&gt;(result.key, result.count));  <span class="comment">// emit the state on timeout</span>
        }
    }
}</code></pre>
<div class="paragraph center small"><small>(Beispiel adaptiert aus <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/process_function/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/process_function/</a>)</small></div></section>
<section id="datastream_api_evictors"><h2>DataStream API : Evictors</h2><div class="ulist"><ul><li><p>ein <strong>Evictor</strong> kann durch Implementieren des <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/evictors/Evictor.html">Evictor&lt;T,W extends Window&gt;</a></strong> Interface definiert werden</p></li><li><p>folgende Methoden müssen implementiert werden:</p><div class="ulist"><ul><li><p><em>void evictBefore(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext)</em></p></li><li><p><em>void evictAfter(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext)</em></p></li></ul></div></li><li><p>Diese Methoden werden <strong>nach jedem Feuern eines Triggers</strong> aufgerufen</p><div class="ulist"><ul><li><p><em>evictBefore</em> vor Anwenden der Window Function</p></li><li><p><em>evictAfter</em> nach Anwenden der Window Function</p></li></ul></div></li><li><p><em>elements</em> enthält die Elemente im Window, <em>size</em> ist ihre Anzahl</p></li><li><p>Der Parameter <em>elements</em> ist <strong>mutable</strong> und es können nicht gewüschte Elemente darin entfernt werden</p></li></ul></div></section>
<section id="datastream_api_evictors_2"><h2>DataStream API : Evictors (2)</h2><div class="ulist"><ul><li><p><strong>vordefinierte</strong> Evictors sind:</p><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-stable/api/java/org/apache/flink/streaming/api/windowing/evictors/CountEvictor.html">CountEvictor</a></strong></p><div class="ulist"><ul><li><p>entfernt alle Elemente, die eine gegebene maximale Anzahl überschreiten</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/evictors/TimeEvictor.html">TimeEvictor</a></strong></p><div class="ulist"><ul><li><p>erhält als Parameter ein <em>interval</em> und entfernt alle Elemente, die weiter als <em>interval</em> Millisekunden als das
jüngste Element im Window zurückliegen (jeweils nach Timestamp, also event time)</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-stable/api/java/org/apache/flink/streaming/api/windowing/evictors/DeltaEvictor.html">DeltaEvictor</a></strong></p><div class="ulist"><ul><li><p>erhält ein <em>threshold</em> (Typ double) als Parameter</p></li><li><p>erhält eine <strong>DeltaFunction</strong>, die 2 Inputs des Streamdatentyps nimmt und ein double zurückgibt</p></li><li><p>der Evictor nimmt das letzte Element im Window (nicht event time basiert) und wendet für jedes andere Element im Window einmal die DeltaFunction auf beide an</p></li><li><p>diejenigen anderen Elemente, bei denen das Resultat größer als <em>threshold</em> ist, werden entfernt</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_evictors_beispiel"><h2>DataStream API : Evictors Beispiel</h2><div class="ulist"><ul><li><p>Beispiel : Implementierung von <a href="https://nightlies.apache.org/flink/flink-docs-stable/api/java/org/apache/flink/streaming/api/windowing/evictors/CountEvictor.html">CountEvictor</a> (vereinfacht, unvollständig) :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">CountEvictor</span>&lt;W <span class="directive">extends</span> <span class="predefined-type">Window</span>&gt; <span class="directive">implements</span> Evictor&lt;<span class="predefined-type">Object</span>, W&gt; {

    <span class="directive">private</span> <span class="directive">final</span> <span class="type">long</span> maxCount;
    <span class="directive">private</span> CountEvictor(<span class="type">long</span> count) {
        <span class="local-variable">this</span>.maxCount = count;
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> evictAfter(<span class="predefined-type">Iterable</span>&lt;TimestampedValue&lt;<span class="predefined-type">Object</span>&gt;&gt; elements, <span class="type">int</span> size, EvictorContext ctx) {
        <span class="keyword">if</span> (size &lt;= maxCount) {
            <span class="keyword">return</span>;
        }
        <span class="type">int</span> evictedCount = <span class="integer">0</span>;
        <span class="keyword">for</span> (<span class="predefined-type">Iterator</span>&lt;TimestampedValue&lt;<span class="predefined-type">Object</span>&gt;&gt; iterator = elements.iterator(); iterator.hasNext(); ) {
                iterator.next();
                <span class="keyword">if</span> (++evictedCount &gt; size - maxCount) {
                    <span class="keyword">break</span>;
                } <span class="keyword">else</span> {
                    iterator.remove();
                }
        }
    }
}</code></pre></section>
<section id="datastream_api_lateness_umgang_mit_verspäteten_datensätzen"><h2>DataStream API : Lateness (Umgang mit verspäteten Datensätzen)</h2><div class="ulist"><ul><li><p>Wenn Windows über <strong>event time</strong> definiert sind, kann es <strong>verspäteten Datensätze</strong> geben</p><div class="ulist"><ul><li><p>Per Default werden diese einfach verworfen</p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong><em>allowedLateness(lateness)</em></strong></p><div class="ulist"><ul><li><p>Window wartet auf <strong>verspätete</strong> Datensätze, bis ein <strong>Watermark</strong> erreicht wurde, das <strong>die Summe aus Endzeitpunkt des Fensters und <em>lateness</em></strong> überschreitet</p></li><li><p>&#8594; Wirkt <strong>zusätzlich</strong> zu der ggf. schon eingestellten Toleranz in der <strong>WatermarkStrategy</strong></p></li><li><p>Kann je nach Trigger dazu führen, dass der <strong>Trigger</strong> für späte Elemente <strong>erneut feuert</strong></p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong><em>sideOutputLateData(OutputTag)</em></strong></p><div class="ulist"><ul><li><p>Verspätete Datensätze werden an einen <strong>zusätzlichen Stream</strong> weitergeleitet (z.B. für Logging)</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_outputstream_von_window_operators"><h2>DataStream API : Outputstream von Window Operators</h2><div class="ulist"><ul><li><p><strong>Outputstream</strong> eines Window Operators</p><div class="ulist"><ul><li><p>DataStream <strong>ohne Keys</strong></p><div class="ulist"><ul><li><p>Auch wenn der ursprüngliche Stream ein <strong>KeyedStream</strong> war und <strong>Keyed Windows</strong> verwendet wurden</p></li><li><p>&#8594; Keys müssen ggf. in den Datensätzen festgehalten und dann <strong>neu assigned</strong> werden</p></li></ul></div></li><li><p><strong>Timestamps</strong> sind per Default gleich dem <strong>spätesten möglichen Zeitpunkt im Window</strong></p></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_lifecycle"><h2>DataStream API : Window Lifecycle</h2><div class="ulist"><ul><li><p><strong>Erstellung</strong> eines Windows</p><div class="ulist"><ul><li><p>wenn das erste Element im Stream erscheint, das zu diesem Window gehört</p></li></ul></div></li><li><p><strong>Löschen</strong> eines Windows</p><div class="ulist"><ul><li><p>nachdem sein Endzeitpunkt (plus ggf. <em>allowed lateness</em>) erreicht wurde</p><div class="ulist"><ul><li><p>Endzeitpunkt wird durch den WindowAssigner bestimmt (statisch oder dynamisch)</p></li></ul></div></li><li><p>Window State sowie Metadaten werden gelöscht und es sind keine weiteren Trigger mehr möglich</p><div class="ulist"><ul><li><p>Bei <strong>nicht-zeitbasierten Windows</strong> (z.B. Global Window) geschieht das <strong>nicht garantiert automatisch</strong></p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_join"><h2>DataStream API : Window Join</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#join-org.apache.flink.streaming.api.datastream.DataStream-">Join</a></strong> von 2 Streams je <strong>innerhalb gemeinsamer Fenster</strong> (Codeschema):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">stream.join(otherStream)
    .where(&lt;KeySelector&gt;)
    .equalTo(&lt;KeySelector&gt;)
    .window(&lt;WindowAssigner&gt;)
    .apply(&lt;JoinFunction&gt;);</code></pre>
<div class="ulist"><ul><li><p><em>stream</em> und <em>otherStream</em> werden als einfache DataStreams behandelt</p></li><li><p>Mit <em>where</em> und <em>equalTo</em> werden <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/java/functions/KeySelector.html">KeySelectors</a></strong> für die beiden Streams festgelegt</p></li><li><p>Mit <em>window</em> wird ein <strong>gemeinsamer</strong> <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/assigners/WindowAssigner.html">WindowAssigner</a> festgelegt</p></li><li><p>Optional können danach auch noch <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.html">Trigger</a>, Evictor und Lateness</strong> konfiguriert werden</p></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/functions/JoinFunction.html">JoinFunction</a></strong> nimmt als Inputs je ein Element von <em>stream</em> und <em>otherstream</em></p><div class="ulist"><ul><li><p>Alternativ: <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.3/api/java/org/apache/flink/api/common/functions/FlatJoinFunction.html">FlatJoinFunction</a></strong>, die mehrere Datensätze ausgeben kann (analog zu FlatMapFunction)</p></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_join_beispiel"><h2>DataStream API : Window Join Beispiel</h2><div class="ulist"><ul><li><p>Beispiel für Window Join mit TumblingWindows :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">Integer</span>&gt; orangeStream = env.fromSource(orangeSource);
DataStream&lt;<span class="predefined-type">Integer</span>&gt; greenStream = env.fromSource(greenSource);

orangeStream.join(greenStream)
    .where(<span class="keyword">new</span> OrangeKeySelector())
    .equalTo(<span class="keyword">new</span> GreenKeySelector())
    .window(TumblingEventTimeWindows.of(<span class="predefined-type">Time</span>.milliseconds(<span class="integer">2</span>)))
    .apply (<span class="keyword">new</span> JoinFunction&lt;<span class="predefined-type">Integer</span>, <span class="predefined-type">Integer</span>, <span class="predefined-type">String</span>&gt; (){
        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="predefined-type">String</span> join(<span class="predefined-type">Integer</span> first, <span class="predefined-type">Integer</span> second) {
            <span class="keyword">return</span> first + <span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span> + second;
        }
    });</code></pre>
<div class="ulist"><ul><li><p>Mit SlidingWindows ändert sich nur die Zeile :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">.window(SlidingEventTimeWindows.of(<span class="predefined-type">Time</span>.milliseconds(<span class="integer">2</span>) <span class="comment">/* size */</span>, <span class="predefined-type">Time</span>.milliseconds(<span class="integer">1</span>) <span class="comment">/* slide */</span>))</code></pre>
<div class="paragraph center small"><small>(Beispiel aus <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/</a>)</small></div></section>
<section id="datastream_api_window_join_beispiel_grafik"><h2>DataStream API : Window Join Beispiel : Grafik</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Tumbling Window Join</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Session Window Join</p></li></ul></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/tumbling-window-join.svg" alt="tumbling window join" height="700"></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock" style=""><img src="images/session-window-join.svg" alt="session window join" height="700"></div></div></td></tr></table>
<div class="paragraph center small"><small><em>(Bildquelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/</a>)</em></small></div></section>
<section id="datastream_api_window_join_2"><h2>DataStream API : Window Join (2)</h2><div class="ulist"><ul><li><p>Bei jedem <strong>Feuern des Triggers</strong> wird die JoinFunction bzw. FlatJoinFunction auf <strong>alle Datensatzpaare</strong> aus <em>stream</em> und <em>otherstream</em> im Window <strong>mit dem gleichen Key</strong> angewendet</p><div class="ulist"><ul><li><p>&#8594; <strong>inner join</strong></p></li></ul></div></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/DataStream.html#coGroup-org.apache.flink.streaming.api.datastream.DataStream-"><em>cogroup</em></a></strong></p><div class="ulist"><ul><li><p>kann statt <em>join</em> verwendet werden</p></li><li><p>es wird statt einer JoinFunction eine <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/functions/CoGroupFunction.html">CoGroupFunction</a></strong> angegeben</p></li><li><p>diese wird nur <strong>(höchstens) einmal pro Key</strong> aufgerufen und erhält <strong>Iterators</strong> über alle Datensätze im Window mit diesem Key</p><div class="ulist"><ul><li><p>&#8594; komplexere Logik möglich</p></li><li><p>&#8594; <strong>outer join</strong> möglich</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="datastream_api_window_join_mit_cogroup_beispiel"><h2>DataStream API : Window Join mit Cogroup Beispiel</h2><div class="ulist"><ul><li><p>Vorheriges Beispiel als Left Outer Join mit Cogroup:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">DataStream&lt;<span class="predefined-type">Integer</span>&gt; orangeStream = env.fromSource(orangeSource);
DataStream&lt;<span class="predefined-type">Integer</span>&gt; greenStream = env.fromSource(greenSource);

orangeStream.coGroup(greenStream)
    .where(<span class="keyword">new</span> OrangeKeySelector())
    .equalTo(<span class="keyword">new</span> GreenKeySelector())
    .window(TumblingEventTimeWindows.of(<span class="predefined-type">Time</span>.milliseconds(<span class="integer">2</span>)))
    .apply(<span class="keyword">new</span> CoGroupFunction&lt;<span class="predefined-type">Integer</span>, <span class="predefined-type">Integer</span>, <span class="predefined-type">String</span>&gt; (){
        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="type">void</span> coGroup(<span class="predefined-type">Iterable</span>&lt;<span class="predefined-type">Integer</span>&gt; firstElements, <span class="predefined-type">Iterable</span>&lt;<span class="predefined-type">Integer</span>&gt; secondElements, Collector&lt;<span class="predefined-type">String</span>&gt; out) {
            <span class="keyword">if</span> (!secondElements.hasNext()) {
                <span class="keyword">for</span> (<span class="predefined-type">Integer</span> first : firstElements) {
                    out.collect(first + <span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span>);
                }
            }
            <span class="keyword">else</span> {
                <span class="keyword">for</span> (<span class="predefined-type">Integer</span> first : firstElements) {
                    <span class="keyword">for</span> (<span class="predefined-type">Integer</span> second : secondElements) {
                        out.collect(first + <span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span> + second);
                    }
                }
            }
        }
    });</code></pre></section>
<section id="datastream_api_interval_join"><h2>DataStream API : Interval Join</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/datastream/KeyedStream.html#intervalJoin-org.apache.flink.streaming.api.datastream.KeyedStream-">Interval Joins</a></strong></p><div class="ulist"><ul><li><p>ausgehende Streams sind <strong>KeyedStreams</strong></p></li><li><p>Parameter: Offsets <strong><em>lower</em></strong> und <strong><em>upper</em></strong> nach unten und oben</p></li><li><p>Wir betrachten dann <strong>Intervalle</strong> der Art</p><div class="ulist"><ul><li><p>[<em>t</em> - <em>lower</em>, <em>t</em> + <em>upper</em>]<br></p></li></ul></div></li><li><p>um die <strong>event time</strong> <em>t</em> von Datensätzen</p></li></ul></div></li></ul></div>
<div class="imageblock" style=""><img src="images/interval-join.svg" alt="interval join" height="400"></div>
<div class="paragraph center small"><small><em>(Bildquelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/</a>)</em></small></div></section>
<section id="datastream_api_interval_join_2"><h2>DataStream API : Interval Join (2)</h2><div class="ulist"><ul><li><p>2 Datensätze aus den beiden Streams werden <strong>kombiniert</strong>, wenn sie</p><div class="ulist"><ul><li><p>den <strong>gleichen Key</strong> haben <strong>und</strong></p></li><li><p>die event time eines Elements <strong>innerhalb des Intervalls</strong> um die event time des anderen Elements liegt</p></li></ul></div></li><li><p>&#8594; In dem Fall, dass <em>lower</em> und <em>upper</em> <strong>gleich</strong> sind, werden genau die Datensätze kombiniert, die zeitlich <strong>höchstens diese Anzahl von Millisekunden
auseinanderliegen</strong></p></li><li><p>es wird auf diese Paare eine <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/co/ProcessJoinFunction.html">ProcessJoinFunction</a></strong> angewendet</p></li><li><p>Codebeispiel  :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">orangeStream
    .keyBy(<span class="keyword">new</span> OrangeKeySelector())
    .intervalJoin(greenStream.keyBy(<span class="keyword">new</span> GreenKeySelector()))
    .between(<span class="predefined-type">Time</span>.milliseconds(-<span class="integer">2</span>), <span class="predefined-type">Time</span>.milliseconds(<span class="integer">1</span>))
    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;<span class="predefined-type">Integer</span>, <span class="predefined-type">Integer</span>, <span class="predefined-type">String</span>&gt;(){
        <span class="annotation">@Override</span>
        <span class="directive">public</span> <span class="type">void</span> processElement(<span class="predefined-type">Integer</span> left, <span class="predefined-type">Integer</span> right, <span class="predefined-type">Context</span> ctx, Collector&lt;<span class="predefined-type">String</span>&gt; out) {
            out.collect(left + <span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span> + right);
        }
    });</code></pre></section>
<section id="aufgabe_5"><h2>Aufgabe 5</h2><div class="paragraph heading"><p>Vergleich von korrelierten Metriken</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Erstellen Sie einen Flink-Job:</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>2 identische Quellen, die <strong>zufällige Zahlenwerte</strong> im Bereich von 0 bis 1 mit <strong>zufälligen Keys</strong> von 1 bis 5 generieren (10 pro Sekunde)</p></li><li><p>auf die erste Quelle folgt ein <strong>Window Operator</strong>, der sekundenweise nach Keys den <strong>Durchschnitt</strong> der Zahlen ausrechnet</p></li><li><p>auf die zweite Quelle folgt ein <strong>Window Operator</strong>, der sekundenweise nach Keys den <strong>Median</strong> der Zahlen ausrechnet</p></li><li><p>dann folgt ein <strong>Window Join</strong> mit den gleichen Windows</p><div class="ulist"><ul><li><p>Wenn der <strong>Key gleich</strong> ist und die <strong>Werte mehr als 20% Abstand</strong> haben, wird eine <strong>Warnmeldung</strong> geschickt (Datei oder stdout)</p></li></ul></div></li></ol></div></li></ol></div></section>
<section id="aufgabe_5_hinweise"><h2>Aufgabe 5 : Hinweise</h2><div class="ulist"><ul><li><p>Die Quellen können mit Hilfe einer <strong>DataGeneratorSource</strong> (<strong>flink-connector-datagen</strong>) erstellt werden</p><div class="ulist"><ul><li><p>zur Benutzung siehe auch <a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/datagen/" class="bare">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/datagen/</a></p></li><li><p>zu beachten: Diese <strong>Dependency</strong> wird nicht vom Flinkcluster provided, benutzte Klassen müssen also von Maven <strong>in die JAR eingebunden</strong> werden (z.B. maven-assembly-plugin nutzen
und eine jar-with-dependencies bauen)</p></li></ul></div></li><li><p>Für Operatoren für Durchschnittsberechnung und Medianberechnung können Beispielen auf vorherigen Folien als Basis genommen werden</p></li><li><p>Keys sollten dem Output der ersten beiden Operatoren angeheftet werden</p></li></ul></div></section>
<section id="anbindung_externer_systeme"><h2>Anbindung externer Systeme</h2><div class="paragraph heading center"><p>Anbindung externer Systeme</p></div></section>
<section id="anbindung_externer_systeme_agenda"><h2>Anbindung externer Systeme : Agenda</h2><div class="ulist"><ul><li><p>Wir behandeln in diesem Kapitel folgende Themen:</p><div class="ulist"><ul><li><p>End-To-End Consistency mit externen Systemen</p><div class="ulist"><ul><li><p>Was für Lösungen bietet Flink an, um end-to-end exactly-once Konsistenz zu erreichen?</p></li></ul></div></li><li><p>File System Connector</p><div class="ulist"><ul><li><p>Wie funktioniert die API für Datei I/O?</p></li></ul></div></li><li><p>Kafka Connector</p><div class="ulist"><ul><li><p>Wie funktioniert die API für Kommunikation mit Kafka-Clustern?</p></li></ul></div></li><li><p>Eigener Connector</p><div class="ulist"><ul><li><p>Wie kann man I/O-Logik für eigene (nichtstandard) Systeme implementieren?</p></li></ul></div></li><li><p>Async I/O</p><div class="ulist"><ul><li><p>Wie kann ein Operator non-blocking und ohne Quellen externe Systeme abfragen?</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="anbindung_externer_systeme_connectors"><h2>Anbindung externer Systeme : Connectors</h2><div class="ulist"><ul><li><p>Flink Jobs sind über <strong>Quellen</strong> und <strong>Senken</strong> jeweils mit <strong>externen Systemen</strong> verbunden</p></li><li><p>Für die unterstützten externen Systeme bietet Flink <strong>Connectors</strong> an</p><div class="ulist"><ul><li><p>Diese sind spezielle Dependencies, die Implementierungen von Quellen und Senken enthalten</p></li></ul></div></li><li><p>Maven Dependencies für Connectors:</p><div class="ulist"><ul><li><p>flink-connector-files</p></li><li><p>flink-connector-kafka</p></li><li><p>flink-connector-jdbc</p></li><li><p>flink-connector-cassandra</p></li></ul></div></li><li><p>Für eine vollständige Liste vordefinierter Connectors, siehe <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/overview/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/overview/</a></p></li></ul></div></section>
<section id="end_to_end_consistency_mit_externen_systemen"><h2>End-To-End Consistency mit externen Systemen</h2><div class="paragraph heading center"><p>End-To-End Consistency mit externen Systemen</p></div></section>
<section id="end_to_end_consistency_mit_externen_systemen_2"><h2>End-To-End Consistency mit externen Systemen</h2><div class="ulist"><ul><li><p>Wie erwähnt kann über Senken end-to-end at-most-once garantiert werden, wenn diese effektiv zurückrollbar sind</p></li><li><p>Wir gehen auf 2 Techniken für Senken ein, die dies ermöglichen können</p></li></ul></div></section>
<section id="idempotent_writes"><h2>Idempotent Writes</h2><div class="ulist"><ul><li><p>Idempotent Writes</p><div class="ulist"><ul><li><p>dies sind Schreiboperationen, die bei wiederholter Ausführung in keinen weiteren Änderungen im Zielsystem resultieren</p></li><li><p>wenn die Senke idempotente Writes unterstützt, dann werden Writes, die der Flinkcluster bei erneuter (deterministischer) Bearbeitung der gleichen Datensätze auslöst,
keine weitere Veränderung im Zielsystem bewirken</p></li><li><p>somit werden die erneuten Writes nach Laden eines Checkpoints effektiv übersprungen &#8594; at-most-once</p></li><li><p>Einschränkung: Wenn ein Zustand im Zielsystem bei Bearbeitung der Datensätze nach dem geladenen Checkpoint mehr als einmal verändert wurde (Update),
dann wird auch die Neubearbeitung die Zwischenzustände wiederholen</p><div class="ulist"><ul><li><p>damit dieser Fall eintreten kann, muss das Zielsystem Zustandupdates unterstützen</p></li></ul></div></li><li><p>Weitere Einschränkung: Falls das Zielsystem die Daten selbst an weitere Clients weiterleitet, ist es dafür zuständig, seinerseits at-most-once zu garantieren</p></li></ul></div></li></ul></div></section>
<section id="transactional_writes"><h2>Transactional Writes</h2><div class="ulist"><ul><li><p>Transactional Writes</p><div class="ulist"><ul><li><p>Wenn transaktionale Writes aktiviert sind, dann werden alle Schreiboperationen einer Senke gebündelt nur einmal (als Transaktion)
nach jedem erfolgreichen Checkpoint ausgeführt (committed)</p></li><li><p>dadurch sind bei Recovery effektiv keine Writes seit dem letzten Checkpoint passiert &#8594; at-most-once</p></li><li><p>es ergibt sich aber eine erhöhte Latenz</p></li><li><p>Die noch nicht ausgeführten Writes gehören zum Zustand der jeweiligen Senken und werden im Checkpoint gespeichert</p></li><li><p>Es kann allerdings immer noch zu Inkonsistenzen führen, wenn die Commits von verschiedenen (Instanzen von) Senken nicht koordiniert werden</p><div class="ulist"><ul><li><p>diese Commits sollten daher effektiv als eine Transaktion passieren, d.h. wenn einer der Commits fehlschlägt, sollten die anderen zurückgerollt werden können
und die Transaktion wiederholt werden</p></li><li><p>&#8594; dies erfordert, dass das externe System transaktionale Commits unterstützt</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="transactional_writes_write_ahead_log_wal"><h2>Transactional Writes : Write-Ahead-Log (WAL)</h2><div class="ulist"><ul><li><p>Flink stellt 2 Mechanismen für transaktionale Senken-Konnektoren zu Verfügung:</p></li><li><p>Write-Ahead-Log (WAL)</p><div class="ulist"><ul><li><p>Noch nicht commitete Writes werden von den Senken gebuffert</p></li><li><p>Diese gehören zum internen Zustand des Flinkclusters und werden mit dem Checkpoint gespeichert</p></li><li><p>Vorteil : Geringe Anforderungen an das Zielsystem</p></li><li><p>Nachteile :</p><div class="ulist"><ul><li><p>Konsistenz nicht wirklich garantiert (s.o.)</p></li><li><p>vergrößert Speicherbedarf des internen Zustands des Flink-Clusters</p></li><li><p>kann ineffizient sein</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="transactional_writes_two_phase_commit_2pc"><h2>Transactional Writes : Two-Phase-Commit (2PC)</h2><div class="ulist"><ul><li><p>Wenn 2PC verwendet wird, dann emittieren die Quellen ihre Ausgaben kontinuierlich an ihr Zielsystem</p></li><li><p>alle Ausgaben von Senken eines Jobs in ein Zielsystem zwischen 2 Checkpoints werden vom Zielsystem als gehörig zu einer einzelnen Transaktion betrachtet</p></li><li><p>Nach einem erfolgreichen Checkpoint wird das Zielsystem benachrichtigt, einen Commit für diese Transaktion auszuführen</p></li><li><p>Bei Recovery nach failure wird stattdessen die Transaktion zurückgerollt</p></li><li><p>&#8594; at-most-once</p></li><li><p>erfordert offensichtlich ein kompatibles Zielsystem wie z.B. einen Kafka Cluster</p></li><li><p>Zustandshaltung wird in das Zielsystem verlagert</p></li></ul></div></section>
<section id="transactional_writes_two_phase_commit_2pc_2"><h2>Transactional Writes : Two-Phase-Commit (2PC) (2)</h2><div class="ulist"><ul><li><p>2PC involviert vor dem Commit eine zusätzliche Precommit Phase während des Checkpointings</p></li><li><p>Wenn eine Senkeninstanz eine Checkpoint Barrier erhält, speichert sie nicht nur ihren internen Zustand, sondern flusht auch ihre noch vorhandenen Outputbuffer
in das externe System und meldet dort einen Precommit an</p></li><li><p>nach Erfolg wird der Checkpoint Coordinator benachrichtigt</p></li><li><p>nachdem alle beteiligten Komponenten des Jobs ihren Erfolg gemeldet haben, stößt der Checkpoint Coordinator einen koordinierten Commit auf dem externen System an</p></li><li><p>&#8594; Inkonsistenzen sind (fast) ausgeschlossen</p></li><li><p>2PC ist meist der bevorzugte Mechanismus, um end-to-end consistency zu erreichen, wenn verfügbar</p></li></ul></div></section>
<section id="transactional_writes_two_phase_commit_2pc_beispielgrafik"><h2>Transactional Writes : Two-Phase-Commit (2PC) (Beispielgrafik)</h2><div class="imageblock" style=""><img src="images/eo-post-graphic-4.png" alt="eo post graphic 4" height="380"></div>
<div class="imageblock" style=""><img src="images/eo-post-graphic-5.png" alt="eo post graphic 5" height="380"></div>
<div class="paragraph center small"><small><em>(Bildquellen: <a href="https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/" class="bare">https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/</a>)</em></small></div></section>
<section id="file_system_connector"><h2>File System Connector</h2><div class="paragraph heading center"><p>File System Connector</p></div></section>
<section id="filesystem_connector_filesource"><h2>Filesystem Connector: FileSource</h2><div class="ulist"><ul><li><p>FileSystem Connector funktioniert mit allen gängigen Dateisystemen</p></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.html">FileSource</a></strong> :</p><div class="ulist"><ul><li><p>Kann aus beliebig vielen Pfaden lesen</p></li><li><p>Benötigt ein <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/reader/StreamFormat.html">StreamFormat</a></strong> oder ein <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/reader/BulkFormat.html">BulkFormat</a></strong></p></li><li><p>Erfordert Einstellung zum <strong>Splitting</strong> der Dateien</p><div class="ulist"><ul><li><p>&#8594; Paralleles Lesen</p></li></ul></div></li><li><p>Monitoring</p><div class="ulist"><ul><li><p>Pfade werden regelmäßig auf Änderungen geprüft</p></li><li><p>Bei Änderung oder Erstellung einer Datei wird diese <strong>vollständig neu</strong> eingelesen</p></li><li><p>Wenn nicht konfiguriert : Beschränkter Stream</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_filesource_2"><h2>Filesystem Connector: FileSource (2)</h2><div class="ulist"><ul><li><p>Erstellung eines <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.FileSourceBuilder.html">FileSourceBuilder</a></strong> :</p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.html#forRecordStreamFormat-org.apache.flink.connector.file.src.reader.StreamFormat-org.apache.flink.core.fs.Path&#8230;&#8203;-"><em>FileSource.forRecordStreamFormat(StreamFormat,Path&#8230;&#8203;)</em></a></p><div class="ulist"><ul><li><p>Liest Datensätze aus einem Filestream (high level API)</p></li></ul></div></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.html#forBulkFileFormat-org.apache.flink.connector.file.src.reader.BulkFormat-org.apache.flink.core.fs.Path&#8230;&#8203;-"><em>FileSource.forBulkFileFormat(BulkFormat,Path&#8230;&#8203;)</em></a></p><div class="ulist"><ul><li><p>Liest Datensätze in Batches ein (low level API)</p></li></ul></div></li></ul></div></li><li><p>Wichtige Einstellungen im <strong><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.FileSourceBuilder.html">FileSourceBuilder</a></strong>:</p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/AbstractFileSource.AbstractFileSourceBuilder.html#monitorContinuously-java.time.Duration-"><em>monitorContinuously</em></a></p></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/AbstractFileSource.AbstractFileSourceBuilder.html#setFileEnumerator-org.apache.flink.connector.file.src.enumerate.FileEnumerator.Provider-"><em>setFileEnumerator</em></a></p></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/AbstractFileSource.AbstractFileSourceBuilder.html#setSplitAssigner-org.apache.flink.connector.file.src.assigners.FileSplitAssigner.Provider-"><em>setSplitAssigner</em></a></p></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_filesource_beispiel"><h2>Filesystem Connector : FileSource Beispiel</h2><pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">final</span> FileSource&lt;<span class="predefined-type">String</span>&gt; source =
        FileSource.forRecordStreamFormat(
                        <span class="keyword">new</span> TextLineInputFormat(),
                        <span class="string"><span class="delimiter">&quot;</span><span class="content">/path/to/files</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">/path/to/other/files</span><span class="delimiter">&quot;</span></span>)
                  .monitorContinuously(<span class="predefined-type">Duration</span>.ofMillis(<span class="integer">5</span>))
                  .build();</code></pre>
<div class="ulist"><ul><li><p>Beispiel für Erstellung einer FileSource</p><div class="ulist"><ul><li><p>Liest Textzeilen aus allen Dateien in den Ordnern "/path/to/files" und "/path/to/other/files"</p></li><li><p>Prüft alle 5 Millisekunden auf Veränderungen</p><div class="ulist"><ul><li><p>Liest kontinuierlich neue oder veränderte Dateien ein</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_streamformat"><h2>Filesystem Connector: StreamFormat</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/reader/StreamFormat.html">StreamFormat</a></strong></p><div class="ulist"><ul><li><p>Legt als Teil einer <a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.html">FileSource</a> fest, wie Inhalte einer Datei in Datensätze übersetzt werden</p></li><li><p>Enthält Einstellung für Splitting der Inputdatei</p><div class="ulist"><ul><li><p><strong>State</strong> der FileSource enthält n Dateioffsets, wobei n die Anzahl der Splits ist</p></li></ul></div></li><li><p>Enthält Default-Einstellungen für IO-Batching und Checkpointing, die in den meisten Fällen sinnvoll sind</p></li></ul></div></li><li><p>Alternativ zu StreamFormat kann <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/reader/BulkFormat.html">BulkFormat</a> verwendet werden, das mehr Flexibilität bietet</p></li></ul></div></section>
<section id="filesystem_connector_simplestreamformat"><h2>Filesystem Connector: SimpleStreamFormat</h2><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/index.html?org/apache/flink/connector/file/src/reader/SimpleStreamFormat.html">SimpleStreamFormat</a></p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/reader/StreamFormat.html">StreamFormat</a> ohne Splitting</p><div class="ulist"><ul><li><p>&#8594; State der <a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.html">FileSource</a> enthält nur <strong>ein</strong> File Offset</p></li></ul></div></li></ul></div></li><li><p>Vordefinierte Beispiele:</p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java//org/apache/flink/connector/file/src/reader/TextLineInputFormat.html">TextLineInputFormat</a></p><div class="ulist"><ul><li><p>Liest Textdateien zeilenweise ein</p></li></ul></div></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/formats/csv/CsvReaderFormat.html">CsvReaderFormat</a></p></li></ul></div></li><li><p>Eigenes <strong>SimpleStreamFormat</strong> :</p><div class="ulist"><ul><li><p>Erfordert nur Angabe eines <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/src/reader/StreamFormat.Reader.html"><em>StreamFormat.Reader</em></a></p><div class="ulist"><ul><li><p>Reader liest seriell aus einer Datei und kann z.B. mittels java.io Klassen wie BufferedReader implementiert werden</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_simplestreamformat_beispiel"><h2>Filesystem Connector : SimpleStreamFormat Beispiel</h2><div class="ulist"><ul><li><p>Beispiel Implementierung von TextLineInputFormat (vereinfacht) :</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">TextLineInputFormat</span> <span class="directive">extends</span> SimpleStreamFormat&lt;<span class="predefined-type">String</span>&gt; {

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="predefined-type">Reader</span> createReader(<span class="predefined-type">Configuration</span> config, FSDataInputStream stream) <span class="directive">throws</span> <span class="exception">IOException</span> {
        <span class="predefined-type">BufferedReader</span> reader = <span class="keyword">new</span> <span class="predefined-type">BufferedReader</span>(<span class="keyword">new</span> <span class="predefined-type">InputStreamReader</span>(stream, <span class="string"><span class="delimiter">&quot;</span><span class="content">UTF-8</span><span class="delimiter">&quot;</span></span>));
        <span class="keyword">return</span> <span class="keyword">new</span> TextLineInputFormat.Reader(reader);
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> TypeInformation&lt;<span class="predefined-type">String</span>&gt; getProducedType() {
        <span class="keyword">return</span> <span class="predefined-type">Types</span>.STRING;
    }

    <span class="directive">public</span> <span class="directive">static</span> <span class="directive">final</span> <span class="type">class</span> <span class="class">Reader</span> <span class="directive">implements</span> StreamFormat.Reader&lt;<span class="predefined-type">String</span>&gt; {
        <span class="directive">private</span> <span class="directive">final</span> <span class="predefined-type">BufferedReader</span> reader;

        <span class="predefined-type">Reader</span>(<span class="predefined-type">BufferedReader</span> reader) {
            <span class="local-variable">this</span>.reader = reader;
        }
        <span class="directive">public</span> <span class="predefined-type">String</span> read() <span class="directive">throws</span> <span class="exception">IOException</span> {
            <span class="keyword">return</span> <span class="local-variable">this</span>.reader.readLine();
        }
        <span class="directive">public</span> <span class="type">void</span> close() <span class="directive">throws</span> <span class="exception">IOException</span> {
            <span class="local-variable">this</span>.reader.close();
        }
    }
}</code></pre></section>
<section id="filesystem_connector_filesink"><h2>Filesystem Connector: FileSink</h2><div class="ulist"><ul><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/sink/FileSink.html">FileSinks</a></strong> schreiben ihre Datensätze in <strong>Buckets</strong></p><div class="ulist"><ul><li><p>Buckets entsprechen einem Ordner (bei seriellen Senken einer Datei)</p></li><li><p>In gewissen zeitlichen Abständen (Default: einmal pro Stunde) wird der Bucket gewechselt</p></li></ul></div></li><li><p>Innerhalb eines Buckets schreiben Subtasks der Senke ihre Outputs in verschiedene <strong>Teile</strong></p></li><li><p>Gemäß einer <strong>rolling policy</strong> können die Teile ihre Zieldatei ändern</p><div class="ulist"><ul><li><p>z.B. Wechsel nach Erreichen einer maximalen Größe oder nach Zeit, oder bei Checkpoints</p></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_filesink_2"><h2>Filesystem Connector: FileSink (2)</h2><div class="imageblock" style=""><img src="images/streamfilesink_bucketing.png" alt="streamfilesink bucketing" height="600"></div>
<div class="paragraph center small"><small><em>(Bildquelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/filesystem/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/filesystem/</a>)</em></small></div></section>
<section id="filesystem_connector_filesink_3"><h2>Filesystem Connector: FileSink (3)</h2><div class="ulist"><ul><li><p>Beispiel für Erstellen einer <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/sink/FileSink.html"><strong>FileSink</strong></a>:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">final</span> FileSink&lt;<span class="predefined-type">String</span>&gt; sink = FileSink
    .forRowFormat(<span class="keyword">new</span> Path(outputPath), <span class="keyword">new</span> SimpleStringEncoder&lt;<span class="predefined-type">String</span>&gt;(<span class="string"><span class="delimiter">&quot;</span><span class="content">UTF-8</span><span class="delimiter">&quot;</span></span>))
    .withRollingPolicy(
        DefaultRollingPolicy.builder()
            .withRolloverInterval(<span class="predefined-type">Duration</span>.ofMinutes(<span class="integer">15</span>))
            .withInactivityInterval(<span class="predefined-type">Duration</span>.ofMinutes(<span class="integer">5</span>))
            .withMaxPartSize(MemorySize.ofMebiBytes(<span class="integer">1024</span>))
            .build())
        .build();</code></pre>
<div class="ulist"><ul><li><p>Mit <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/sink/FileSink.html#forRowFormat-org.apache.flink.core.fs.Path-org.apache.flink.api.common.serialization.Encoder-"><strong><em>forRowFormat</em></strong></a> ist die Angabe eines <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/serialization/Encoder.html"><strong>Encoder&lt;T&gt;</strong></a> nötig, der einzelne Datensätze serialisiert in eine Datei schreibt</p></li><li><p>Alternativ mit <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/sink/FileSink.html#forBulkFormat-org.apache.flink.core.fs.Path-org.apache.flink.api.common.serialization.BulkWriter.Factory-"><strong><em>forBulkFormat</em></strong></a> und Angabe einer <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/serialization/BulkWriter.Factory.html"><strong>BulkWriter&lt;T&gt;.Factory</strong></a></p><div class="ulist"><ul><li><p>Batching-Verhalten muss dann zusätzlich angegeben werden</p></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_filesink_4"><h2>Filesystem Connector: FileSink (4)</h2><div class="ulist"><ul><li><p>Beispiele für vordefinierte <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/serialization/BulkWriter.Factory.html"><strong>BulkWriter.Factory</strong></a> :</p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.11/api/java/org/apache/flink/formats/avro/AvroWriterFactory.html">AvroWriterFactory</a></p></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.10/api/java/org/apache/flink/formats/compress/CompressWriterFactory.html">CompressWriterFactory</a></p></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.9/api/java/org/apache/flink/formats/parquet/ParquetWriterFactory.html">ParquetWriterFactory</a></p></li></ul></div></li></ul></div></section>
<section id="filesystem_connector_link_zur_dokumentation"><h2>Filesystem Connector : Link zur Dokumentation</h2><div class="ulist"><ul><li><p>&#8594; Für mehr Details zum File System Connector siehe <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/filesystem/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/filesystem/</a></p></li></ul></div></section>
<section id="aufgabe_6"><h2>Aufgabe 6</h2><div class="olist arabic"><ol class="arabic"><li><p>Implementieren Sie eine <a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/api/java/org/apache/flink/connector/file/src/FileSource.html"><strong>FileSource</strong></a>, die eine Textdatei einliest und für jede Zeile einen int Array generiert</p><div class="ulist"><ul><li><p>Sie müssen die Dependency <a href="https://mvnrepository.com/artifact/org.apache.flink/flink-connector-files">flink-connector-files</a> nutzen</p></li><li><p>Die Zeilen der Inputdatei sollen als kommaseparierte ganze Zahlen angenommen werden</p></li><li><p>Erweitern Sie <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/index.html?org/apache/flink/connector/file/src/reader/SimpleStreamFormat.html">SimpleStreamFormat</a> und lassen Sie sich von der Implementierung von <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java//org/apache/flink/connector/file/src/reader/TextLineInputFormat.html">TextLineInputFormat</a> inspirieren</p><div class="ulist"><ul><li><p>Als <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/typeinfo/TypeInformation.html">TypeInformation&lt;int[</a>&gt;] kann <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/common/typeinfo/PrimitiveArrayTypeInfo.html">PrimitiveArrayTypeInfo.INT_PRIMITIVE_ARRAY_TYPE_INFO</a> gewählt werden</p></li></ul></div></li></ul></div></li><li><p>Implementieren Sie dann noch eine <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/file/sink/FileSink.html"><strong>FileSink</strong></a>, die int Arrays wieder als Zeilen in eine Textdatei schreibt</p><div class="ulist"><ul><li><p>separiert mit "+" statt ","</p></li><li><p>Als Beispiel für einen Encoder können Sie sich die Implementierung von <a href="https://nightlies.apache.org/flink/flink-docs-stable/api/java/index.html?org/apache/flink/api/common/serialization/SimpleStringEncoder.html">SimpleStringEncoder</a> ansehen</p></li></ul></div></li><li><p>Testen Sie ihre Quelle und Senke in einem Flink Job, bei dem Sie eine von Ihnen erstellte Inputdatei als DataStream&lt;int[]&gt; einlesen
und wieder in eine Datei schreiben</p></li></ol></div></section>
<section id="kafka_connector"><h2>Kafka Connector</h2><div class="paragraph heading center"><p>Kafka Connector</p></div></section>
<section id="kafka_connector_kafkasource"><h2>Kafka Connector : KafkaSource</h2><div class="ulist"><ul><li><p>Datenstreams in Kafka sind in <strong>Topics</strong> organisiert, die wiederum in <strong>Partitionen</strong> unterteilt sind</p><div class="ulist"><ul><li><p>Die Partitionen werden auf die Instanzen der <strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSource.html">KafkaSource</a></strong> aufgeteilt</p></li></ul></div></li><li><p><strong><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSource.html">KafkaSource</a></strong> wird über einen <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.html">KafkaBuilder</a> erstellt</p><div class="ulist"><ul><li><p>Beispiel:</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaSource&lt;<span class="predefined-type">String</span>&gt; source = KafkaSource.&lt;<span class="predefined-type">String</span>&gt;builder()
    .setBootstrapServers(brokers)
    .setTopics(<span class="string"><span class="delimiter">&quot;</span><span class="content">input-topic</span><span class="delimiter">&quot;</span></span>)
    .setGroupId(<span class="string"><span class="delimiter">&quot;</span><span class="content">my-group</span><span class="delimiter">&quot;</span></span>)
    .setStartingOffsets(OffsetsInitializer.earliest())
    .setValueOnlyDeserializer(<span class="keyword">new</span> SimpleStringSchema())
    .build();

env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string"><span class="delimiter">&quot;</span><span class="content">Kafka Source</span><span class="delimiter">&quot;</span></span>);</code></pre></section>
<section id="kafka_connector_kafkasource_2"><h2>Kafka Connector : KafkaSource (2)</h2><div class="ulist"><ul><li><p>Auswahl der Topics und Partitionen :</p><div class="ulist"><ul><li><p><strong><em>setTopics</em></strong> für alle Partitionen der angegebenen Topics</p></li><li><p>Oder Topics mit Pattern auswählen mit <strong><em>setTopicPattern</em></strong></p></li><li><p>Auswahl einzelner Partitionen (Beispiel):</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">final</span> <span class="predefined-type">HashSet</span>&lt;TopicPartition&gt; partitionSet = <span class="keyword">new</span> <span class="predefined-type">HashSet</span>&lt;&gt;(<span class="predefined-type">Arrays</span>.asList(
        <span class="keyword">new</span> TopicPartition(<span class="string"><span class="delimiter">&quot;</span><span class="content">topic-a</span><span class="delimiter">&quot;</span></span>, <span class="integer">0</span>),    <span class="comment">// Partition 0 of topic &quot;topic-a&quot;</span>
        <span class="keyword">new</span> TopicPartition(<span class="string"><span class="delimiter">&quot;</span><span class="content">topic-b</span><span class="delimiter">&quot;</span></span>, <span class="integer">5</span>)));  <span class="comment">// Partition 5 of topic &quot;topic-b&quot;</span>
KafkaSource.builder().setPartitions(partitionSet);</code></pre></section>
<section id="kafka_connector_kafkasource_3"><h2>Kafka Connector : KafkaSource (3)</h2><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.html">Builder</a>-Option <strong><em>setStartingOffsets(OffsetsInitializer)</em></strong> bezieht sich auf Offsets der Kafka-Partitionen</p></li><li><p>Mögliche Werte von <strong>OffsetsInitializer</strong> :</p><div class="ulist"><ul><li><p>OffsetsInitializer.earliest()</p></li><li><p>OffsetsInitializer.latest()</p></li><li><p>OffsetsInitializer.committedOffsets()</p><div class="ulist"><ul><li><p>Erstes commited Offset (Kafka-Feature)</p></li></ul></div></li><li><p>OffsetsInitializer.timestamp(&lt;timestamp&gt;)</p><div class="ulist"><ul><li><p>Erstes Offset ab dem Timestamp</p></li></ul></div></li><li><p>OffsetsInitializer.offsets(Map&lt;TopicPartition,Long&gt; offsets)</p><div class="ulist"><ul><li><p>individuell festgelegte Offsets</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="kafka_connector_kafkasource_4"><h2>Kafka Connector : KafkaSource (4)</h2><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.html">Builder</a>-Option <strong>setGroupId(groupName)</strong></p><div class="ulist"><ul><li><p>Legt den Namen einer Consumer Group (Kafka Feature) fest, zu der die Quelle gehören soll</p></li></ul></div></li><li><p><strong>Deserialisierung</strong> der Kafka-Datensätze (Builder-Optionen):</p><div class="ulist"><ul><li><p><em>setValueOnlyDeserializer(DeserializationSchema&lt;T&gt;)</em></p><div class="ulist"><ul><li><p>Benutzt nur Werte</p></li></ul></div></li><li><p><em>setDeserializer(KafkaRecordDeserializationSchema&lt;T&gt;)</em></p><div class="ulist"><ul><li><p>Zugriff auf Keys und Header möglich</p></li></ul></div></li></ul></div></li><li><p>Es gibt im Flink Kafka Connector vordefinierte Deserialisierer für die in Kafka gängigsten Serialisierer</p></li></ul></div></section>
<section id="kafka_connector_kafkasource_5"><h2>Kafka Connector : KafkaSource (5)</h2><div class="ulist"><ul><li><p><strong>Timestamps</strong> aus Kafka-Datensätzen werden per Default übernommen</p></li><li><p><strong>State</strong> der <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSource.html">KafkaSource</a> : Offsets für jede (Kafka-)Inputpartition</p></li><li><p>Einstellungen zur Sicherheit (Verschlüsselung, Authentifizierung) über den <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.html">Builder</a> möglich</p><div class="ulist"><ul><li><p>Beispiel :</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaSource
    .builder()
    .setProperty(<span class="string"><span class="delimiter">&quot;</span><span class="content">security.protocol</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">SASL_PLAINTEXT</span><span class="delimiter">&quot;</span></span>)
    .setProperty(<span class="string"><span class="delimiter">&quot;</span><span class="content">sasl.mechanism</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">PLAIN</span><span class="delimiter">&quot;</span></span>)
    .setProperty(<span class="string"><span class="delimiter">&quot;</span><span class="content">sasl.jaas.config</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.common.security.plain.PlainLoginModule required username=</span><span class="char">\&quot;</span><span class="content">username</span><span class="char">\&quot;</span><span class="content"> password=</span><span class="char">\&quot;</span><span class="content">password</span><span class="char">\&quot;</span><span class="content">;</span><span class="delimiter">&quot;</span></span>);</code></pre></section>
<section id="kafka_connector_kafkasink"><h2>Kafka Connector : KafkaSink</h2><div class="ulist"><ul><li><p>Erstellung einer <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/sink/KafkaSink.html"><strong>KafkaSink</strong></a> (Beispiel):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaSink&lt;<span class="predefined-type">String</span>&gt; sink = KafkaSink
        .&lt;<span class="predefined-type">String</span>&gt;builder()
        .setBootstrapServers(brokers)
        .setRecordSerializer(serializationschema)
        .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
        .build();</code></pre>
<div class="ulist"><ul><li><p>Angabe eines <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/kafka/sink/KafkaRecordSerializationSchema.html"><strong>KafkaRecordSerializationSchema</strong></a> für Datensätze nötig</p><div class="ulist"><ul><li><p>Beispiel für Erstellung:</p></li></ul></div></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">KafkaRecordSerializationSchema
    .builder()
    .setTopicSelector(MyUtil::selectTopicByElement)
    .setValueSerializationSchema(<span class="keyword">new</span> SimpleStringSchema())
    .setKeySerializationSchema(<span class="keyword">new</span> SimpleStringSchema())
    .setPartitioner(<span class="keyword">new</span> FlinkFixedPartitioner())
    .build();</code></pre></section>
<section id="kafka_connector_kafkasink_2"><h2>Kafka Connector : KafkaSink (2)</h2><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.3/api/java/org/apache/flink/streaming/connectors/kafka/partitioner/FlinkKafkaPartitioner.html"><strong>FlinkKafkaPartitioner</strong></a></p><div class="ulist"><ul><li><p>Ordnet den Datensätzen die Nummer der Partition des Kafka-Topics zu</p></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.3/api/java/org/apache/flink/streaming/connectors/kafka/partitioner/FlinkFixedPartitioner.html"><strong>FlinkFixedPartitioner</strong></a></p><div class="ulist"><ul><li><p>Ordnet Datensätze der gleichen Flink Partition immer der gleichen Kafka-Partition zu</p></li></ul></div></li></ul></div></li><li><p>Outputs können (Kafka-)Keys enthalten</p><div class="ulist"><ul><li><p>&#8594; SerializationSchema für Keys aus den Datensätzen (nicht den Flink-Keys) erforderlich</p></li></ul></div></li></ul></div></section>
<section id="kafka_connector_kafkasink_3"><h2>Kafka Connector : KafkaSink (3)</h2><div class="ulist"><ul><li><p>Mit der Angabe einer <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/base/DeliveryGuarantee.html"><strong>DeliveryGuarantee</strong></a> lassen sich eine von 3 möglichen Verhaltensweisen der KafkaSink auswählen:</p><div class="ulist"><ul><li><p>DeliveryGuarantee.NONE</p><div class="ulist"><ul><li><p>Die Senke verwendet keine Mechanismen zum Erreichen von end-to-end Konsistenz</p></li></ul></div></li><li><p>DeliveryGuarantee.AT_LEAST_ONCE</p><div class="ulist"><ul><li><p>Die Senke wartet auf eine Bestätigung des Kafka Brokers über den Erhalt aller ausstehenden Datensätze vor jedem Checkpoint und schickt
sie ggf. erneut</p></li></ul></div></li><li><p>DeliveryGuarantee.EXACTLY_ONCE</p><div class="ulist"><ul><li><p>Verwendet zusätzlich den vorher besprochenen Two-Phase-Commit Mechanismus</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="kafka_connector_kafkasink_4"><h2>Kafka Connector : KafkaSink (4)</h2><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/connector/base/DeliveryGuarantee.html">DeliveryGuarantee.EXACTLY_ONCE</a></p><div class="ulist"><ul><li><p>Konsumenten der Kafka-Topics sollten eingestellt sein, nur "committed" Daten zu lesen</p><div class="ulist"><ul><li><p>Wenn sie auf garantierte Konsistenz angewiesen sind</p></li><li><p>Oder eigenes Rollback implementieren</p></li></ul></div></li><li><p>Im Kafkacluster sollte der folgende Parameter groß genug eingestellt sein (Dauer Checkpointing plus Puffer für Recovery):</p><div class="ulist"><ul><li><p><em>producer transaction.timeout</em></p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="eigener_connector"><h2>Eigener Connector</h2><div class="paragraph heading center"><p>Eigener Connector</p></div></section>
<section id="custom_source"><h2>Custom Source</h2><div class="ulist"><ul><li><p>Quellen in Flink teilen zu konsumierende Daten in sog. Splits ein</p></li><li><p>Eine Quelle besteht allgemein aus den folgenden Komponenten:</p><div class="ulist"><ul><li><p>Split Enumerator : generiert, organisiert und verteilt Splits</p><div class="ulist"><ul><li><p>pro Quelle nur eine Instanz</p></li></ul></div></li><li><p>SourceReader : erhält Splits und extrahiert Datensätze aus diesen</p><div class="ulist"><ul><li><p>Anzahl der Instanzen entspricht Parallelismus der Quelle</p></li></ul></div></li><li><p>SplitSerializer : Serialisiert Splits für den Transfer zwischen Enumerator und Reader und für Checkpointing</p></li></ul></div></li><li><p>Eine selbst erstellte Quelle muss diese Komponenten implementieren</p></li><li><p>Für Details siehe <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/sources/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/sources/</a></p></li></ul></div></section>
<section id="custom_sink"><h2>Custom Sink</h2><div class="ulist"><ul><li><p>Um eine eigene Senke zu erstellen, muss im Sink&lt;T&gt; interface nur die createWriter(Sink.InitContext context) Methode implementiert werden</p><div class="ulist"><ul><li><p>diese Methode wird bei der Erstellung eines Senken-Subtasks aufgerufen</p></li><li><p>Ein Sink&lt;T&gt;.InitContext enthält Metainformationen über den Job, Task und Subtask</p></li></ul></div></li><li><p>Um einen SinkWriter&lt;T&gt; zu erstellen, sind folgende Methoden zu implementieren:</p><div class="ulist"><ul><li><p>void write(T element, SinkWriter.Context context)</p><div class="ulist"><ul><li><p>schreibt ein Element an das Zielsystem (oder gibt an einen Writer weiter)</p></li></ul></div></li><li><p>void flush(boolean endOfInput)</p><div class="ulist"><ul><li><p>wird bei einem Checkpoint oder Ende des Inputs aufgerufen und löst Flushen aller verbleibenden Outputs des Writers aus</p></li></ul></div></li></ul></div></li><li><p>Falls Watermarks an das Zielsystem propagiert werden sollen, kann zusätzlich</p><div class="ulist"><ul><li><p>default void writeWatermark(Watermark watermark) implementiert werden</p></li></ul></div></li></ul></div></section>
<section id="custom_sink_2"><h2>Custom Sink (2)</h2><div class="ulist"><ul><li><p>Für Senken mit erweiterter Funktionalität gibt es u.a. folgende vordefinierte Subinterfaces von Sink&lt;T&gt; :</p><div class="ulist"><ul><li><p>StatefulSink&lt;T, WriterStateT&gt;</p></li><li><p>TwoPhaseCommittingSink&lt;InputT,CommT&gt;</p></li></ul></div></li><li><p>Zusätzlich bündelt die abstrakte Klasse AsyncSinkBase&lt;T,RequestEntryT&gt; die Unterstützung für Funktionalität von nicht-transaktionalen Senken, die ihren
Output asynchron in Batches schicken</p></li></ul></div></section>
<section id="asynchronous_io"><h2>Asynchronous I/O</h2><div class="paragraph heading center"><p>Asynchronous I/O</p></div></section>
<section id="asynchronous_io_for_external_data_access"><h2>Asynchronous I/O for External Data Access</h2><div class="ulist"><ul><li><p>Async I/O API</p><div class="ulist"><ul><li><p>Ermöglicht (non-blocking) Abfragen gegen externe Systeme</p></li><li><p>Erfordert Angabe einer AsyncFunction</p><div class="ulist"><ul><li><p>Kann von allen Operatoren bei der Bearbeitung eines Datensatzes verwendet werden</p></li></ul></div></li></ul></div></li><li><p>Interface <strong>RichAsyncFunction&lt;IN,OUT&gt;</strong></p><div class="ulist"><ul><li><p><em>void open(Configuration)</em></p><div class="ulist"><ul><li><p>Verbindung zum externen System initial herstellen</p></li></ul></div></li><li><p><em>void asyncInvoke(IN, ResultFuture&lt;OUT&gt;)</em></p><div class="ulist"><ul><li><p>Bearbeitung eines Datensatzes</p></li></ul></div></li><li><p><em>void close()</em></p><div class="ulist"><ul><li><p>Verbindung schließen</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="asynchronous_io_for_external_data_access_2"><h2>Asynchronous I/O for External Data Access (2)</h2><div class="ulist"><ul><li><p>ResultFuture&lt;OUT&gt; hat eine Methode</p><div class="ulist"><ul><li><p>void complete(Collection&lt;OUT&gt; output),</p></li></ul></div></li><li><p>die nach Vollendung der asynchronen Operation mit dem Output ausgeführt werden kann</p></li><li><p>Um eine AsyncFunction in eine Pipeline einzubinden, werden statische Methoden auf der Klasse AsyncDataStream verwendet, Beispiel:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="comment">// apply the async I/O transformation without retry</span>
DataStream&lt;Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt;&gt; resultStream =
    AsyncDataStream.unorderedWait(stream, <span class="keyword">new</span> AsyncDatabaseRequest(), <span class="integer">1000</span>, <span class="predefined-type">TimeUnit</span>.MILLISECONDS, <span class="integer">100</span>);

<span class="comment">// or apply the async I/O transformation with retry</span>
<span class="comment">// create an async retry strategy via utility class or a user defined strategy</span>
AsyncRetryStrategy asyncRetryStrategy =
        <span class="keyword">new</span> AsyncRetryStrategies.FixedDelayRetryStrategyBuilder(<span class="integer">3</span>, <span class="integer">100L</span>) <span class="comment">// maxAttempts=3, fixedDelay=100ms</span>
                .ifResult(RetryPredicates.EMPTY_RESULT_PREDICATE)
                .ifException(RetryPredicates.HAS_EXCEPTION_PREDICATE)
                .build();

<span class="comment">// apply the async I/O transformation with retry</span>
DataStream&lt;Tuple2&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt;&gt; resultStream =
        AsyncDataStream.unorderedWaitWithRetry(stream, <span class="keyword">new</span> AsyncDatabaseRequest(), <span class="integer">1000</span>, <span class="predefined-type">TimeUnit</span>.MILLISECONDS, <span class="integer">100</span>, asyncRetryStrategy);</code></pre></section>
<section id="asynchronous_io_for_external_data_access_3"><h2>Asynchronous I/O for External Data Access (3)</h2><div class="ulist"><ul><li><p>Codebeispiel aus: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/</a></p><div class="ulist"><ul><li><p>siehe auch dort für Details zur Konfiguration von Timeouts, Retries und mehr</p></li></ul></div></li></ul></div></section>
<section id="aufgabe_7"><h2>Aufgabe 7</h2><div class="olist arabic"><ol class="arabic"><li><p>Öffnen Sie die Vorlage zu Aufgabe 7 im Git-Repository der Schulung</p><div class="ulist"><ul><li><p>führen Sie einmal "mvn compile" aus, um sich die fehlenden Dependencies herunterzuladen</p><div class="ulist"><ul><li><p>Ziel ist es, einen Testjob zu erstellen, der folgendes tut:</p></li></ul></div></li><li><p>Testdaten (IDs von Kunden) in ein Kafka Topic schreiben</p></li><li><p>Diese danach wieder aus dem Topic auslesen und asynchron über eine Datenbank die Kundennamen passend zu den IDs abfragen</p></li><li><p>Die Kundendaten auf dem Bildschirm ausgeben</p><div class="ulist"><ul><li><p>Hierfür ist das Gerüst des Tests schon vorgegeben. Es enthält eine Konfiguration von Dockercontainern für Kafka und eine Postgres Datenbank mit Hilfe der Open-Source Library Testcontainers</p></li></ul></div></li></ul></div></li><li><p>Ihre Aufgabe ist, an den beiden markierten Stellen im Quellcode der Klasse "Aufgabe7Test" je einen geeigneten Flinkjob zu implementieren, wobei erst auch noch
die Implementierung von AsyncCustomerLookup abgeschlossen werden muss</p></li><li><p>Testen Sie ihren Code durch Ausführen von "mvn test"</p></li></ol></div></section>
<section id="aufgabe_7_hinweise"><h2>Aufgabe 7 (Hinweise)</h2><div class="ulist"><ul><li><p>Wir verwenden die Dependency flink-clients, um für Tests bei Aufruf von StreamExecutionEnvironment.getExecutionEnvironment() ein lokales ExecutionEnvironment zu erhalten</p><div class="ulist"><ul><li><p>dies ermöglicht es, Flinkjobs zu testen, ohne sie in einen Flinkcluster zu laden</p></li></ul></div></li><li><p>Die Konfiguration der Testklasse ist so, dass die benötigten Container jeweils vor Ausführung der Tests gestartet werden</p><div class="ulist"><ul><li><p>Hierfür muss ein Docker Execution Environment auf Ihrer Maschine gestartet sein</p></li><li><p>Beim ersten Start der Tests müssen wahrscheinlich erst noch die Containerimages geladen werden, daher dauert es etwas länger</p></li></ul></div></li><li><p>Eine KafkaSource kann im Builder mit .setBounded(OffsetsInitializer.latest()) so konfiguriert werden, dass der Job abbricht,
wenn das aktuell letzte Offset des Inputtopics geladen wurde (Batchmodus)</p><div class="ulist"><ul><li><p>per default wartet die Source unbeschränkt auf weitere Datensätze im Stream (Streamingmodus)</p></li></ul></div></li></ul></div></section>
<section id="einführung_flink_table_api"><h2>Einführung Flink Table API</h2><div class="paragraph heading center"><p>Einführung Flink Table API</p></div></section>
<section id="flink_table_api_kurzeinführung"><h2>Flink Table API: Kurzeinführung</h2><div class="ulist"><ul><li><p>inspiriert von relationalen Datenbanken</p><div class="ulist"><ul><li><p>API-Calls ähneln SQL-Syntax, aber mit fluent API statt Befehlen als Strings</p></li></ul></div></li><li><p>Grundabstraktion ist das "Table" Interface (analog zu DataStream in Streams API)</p></li><li><p>Eigentliche Streams werden als Folge von Inserts und Updates auf Tabellen interpretiert</p></li><li><p>benötigt Erstellung eines TableEnvironment (z.B. StreamTableEnvironment ) analog zu ExeutionEnvironment für Streams API</p></li><li><p>Streams und Table API sind kombinierbar, es gibt Möglichkeiten zwischen DataStream und Table zu konvertieren:</p><div class="ulist"><ul><li><p>Komponenten von Datensätzen in DataStreams können als Spalten einer Table genommen werden</p><div class="ulist"><ul><li><p>einzelne Datensätze im Stream sind Inserts oder auch Updates (bei gleichem Primärschlüssel) in die Tabelle</p></li></ul></div></li><li><p>entsprechend können Zeilen in Tabellen zu Stream-Datensätzen konvertiert werden</p></li></ul></div></li><li><p>Table API kann prinzipiell alles, was die Streams API auch kann</p></li></ul></div></section>
<section id="flink_table_api_kurzeinführung_2"><h2>Flink Table API: Kurzeinführung (2)</h2><div class="ulist"><ul><li><p>Beispiel:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">EnvironmentSettings settings = EnvironmentSettings
    .newInstance()
    .inStreamingMode()
    .build();

TableEnvironment tEnv = TableEnvironment.create(settings);

<span class="comment">// register Orders table in table environment</span>
<span class="comment">// ...</span>

<span class="comment">// specify table program</span>
Table orders = tEnv.from(<span class="string"><span class="delimiter">&quot;</span><span class="content">Orders</span><span class="delimiter">&quot;</span></span>); <span class="comment">// Orders Tabelle folgt dem Schema (a, b, c, rowtime), wurde vorher registriert (nicht gezeigt)</span>

Table counts = orders
        .groupBy(<span class="error">$</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">a</span><span class="delimiter">&quot;</span></span>))
        .select(<span class="error">$</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">a</span><span class="delimiter">&quot;</span></span>), <span class="error">$</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">b</span><span class="delimiter">&quot;</span></span>).count().as(<span class="string"><span class="delimiter">&quot;</span><span class="content">cnt</span><span class="delimiter">&quot;</span></span>));

counts.execute().print();</code></pre>
<div class="ulist"><ul><li><p>Codebeispiel und mehr Details auf <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/</a></p></li><li><p>Ein Unterschied zur Streams API: "Table" ist kein generischer Typ. Statt dessen gibt es für das Tabellenschema einen spezielle Klasse "Schema"</p></li></ul></div></section>
<section id="machine_learning_mit_flink_ml"><h2>Machine Learning mit Flink ML</h2><div class="paragraph heading center"><p>Machine Learning mit Flink ML</p></div></section>
<section id="machine_learning_mit_flink_ml_2"><h2>Machine Learning mit Flink ML</h2><div class="ulist"><ul><li><p>Flink ML ist eine Library, die wie Flink selbst von der Flink Community entwickelt wird</p></li><li><p>Stellt eine API zu Verfügung, um Pipelines (oder Graphen) für ML-Jobs zu bauen</p></li><li><p>Enthält gebundled bereits einige grundlegende ML Algorithmen</p><div class="ulist"><ul><li><p>Klassifikation: K nearest neighbours (KNN), Naive Bayes, ..</p></li><li><p>Clustering: KMeans, ..</p></li><li><p>Feature Engineering: diverse kleinere Algorithmen zum Transformieren von Daten und Extrahieren von Features</p></li></ul></div></li><li><p>API ist high-level und basiert auf der <strong>Flink Table API</strong></p></li><li><p>Für Java müssen zur Verwendung von Flink ML folgende Dependencies eingebunden werden:</p><div class="ulist"><ul><li><p>flink-table-api-java-bridge (für die Table API)</p></li><li><p>flink-ml-uber (für Flink ML selbst)</p></li></ul></div></li></ul></div></section>
<section id="flink_ml_komponenten"><h2>Flink ML Komponenten</h2><div class="ulist"><ul><li><p>Komponenten von Flink ML-Jobs (Interfacetypen):</p><div class="ulist"><ul><li><p>AlgoOperator</p><div class="ulist"><ul><li><p>Abstraktion für beliebige Operatoren mit n Inputtabellen und m Outputtabellen</p></li></ul></div></li><li><p>Transformer</p><div class="ulist"><ul><li><p>Spezieller AlgoOperator</p></li><li><p>Für 1:1 Mapping von einzelnen Datensätzen (keine Aggregation möglich)</p></li></ul></div></li><li><p>Model</p><div class="ulist"><ul><li><p>Spezieller Transformer</p></li><li><p>Hat einen State (ModelData), der eine Anpassung des Models an Trainingsdaten repräsentiert</p></li></ul></div></li><li><p>Estimator</p><div class="ulist"><ul><li><p>Generiert Model (inkl. ModelData) aus Inputtabellen</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="flink_ml_beispiel_tokenizer"><h2>Flink ML Beispiel (Tokenizer)</h2><div class="ulist"><ul><li><p>Beispiel zur Anwendung eines Transformers (Tokenizer):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

        <span class="comment">// Generates input data.</span>
        DataStream&lt;Row&gt; inputStream =
                env.fromElements(Row.of(<span class="string"><span class="delimiter">&quot;</span><span class="content">Test for tokenization.</span><span class="delimiter">&quot;</span></span>), Row.of(<span class="string"><span class="delimiter">&quot;</span><span class="content">Te,st. punct</span><span class="delimiter">&quot;</span></span>));
        Table inputTable = tEnv.fromDataStream(inputStream).as(<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>);

        <span class="comment">// Creates a Tokenizer object and initializes its parameters.</span>
        Tokenizer tokenizer = <span class="keyword">new</span> Tokenizer().setInputCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>).setOutputCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">output</span><span class="delimiter">&quot;</span></span>);

        <span class="comment">// Uses the Tokenizer object for feature transformations.</span>
        Table outputTable = tokenizer.transform(inputTable)[<span class="integer">0</span>];

        <span class="comment">// Extracts and displays the results.</span>
        <span class="keyword">for</span> (CloseableIterator&lt;Row&gt; it = outputTable.execute().collect(); it.hasNext(); ) {
            Row row = it.next();

            <span class="predefined-type">String</span> inputValue = (<span class="predefined-type">String</span>) row.getField(tokenizer.getInputCol());
            <span class="predefined-type">String</span><span class="type">[]</span> outputValues = (<span class="predefined-type">String</span><span class="type">[]</span>) row.getField(tokenizer.getOutputCol());

            <span class="predefined-type">System</span>.out.printf(
                    <span class="string"><span class="delimiter">&quot;</span><span class="content">Input Value: %s </span><span class="char">\t</span><span class="content">Output Values: %s</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>,
                    inputValue, <span class="predefined-type">Arrays</span>.toString(outputValues));
        }</code></pre>
<div class="ulist"><ul><li><p>Beispielcode aus aus org.apache.flink.ml.examples.feature.TokenizerExample</p></li></ul></div></section>
<section id="flink_ml_beispiel_naivebayes"><h2>Flink ML Beispiel (NaiveBayes)</h2><div class="ulist"><ul><li><p>Beispiel zur Anwendung eines Estimators, der ein Model erstellt (NaiveBayes):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">        <span class="comment">// Generates input training and prediction data.</span>
        DataStream&lt;Row&gt; trainStream =
                env.fromElements(
                        Row.of(Vectors.dense(<span class="integer">0</span>, <span class="integer">0</span>.), <span class="integer">11</span>), (...weitere...));
        Table trainTable = tEnv.fromDataStream(trainStream).as(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">label</span><span class="delimiter">&quot;</span></span>);

        DataStream&lt;Row&gt; predictStream =
                env.fromElements(
                        Row.of(Vectors.dense(<span class="integer">0</span>, <span class="integer">1</span>.)), (...weitere...));
        Table predictTable = tEnv.fromDataStream(predictStream).as(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>);

        <span class="comment">// Creates a NaiveBayes object and initializes its parameters.</span>
        NaiveBayes naiveBayes =
                <span class="keyword">new</span> NaiveBayes()
                        .setSmoothing(<span class="float">1.0</span>)
                        .setFeaturesCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>)
                        .setLabelCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">label</span><span class="delimiter">&quot;</span></span>)
                        .setPredictionCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">prediction</span><span class="delimiter">&quot;</span></span>)
                        .setModelType(<span class="string"><span class="delimiter">&quot;</span><span class="content">multinomial</span><span class="delimiter">&quot;</span></span>);

        <span class="comment">// Trains the NaiveBayes Model.</span>
        NaiveBayesModel naiveBayesModel = naiveBayes.fit(trainTable);

        <span class="comment">// Uses the NaiveBayes Model for predictions.</span>
        Table outputTable = naiveBayesModel.transform(predictTable)[<span class="integer">0</span>];</code></pre>
<div class="ulist"><ul><li><p>Beispielcode abgekürzt aus aus org.apache.flink.ml.examples.classification.NaiveBayesExample</p></li></ul></div></section>
<section id="flink_ml_pipeline"><h2>Flink ML : Pipeline</h2><div class="ulist"><ul><li><p>Pipeline</p><div class="ulist"><ul><li><p>Basisklasse für einfache Flink ML-Jobs</p></li><li><p>Besteht aus Stages</p><div class="ulist"><ul><li><p>AlgoOperator und Estimator sind Stages</p></li></ul></div></li><li><p>Ist selbst ein Estimator</p></li></ul></div></li><li><p>Erstellung eines <strong>PipelineModel</strong> aus einer Pipeline folgt folgendem Algorithmus:</p><div class="ulist"><ul><li><p>Führe die Stages hintereinander aus</p><div class="ulist"><ul><li><p>Die erste Stage erhält gegebene Tabellen (Parameter) als Input</p></li><li><p>Jede folgende Stage erhält als Input den Output der vorherigen Stage</p></li></ul></div></li><li><p>Wenn eine Stage ein Estimator ist, <strong>fitte</strong> diesen erst mit seinem Input</p><div class="ulist"><ul><li><p>Wende dann das erhaltene Model auf den (gleichen) Input an</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="flink_ml_pipeline_2"><h2>Flink ML : Pipeline (2)</h2><div class="ulist"><ul><li><p>zu beachten ist noch, dass Input und Output einer Pipeline und aller Operatoren in der Pipeline immer ein Array von Tables ist</p></li><li><p>Beispiel zur Erstellung einer Pipeline (aus der Flink-Dokumentation):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="comment">// Suppose SumModel is a concrete subclass of Model, SumEstimator is a concrete subclass of Estimator.</span>

Model modelA = <span class="keyword">new</span> SumModel().setModelData(tEnv.fromValues(<span class="integer">10</span>));
Estimator estimatorA = <span class="keyword">new</span> SumEstimator();
Model modelB = <span class="keyword">new</span> SumModel().setModelData(tEnv.fromValues(<span class="integer">30</span>));

<span class="predefined-type">List</span>&lt;Stage&lt;?&gt;&gt; stages = <span class="predefined-type">Arrays</span>.asList(modelA, estimatorA, modelB);
Estimator&lt;?, ?&gt; estimator = <span class="keyword">new</span> Pipeline(stages);</code></pre>
<div class="ulist"><ul><li><p>ergibt eine Pipeline der Form</p><div class="ulist"><ul><li><p>modelA &#8594; estimatorA &#8594; modelB</p></li></ul></div></li><li><p>nach Fitten mit Daten (nicht im Code gezeigt) erhält man ein PipelineModel der Form</p><div class="ulist"><ul><li><p>modelA &#8594; model_from_estimatorA &#8594; modelB</p></li></ul></div></li><li><p>da Pipelines bzw. PipelineModels Estimators bzw. Models sind, können sie auch selbst als Bausteine für weitere Pipelines verwendet werden</p></li></ul></div></section>
<section id="flink_ml_offline_und_online_learning"><h2>Flink ML : Offline und Online Learning</h2><div class="ulist"><ul><li><p>Offline Learning mit Flink ML:</p><div class="ulist"><ul><li><p>Trainingsdaten sind beschränkte Streams</p></li><li><p>Nach Generieren der Models können die Pipelines zur Prediction auf beschränkte oder unbeschränkte Inputstreams angewendet werden</p></li></ul></div></li><li><p>Online Learning mit Flink ML:</p><div class="ulist"><ul><li><p>Trainingsdaten sind unbeschränkte Streams</p></li><li><p>Models werden kontinuierlich angepasst</p></li></ul></div></li><li><p>Estimators und Pipelines unterstützen beide Varianten</p></li><li><p>Manche vordefinierten Algorithmen von Flink ML haben eine Online-Variante</p><div class="ulist"><ul><li><p>z.B. OnlineKMeans, OnlineLogisticRegression</p></li></ul></div></li></ul></div></section>
<section id="flink_ml_graph"><h2>Flink ML : Graph</h2><div class="ulist"><ul><li><p>Für komplexere ML-Jobs, die sich nicht durch eine einfache Pipeline beschreiben lassen, gibt es noch die Klasse Graph</p></li><li><p>Ein Graph besteht aus GraphNodes, die jeweils einen AlgOperator oder Estimator enthalten</p></li><li><p>GraphNodes können Input aus mehreren GraphNodes erhalten und Output an mehrere GraphNodes übergeben (Kanten im Graphen)</p><div class="ulist"><ul><li><p>Graph ist konzeptuell ähnlich wie ein JobGraph</p></li></ul></div></li><li><p>Es gibt NodeIds für die Knoten und TableIds für die Tabellen</p><div class="ulist"><ul><li><p>über diese wird in der API bei der Erstellung einer GraphNode festgelegt, welche Outputtabellen von anderen GraphNodes diese als Input erhalten soll</p><div class="ulist"><ul><li><p>die initialen Inputtabellen haben natürlich auch IDs</p></li></ul></div></li></ul></div></li><li><p>Der Graph muss azyklisch sein (darf keine Kreise enthalten)</p><div class="ulist"><ul><li><p>für iterative Operationen, bei denen Outputs eines Operators wieder vom gleichen Operator bearbeitet werden sollen, gibt es eine spezielle API (kommt später)</p></li></ul></div></li></ul></div></section>
<section id="flink_ml_graph_2"><h2>Flink ML : Graph (2)</h2><div class="ulist"><ul><li><p>Für das Erstellen eines GraphModel wird in der API ein GraphBuilder verwendet</p></li><li><p>Anders als ein PipelineModel enthält ein GraphModel weiterhin die im Builder angegebenen Estimators</p><div class="ulist"><ul><li><p>bei jeder Anwendung des GraphModels (transform Operation) wird der Graph in einer topologischen Ordnung der Knoten durchlaufen</p></li><li><p>wenn ein Estimator durchlaufen wird, wird ähnlich wie bei der Erstellung eines PipelineModel erst ein Model durch Fitten der Inputs erstellt</p></li><li><p>dann wird dieses Model auf den Input erneut angewendet</p></li></ul></div></li><li><p>Im GraphBuilder wird festgelegt, welche Nodes es gibt, und wie diese miteinander verdrahtet werden sollen</p></li><li><p>mit builder.createTableId() wird eine neue Id für eine Inputtabelle des Graphen erstellt</p><div class="ulist"><ul><li><p>diese kann bei Erstellung von neuen Nodes als Input angegeben werden</p></li></ul></div></li><li><p>hinzufügen eines Operators oder Estimators gibt einen Array von TableIds für den Output zurück</p><div class="ulist"><ul><li><p>diese können in der Definition weiterer Operatoren verwendet werden</p></li></ul></div></li></ul></div></section>
<section id="flink_ml_graph_3"><h2>Flink ML : Graph (3)</h2><div class="ulist"><ul><li><p>Bei Bauen des Models mit builder.buildModel wird angegeben, welche TableIds als Input bzw. Output des gesamten Graphen verwendet werden sollen</p><div class="ulist"><ul><li><p>hierbei können gesondert TableIds für Inputs bzw. Outputs von Aufrufen von getModelData bzw. setModelData auf GraphNodes, die Models sind, festgelegt werden</p></li></ul></div></li><li><p>Beispiel zur Erstellung eines Graph (aus der Flink-Dokumentation):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">GraphBuilder builder = <span class="keyword">new</span> GraphBuilder();
<span class="comment">// Creates nodes.</span>
SumModel stage1 = <span class="keyword">new</span> SumModel().setModelData(tEnv.fromValues(<span class="integer">1</span>));
SumModel stage2 = <span class="keyword">new</span> SumModel();
SumModel stage3 = <span class="keyword">new</span> SumModel().setModelData(tEnv.fromValues(<span class="integer">3</span>));
<span class="comment">// Creates inputs and modelDataInputs.</span>
TableId input = builder.createTableId();
TableId modelDataInput = builder.createTableId();
<span class="comment">// Feeds inputs and gets outputs.</span>
TableId output1 = builder.addAlgoOperator(stage1, input)[<span class="integer">0</span>];
TableId output2 = builder.addAlgoOperator(stage2, output1)[<span class="integer">0</span>];
builder.setModelDataOnModel(stage2, modelDataInput);
TableId output3 = builder.addAlgoOperator(stage3, output2)[<span class="integer">0</span>];
TableId modelDataOutput = builder.getModelDataFromModel(stage3)[<span class="integer">0</span>];

<span class="comment">// Builds a Model from the graph.</span>
TableId<span class="type">[]</span> inputs = <span class="keyword">new</span> TableId<span class="type">[]</span> {input};
TableId<span class="type">[]</span> outputs = <span class="keyword">new</span> TableId<span class="type">[]</span> {output3};
TableId<span class="type">[]</span> modelDataInputs = <span class="keyword">new</span> TableId<span class="type">[]</span> {modelDataInput};
TableId<span class="type">[]</span> modelDataOutputs = <span class="keyword">new</span> TableId<span class="type">[]</span> {modelDataOutput};
Model&lt;?&gt; model = builder.buildModel(inputs, outputs, modelDataInputs, modelDataOutputs);</code></pre></section>
<section id="flink_ml_graph_4"><h2>Flink ML : Graph (4)</h2><div class="ulist"><ul><li><p>Der gerade gezeigte Code generiert einen Graph der folgenden Form:</p></li></ul></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="imageblock" style=""><img src="images/eigene/ml_Graph.svg" alt="ml Graph" width="1500"></div>
<div class="paragraph"><p>&#160;<br></p></div>
<div class="ulist"><ul><li><p>Der Graph selbst hat genau die Nodes "stage1", "stage2", "stage3"</p></li><li><p>"Input", "Model Data Input", "Output", "Model Data Output" repräsentieren Input und Output, wenn das generierte GraphModel als Transformation verwendet wird</p></li></ul></div></section>
<section id="flink_ml_iterations"><h2>Flink ML : Iterations</h2><div class="ulist"><ul><li><p>Flink ML <strong>Iterations API</strong> basiert auf DataStreams API</p></li><li><p>Für die Definition einer <strong>iterativen Operation</strong> werden benötigt :</p><div class="ulist"><ul><li><p>Eine Liste von initialen Werten für die <strong>variablen</strong> Datenstreams</p></li><li><p>Eine zusätzliche Liste von <strong>nicht variablen</strong> Datenstreams</p></li><li><p>Einen <strong>IterationBody</strong>, der den iterativen Code enthält</p><div class="ulist"><ul><li><p>Dieser enthält optional eine Abbruchbedingung (&#8594; bounded Iteration, sonst unbounded)</p></li></ul></div></li><li><p>Eine Konfiguration</p></li></ul></div></li><li><p>Bei jeder Iteration:</p><div class="ulist"><ul><li><p>IterationBody wird durchlaufen</p></li><li><p>Variable Streams erhalten ein Updates</p></li><li><p>Es wird ein Output emittiert</p></li></ul></div></li></ul></div></section>
<section id="flink_ml_iterations_2"><h2>Flink ML : Iterations (2)</h2><div class="ulist"><ul><li><p>Beispiel für Iterations API : Implementierung von Bounded KMeans (Ausschnitt)</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">        StreamTableEnvironment tEnv =
                (StreamTableEnvironment) ((TableImpl) inputs[<span class="integer">0</span>]).getTableEnvironment();
        DataStream&lt;DenseVector&gt; points =
                tEnv.toDataStream(inputs[<span class="integer">0</span>])
                        .map(row -&gt; ((<span class="predefined-type">Vector</span>) row.getField(getFeaturesCol())).toDense());

        DataStream&lt;DenseVector<span class="type">[]</span>&gt; initCentroids = selectRandomCentroids(points, getK(), getSeed());

        IterationConfig config =
                IterationConfig.newBuilder()
                        .setOperatorLifeCycle(IterationConfig.OperatorLifeCycle.ALL_ROUND)
                        .build();

        IterationBody body =
                <span class="keyword">new</span> KMeansIterationBody(  <span class="comment">// Abbruchbedingung : max. Anzahl von Iterationen</span>
                        getMaxIter(), DistanceMeasure.getInstance(getDistanceMeasure()));

        DataStream&lt;KMeansModelData&gt; finalModelData =
                Iterations.iterateBoundedStreamsUntilTermination(
                                DataStreamList.of(initCentroids),  <span class="comment">// variable Streams</span>
                                ReplayableDataStreamList.notReplay(points),  <span class="comment">// konstante Streams</span>
                                config,
                                body)
                        .get(<span class="integer">0</span>);

        Table finalModelDataTable = tEnv.fromDataStream(finalModelData);</code></pre></section>
<section id="flink_ml_beispiel_kmeans"><h2>Flink ML : Beispiel KMeans</h2><div class="ulist"><ul><li><p>Beispiel zur Anwendung von KMeans (abgekürzt aus org.apache.flink.ml.examples.clustering.KMeansExample):</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

        <span class="comment">// Generates input data.</span>
        DataStream&lt;DenseVector&gt; inputStream =
                env.fromElements(
                        Vectors.dense(<span class="float">0.0</span>, <span class="float">0.0</span>),
                        Vectors.dense(<span class="float">0.0</span>, <span class="float">0.3</span>),
                        (... weitere...) );
        Table inputTable = tEnv.fromDataStream(inputStream).as(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>);

        <span class="comment">// Creates a K-means object and initializes its parameters.</span>
        KMeans kmeans = <span class="keyword">new</span> KMeans().setK(<span class="integer">2</span>).setSeed(<span class="integer">1L</span>);

        <span class="comment">// Trains the K-means Model.</span>
        KMeansModel kmeansModel = kmeans.fit(inputTable);

        <span class="comment">// Uses the K-means Model for predictions.</span>
        Table outputTable = kmeansModel.transform(inputTable)[<span class="integer">0</span>];

        <span class="comment">// Extracts and displays the results.</span>
        <span class="keyword">for</span> (CloseableIterator&lt;Row&gt; it = outputTable.execute().collect(); it.hasNext(); ) {
            Row row = it.next();
            DenseVector features = (DenseVector) row.getField(kmeans.getFeaturesCol());
            <span class="type">int</span> clusterId = (<span class="predefined-type">Integer</span>) row.getField(kmeans.getPredictionCol());
            <span class="predefined-type">System</span>.out.printf(<span class="string"><span class="delimiter">&quot;</span><span class="content">Features: %s </span><span class="char">\t</span><span class="content">Cluster ID: %s</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, features, clusterId);
        }</code></pre></section>
<section id="aufgabe_8_einfache_textklassifikation"><h2>Aufgabe 8 : Einfache Textklassifikation</h2><div class="ulist"><ul><li><p>Wir möchten einen (primitiven) Klassifikator erstellen, der einen gegebenen Text als String einliest und eine Kategorie ausgibt</p></li><li><p>Input ist eine Tabelle mit einer Spalte (String) für den Text und einer weiteren möglichen Spalte für ein Label</p></li></ul></div>
<div class="olist arabic"><ol class="arabic" start="1"><li><p>Bauen Sie dazu mit Hilfe von Flink eine Pipeline, die folgende Schritte enthält:</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>ein Tokenizer um aus den Texten String Arrays zu machen</p></li><li><p>eine HashingTF, um aus einem String Array einen Featurevektor konstanter Länge zu extrahieren</p><div class="ulist"><ul><li><p>die Features beschreiben, welche Wörter wie oft vorkommen</p></li></ul></div></li><li><p>ein Knn Estimator, der die Featurevektoren zusammen mit den Labels benutzt</p></li></ol></div></li><li><p>Wenden Sie Ihre Pipeline auf ein paar einfache Strings als Trainings- und Testdaten an</p></li></ol></div></section>
<section id="aufgabe_8_einfache_textklassifikation_hinweise"><h2>Aufgabe 8 : Einfache Textklassifikation (Hinweise)</h2><div class="ulist"><ul><li><p>Die HashingTF ordnet den Wörtern Hashwerte (ganze nichtnegative Zahlen in einem festgelegten Bereich) zu, und gibt als Featurewert an, wie viele Wörter im Text vorkommen,
deren Hashwert der Nummer des Features entspricht</p><div class="ulist"><ul><li><p>zur Verwendung siehe z.B. <a href="https://nightlies.apache.org/flink/flink-ml-docs-master/docs/operators/feature/hashingtf/" class="bare">https://nightlies.apache.org/flink/flink-ml-docs-master/docs/operators/feature/hashingtf/</a></p></li><li><p>Wählen Sie die Anzahl der Features in HashingTF genügend groß, damit unterschiedliche Wörter möglichst nicht gleich gehasht werden</p></li></ul></div></li><li><p>Da Tables in der Table API eine variable Anzahl von Spalten haben können, kann die Pipeline erst mit einer zweispaltigen Tabelle (Text, Labels) als Transformer ausgeführt
werden, um ein PipelineModel erstellen, und dann dieses Model mit einer einspaltigen Tabelle (nur Text) als Transformer verwendet werden</p></li><li><p>Die Namen der Input- und Outputspalten in der Konfiguration der Komponenten der Pipeline müssen aufeinander abgestimmt sein</p></li><li><p>Der Einfachheit halber können Sie in Knn den Wert 1 für K wählen, dann wird jedem Testfeaturevektor das Label des nächstgelegenden Trainingsfeaturevektors zugeordnet</p></li></ul></div></section>
<section id="betrieb"><h2>Betrieb</h2><div class="paragraph heading center"><p>Betrieb eines Flink-Clusters</p></div></section>
<section id="cluster_setups"><h2>Cluster Setups</h2><div class="ulist"><ul><li><p>Mögliche Setups für einen Flinkcluster:</p><div class="ulist"><ul><li><p>Non managed:</p><div class="ulist"><ul><li><p>Standalone ohne Docker</p></li><li><p>Standalone mit Docker</p></li></ul></div></li><li><p>Managed:</p><div class="ulist"><ul><li><p>YARN (Hadoop Cluster)</p></li><li><p>Kubernetes (nativ/nicht-nativ)</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="setup_standalone_cluster_ohne_docker_skizze"><h2>Setup Standalone Cluster (ohne Docker), Skizze</h2><div class="ulist"><ul><li><p>Die Flink-Distribution muss sich auf allen Nodes im Cluster auf dem gleichen Pfad befinden</p></li><li><p>In der Konfiguration sollte Hostname und Port des JobManagers eingestellt sein (<em>jobmanager.rpc.address</em>, <em>jobmanager.rpc.port</em>)</p><div class="ulist"><ul><li><p>In High-Availability Setups wird statt dessen zuerst die Verbindung zu dem verwendeten High-Availability-Service (z.B. Zookeeper) hergestellt</p></li></ul></div></li><li><p>Auf der Node für den JobManager kann dann start-cluster.sh ausgeführt werden, und auf jeder Node manuell die gewünschte Anzahl von TaskManagers
gestartet werden</p></li><li><p>falls benötigt, kann noch die externe Rest-Schnittstelle konfiguriert werden</p></li></ul></div></section>
<section id="setup_standalone_cluster_mit_docker_skizze"><h2>Setup Standalone Cluster (mit Docker), Skizze</h2><div class="ulist"><ul><li><p>Im Setup mit Docker läuft der JobManager und alle TaskManager je in einem Container</p><div class="ulist"><ul><li><p>Als Image kann "flink" (DockerHub) verwendet werden, z.B. flink:latest</p></li></ul></div></li><li><p>Die Konfiguration kann beim Start des Containers über die Umgebungsvariable FLINK_PROPERTIES mitgegeben werden</p></li><li><p>wie vorher sollte diese zumindest die Netzwerkadresse des JobManagers enthalten</p></li><li><p>Beispiel für Start eines JobManagers:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="script language-script">$ docker run \
    --rm \
    --name=jobmanager \
    --network flink-network \
    --publish 8081:8081 \
    --env FLINK_PROPERTIES=&quot;${FLINK_PROPERTIES}&quot; \
    flink:latest jobmanager</code></pre>
<div class="ulist"><ul><li><p>Komplette Anleitung : <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/</a></p></li><li><p>Was sind Vorteile/Nachteile bei der Verwendung von Docker?</p></li></ul></div></section>
<section id="setup_mit_kubernetes_skizze"><h2>Setup mit Kubernetes, Skizze</h2><div class="ulist"><ul><li><p>Ein Flink-Cluster kann auf einem Kubernetes-Cluster ("on top") deployed werden, indem manuell geeignete Deployment-Skripte für die
JobManager- und TaskManager-Services festgelegt werden (nicht-nativer Setup)</p></li><li><p>Auf der Seite <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/</a> befinden
sich Beispielskripte dafür</p></li><li><p>Im nativen Setup wird der Flink-Cluster stärker mit dem Kubernetes-Cluster verzahnt, sodass Flink z.B. direkt TaskManagers allokieren und deallokieren kann (Stichwort "Flink Kubernetes Operator")</p></li><li><p>Anleitung zum Start im nativen Setup aus der Flink-Dokumentation:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="shell language-shell"># (1) Start Kubernetes session
$ ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster

# (2) Submit example job
$ ./bin/flink run \
    --target kubernetes-session \
    -Dkubernetes.cluster-id=my-first-flink-cluster \
    ./examples/streaming/TopSpeedWindowing.jar

# (3) Stop Kubernetes session by deleting cluster deployment
$ kubectl delete deployment/my-first-flink-cluster</code></pre></section>
<section id="konfigurationsparameter"><h2>Konfigurationsparameter</h2><div class="ulist"><ul><li><p>Clusterweite Werte für die Konfigurationsparameter können in <em>conf/flink-conf.yaml</em> im Flink-Verzeichnis festgelegt werden</p><div class="ulist"><ul><li><p>Das Verzeichnis lässt sich über die Umgebungsvariable FLINK_CONF_DIR anpassen</p></li></ul></div></li><li><p>Für die Ausführung eines Jobs spezifische Parameter können auch durch Methoden auf dem ExecutionEnvironment festgelegt werden:</p><div class="ulist"><ul><li><p>Runtime Mode (s.o.), Parallelismus, Max. Parallelismus (= Anzahl möglicher Key Groups), Restart-Strategie, State Backend, Buffer Timeout ..</p></li></ul></div></li></ul></div></section>
<section id="logging"><h2>Logging</h2><div class="ulist"><ul><li><p>Es kann jedes Logging-Framework verwendet werden, das SLF4J unterstützt</p><div class="ulist"><ul><li><p>Default ist Log4J2</p><div class="ulist"><ul><li><p>Die Log4J2-Konfiguration der Anwendung beeinflusst dann das gesamte Logging für die Ausführung des Jobs</p></li></ul></div></li></ul></div></li><li><p>Die Logs werden in einem lokalen Verzeichnis gespeichert und können auch in der WebUI eingesehen werden</p></li><li><p>Best Practice: Nur Ausnahmen wie Fehler loggen</p></li></ul></div></section>
<section id="restart_strategien_für_anwendungen"><h2>Restart-Strategien für Anwendungen</h2><div class="ulist"><ul><li><p>Wenn eine Anwendung abstürzt, versucht Flink diese ggf. neuzustarten</p></li><li><p>um diese Funktionalität zu aktivieren, muss der Parameter <em>restart-strategy.type</em> gesetzt sein (bzw. eine Strategie der Streams API konfiguriert werden)</p></li><li><p>Mögliche Strategien:</p><div class="ulist"><ul><li><p>Fixed Delay: Es wird versucht, die Anwendung in regelmäßigen Abständen neuzustarten, bis zu einer gewissen maximalen Anzahl von Versuchen</p></li><li><p>Failure Rate Restart: Startet die Anwendung neu, sofern eine bestimmte Failure Rate (Anzahl von Crashs in einem gewissen Zeitintervall) nicht überschritten wurde</p></li><li><p>No Restarts (default)</p></li></ul></div></li></ul></div></section>
<section id="security"><h2>Security</h2><div class="ulist"><ul><li><p>Flink kann alle Netzwerkkommunikation mit SSL verschlüsseln</p><div class="ulist"><ul><li><p>Cluster-Intern verwendet Flink für alle Netzwerkkommunikation gegenseitige SSL-Authentifizierung</p></li></ul></div></li><li><p>Für externe Kommunikation unterstützt Flink auch das Kerberos-Protokoll</p><div class="ulist"><ul><li><p>Dies kann nur für bestimmte Konnektoren verwendet werden (Kafka, HDFS, HBase)</p></li></ul></div></li><li><p>Da alle externe Kommunikation (z.B. Anfragen, Jobs starten) über eine REST-Schnittstelle läuft, kann diese auch mit SSL arbeiten</p></li><li><p>Oder man kann manuell einen Proxy-Service für die Authentifizierung an der Schnittstelle einrichten</p><div class="ulist"><ul><li><p>Im managed Cluster kann dieser die Aufgabe übernehmen</p></li></ul></div></li></ul></div></section>
<section id="state_backend_konfiguration"><h2>State Backend Konfiguration</h2><div class="ulist"><ul><li><p>wichtigste Parameter sind</p><div class="ulist"><ul><li><p><em>state.backend.type</em> (out-of-the-box möglich: hashmap, rocksdb)</p></li><li><p><em>state.checkpoint-storage</em> (jobmanager oder filesystem),</p></li><li><p><em>state.backend.local-recovery</em> : wenn true (default: false), wird der State eines Operators zusätzlich im lokalen Dateisystem der Node gespeichert</p><div class="ulist"><ul><li><p>Bei Recovery wird zunächst das lokale Dateisystem abgefragt, dann das externe Backend</p></li><li><p>Kann Recovery beschleunigen, aber verlangsamt den Betrieb, wenn keine Fehler auftreten</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="speicherkonfiguration"><h2>Speicherkonfiguration</h2><div class="ulist"><ul><li><p>Ein eher technisches Betriebsthema, das nicht Teil dieser Einführung ist, aber Details können in der Flink-Dokumentation gefunden werden, z.B.:</p><div class="ulist"><ul><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/</a></p></li><li><p><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/</a></p></li></ul></div></li></ul></div></section>
<section id="slot_sharing_groups"><h2>Slot Sharing Groups</h2><div class="ulist"><ul><li><p>per Default wird bei der Ausführung eines Jobs jeder Slice genau einem Task Slot eines TaskManagers zugeordnet</p></li><li><p>Durch Festlegen von Task Groups (synonym: Slot Sharing Groups) kann ein Slice bzw. eine Pipeline in mehrere Abschnitte unterteilt werden, die dann jeweils einen
eigenen Task Slot belegen</p><div class="ulist"><ul><li><p>Dies kann bei komplexen Pipelines Sinn ergeben, wenn die Resourcen eines Task Slots zu beschränkt sind</p><div class="ulist"><ul><li><p>Alternativ kann auch die Anzahl der Task Slots pro Task Manager oder die Anzahl der Task Manager auf einer Node, oder die zugewiesenen Resourcen für einen TaskManager, oder
der Paralellismus des Jobs manipuliert werden&#8230;&#8203;</p></li></ul></div></li></ul></div></li><li><p>Um die Task Group für einen Operator in der Fluent API festzulegen, kann nach Anwendung des Operators die Methode slotSharingGroup(&lt;group_name&gt;) ausgeführt werden</p><div class="ulist"><ul><li><p>Downstream Operatoren erben dann diese Gruppe</p></li></ul></div></li></ul></div></section>
<section id="einstellungen_für_checkpoints"><h2>Einstellungen für Checkpoints</h2><div class="ulist"><ul><li><p>Es ist zu empfehlen, Checkpointing zu aktivieren (ist per Default nicht aktiv)</p></li><li><p>Hierfür kann der Konfigurationsparameter <em>execution.checkpointing.interval</em> auf einen Wert größer 0 gesetzt werden, oder die Methode enableCheckpointing(interval) auf dem ExecutionEnvironment
ausgeführt werden</p></li><li><p>Weitere mögliche Konfigurationen (jeweils mit Parametern oder Java API möglich):</p><div class="ulist"><ul><li><p>Checkpointing Mode : Exactly Once (default) oder At least Once</p></li><li><p>minimum time between checkpoints : Mindestwartezeit zwischen Abschluss eines Checkpoints und Beginn des nächsten (muss kleiner als Checkpoint Interval sein)</p></li><li><p>tolerable checkpoint failure number : Wenn mehr als diese Anzahl von Malen hintereinander ein Checkpoint fehlschlägt, breche Job ab</p><div class="ulist"><ul><li><p>default ist 0 : Breche bei jedem Checkpoint failure ab</p></li><li><p>nur bestimmte Arten von failures können toleriert werden (siehe <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/</a> )</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="einstellungen_für_checkpoints_2"><h2>Einstellungen für Checkpoints (2)</h2><div class="ulist"><ul><li><p>number of concurrent checkpoints : Anzahl von Checkpoints, die parallel durchgeführt werden dürfen (default : 1)</p></li><li><p>unaligned checkpoints : wenn true, benutze unaligned checkpointing (inkompatibel mit concurrent checkpoints)</p></li><li><p>externalized checkpoints : steuert ob Checkpoints zusätzlich im externen Speicher persistiert werden (default : nein)</p><div class="ulist"><ul><li><p>Checkpoint Storage-Typ kann mit <em>state.checkpoint-storage</em> kontrolliert werden</p></li><li><p>Job kann dann auch nach cancel manuell vom letzten Checkpoint geladen werden</p></li><li><p>ersetzt nicht Savepoints, da weniger flexibel</p></li></ul></div></li><li><p>Checkpoint Compression kann mit executionConfig.setUseSnapshotCompression(true) aktiviert werden (ExecutionConfig aus Environment mit env.getConfig())</p></li></ul></div></section>
<section id="savepoints_konsolenbefehle"><h2>Savepoints : Konsolenbefehle</h2><div class="ulist"><ul><li><p>Kommandozeilenbefehle für Savepoints:</p><div class="ulist"><ul><li><p>./bin/flink savepoint [--type [native/canonical]] &lt;jobId&gt; [savepointPath]</p><div class="ulist"><ul><li><p>erstellt für den Job mit der angegebenen ID einen Savepoint am angegebenen Ordner (wenn leer, nehme als Default den Wert des Parameters <em>savepoints.dir</em>)</p></li><li><p>Default type ist canonical (s.u.)</p></li><li><p>nach Abschluss des Savepoints wird auf der Konsole ein Pfad zu der entsprechenden Datei ausgebenen</p></li></ul></div></li><li><p>./bin/flink savepoint -d &lt;savepointPath&gt;</p><div class="ulist"><ul><li><p>löscht Savepoint mit dem angegebenen Pfad</p></li></ul></div></li><li><p>./bin/flink run -s &lt;savepointPath&gt; [options] &lt;jobJar&gt; [arguments]</p><div class="ulist"><ul><li><p>Startet Job vom angegebenen Savepoint mit ggf. weiteren Optionen und Kommandozeilenargumenten für die Anwendung</p></li></ul></div></li><li><p>./bin/flink stop --type [native/canonical] --savepointPath [savepointPath] &lt;jobId&gt;</p><div class="ulist"><ul><li><p>erstellt Savepoint und stoppt Job direkt danach</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="savepoints_einschränkungen_und_tipps"><h2>Savepoints : Einschränkungen und Tipps</h2><div class="ulist"><ul><li><p>Wichtig : Für das Funktionieren von sowohl Checkpoints als auch Savepoints sollten alle stateful Operatoren einen ID haben, damit die State-Informationen zugeordnet werden können</p><div class="ulist"><ul><li><p>eine ID kann in der API mit der Methode .uid(&lt;id_string&gt;) hinzugefügt werden</p></li><li><p>andernfalls werden Operatoren über ihre Position im JobGraph identifiziert (nicht robust bei Veränderungen)</p></li></ul></div></li><li><p>eine aufschlussreiche Übersicht, welche Arten von Veränderungen im Job von (aligned/unaligned) Checkpoints bzw. Savepoints toleriert werden können, findet sich hier:
<a href="https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/</a></p><div class="ulist"><ul><li><p>kanonische Savepoints (default) verwenden eine generetische Formatierung, die nicht vom State Backend abhängt, und ermöglichen den Wechsel des State Backend Typs</p></li><li><p>native Savepoints sind an eine spezielle Art von Backend gebunden und können schneller sein</p></li></ul></div></li></ul></div></section>
<section id="savepoints_einschränkungen_und_tipps_2"><h2>Savepoints : Einschränkungen und Tipps (2)</h2><div class="ulist"><ul><li><p>Es gibt leichte Einschränkungen, wie Operator State geändert werden darf, sodass die Anwendung noch kompatibel mit einem Savepoint ist:</p><div class="ulist"><ul><li><p>Bei Hinzufügen von State gibt es keine Probleme (Zustand wird als leer initialisiert)</p></li><li><p>Bei Entfernen von State wird die Anwendung per Default nicht gestartet, da es Inkompatibilitäten geben könnte</p><div class="ulist"><ul><li><p>Dieses Verhalten kann mit der Option "-n" übergangen werden</p></li></ul></div></li><li><p>Schema Evolution : Bei Veränderung des Datentyps eines States kann es sein, dass die Anwendung inkompatibel mit dem Savepoint wird</p><div class="ulist"><ul><li><p>wenn der Serialisierer des Statetyps Schema Evolution unterstützt (z.B. POJO- oder Avro-Typen), wird aber Kompatibilität garantiert</p></li></ul></div></li></ul></div></li><li><p>Um Exactly-Once bei Updates oder Rescaling zu garantieren, sollten Savepoints über den stop-Befehl erstellt werden (s.o.)</p><div class="ulist"><ul><li><p>Sonst könnten Daten, die zwischen Erstellung des Savepoints und Stop des Jobs eintreffen, zweimal bearbeitet werden</p></li></ul></div></li></ul></div></section>
<section id="monitoring"><h2>Monitoring</h2><div class="ulist"><ul><li><p>Es gibt einige eingebaute Metriken, die von Flink automatisch erhoben werden, z.B. :</p></li><li><p>JobManager/TaskManager Scope:</p><div class="ulist"><ul><li><p>Speicherbenutzung von TaskManagern und JobManager</p></li></ul></div></li><li><p>Job Scope:</p><div class="ulist"><ul><li><p>Anzahl der Restarts eines Jobs (pro Zeit)</p></li><li><p>Anzahl der (erfolgreichen/nicht erfolgreichen) Checkpoints</p></li></ul></div></li><li><p>Operator Scope:</p><div class="ulist"><ul><li><p>Anzahl der gesendeten/empfangenen Datensätze pro Sekunde (Throughput)</p></li><li><p>aktuelle Watermark</p></li><li><p>Latenz von Quellenoperatoren zu diesem Operator</p><div class="ulist"><ul><li><p>ist nicht per Default aktiviert (Latency Tracking)</p></li><li><p>wird durch Hinzufügen spezieller Datensätze an der Quelle approximativ berechnet</p></li></ul></div></li></ul></div></li><li><p>&#8230;&#8203; u.v.m.</p></li></ul></div></section>
<section id="monitoring_2"><h2>Monitoring (2)</h2><div class="ulist"><ul><li><p>In der Flink Web UI können sämtliche Metriken eingesehen werden (eigene Metriken ggf. mit "Add Metric" in der Jobanzeige hinzufügen)</p></li><li><p>Eigene Metriken können in der Java API über das MetricGroup Interface registriert werden</p><div class="ulist"><ul><li><p>Man erhält eine MetricGroup über die getMetricGroup()-Methode innerhalb einer Operatorfunktion auf einem RuntimeContext (erfordert RichFunction)</p></li></ul></div></li><li><p>Es können die folgenden Typen von Metriken definiert werden:</p><div class="ulist"><ul><li><p>Counter : können einen Zahlenwert inkrementieren oder dekrementieren</p></li><li><p>Gauge : können den Wert einer einzelnen Variable (beliebigen Typs) aktualisieren</p></li><li><p>Meter : zählt die Anzahl von Ereignissen pro Sekunde</p></li><li><p>Histogram : misst die Verteilung von Werten</p></li></ul></div></li><li><p>Man kann innerhalb der RichFunction eine <em>transient</em> Variable deklarieren, die den Wert der Metrik enthält</p></li></ul></div></section>
<section id="monitoring_3"><h2>Monitoring (3)</h2><div class="ulist"><ul><li><p>Codebeispiel Counter:</p></li></ul></div>
<pre class="CodeRay listingblock"><code class="java language-java"><span class="directive">public</span> <span class="type">class</span> <span class="class">MyMapper</span> <span class="directive">extends</span> RichMapFunction&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; {
  <span class="directive">private</span> <span class="directive">transient</span> Counter counter;

  <span class="annotation">@Override</span>
  <span class="directive">public</span> <span class="type">void</span> open(<span class="predefined-type">Configuration</span> config) {
    <span class="local-variable">this</span>.counter = getRuntimeContext()
      .getMetricGroup()
      .counter(<span class="string"><span class="delimiter">&quot;</span><span class="content">myCustomCounter</span><span class="delimiter">&quot;</span></span>, <span class="keyword">new</span> CustomCounter());
  }

  <span class="annotation">@Override</span>
  <span class="directive">public</span> <span class="predefined-type">String</span> map(<span class="predefined-type">String</span> value) <span class="directive">throws</span> <span class="exception">Exception</span> {
    <span class="local-variable">this</span>.counter.inc();
    <span class="keyword">return</span> value;
  }
}</code></pre>
<div class="paragraph"><p>Codequelle: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/</a></p></div></section>
<section id="monitoring_4"><h2>Monitoring (4)</h2><div class="ulist"><ul><li><p>Jede Metric hat einen Scope, der sich aus einem User Scope und einem System Scope zusammensetzt, wobei der User Scope in der Anwendung konfigurierbar ist</p><div class="ulist"><ul><li><p>bei Ausführung von addGroup(&lt;name&gt;) auf einer vorhandenen MetricGroup wird eine neue Metric Group mit geschachteltem Scope erstellt</p></li></ul></div></li><li><p>System Scope enthält z.B. Informationen über den Job und Task, zu denen die Metrik gehört</p></li><li><p>Scope zusammen mit dem Namen der Metric identifizieren diese eindeutig</p></li><li><p>Um die Metriken extern verfügbar zu machen, müssen sog. Reporter verwendet werden</p><div class="ulist"><ul><li><p>Es gibt einige vordefinierte Implementierungen wie z.B. PrometheusReporter, Slf4jReporter, JMXReporter</p></li></ul></div></li><li><p>Die verwendeten Reporter müssen in <em>flink-conf.yaml</em> (implementierungsabhängig) konfiguriert werden</p><div class="ulist"><ul><li><p>Für Details siehe <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/metric_reporters/" class="bare">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/metric_reporters/</a></p></li></ul></div></li></ul></div></section>
<section id="aufgabe_9_checkpoints_und_recovery"><h2>Aufgabe 9 : Checkpoints und Recovery</h2><div class="ulist"><ul><li><p>Wir lassen in dieser Aufgabe einen Job mit verschiedenen Checkpointing-Strategien laufen und beobachten die automatische Recovery-Funktion von Flink</p><div class="olist arabic"><ol class="arabic"><li><p>Für die Aufgabe ist in den Unterlagen eine docker-compose.yml zu finden, die Sie mit "docker compose up --build -d" starten können</p><div class="ulist"><ul><li><p>Dies startet ein Docker-Netzwerk mit einem Jobmanager und 2 Taskmanagern auf verschiedenen Containern</p></li></ul></div></li><li><p>Führen Sie in einem Konsolenfenster "docker compose logs -f" aus und beobachten Sie im Folgenden die Logausgaben</p></li><li><p>Es ist ein kleiner Job vorbereitet, den Sie starten können, indem Sie z.B. auf den Jobmanager mit "docker exec -it aufgabe-09-jobmanager-1 /bin/bash" wechseln und dann
dort "./bin/flink run counting-job.jar" ausführen</p></li></ol></div></li><li><p>Der Job erhöht einmal pro Sekunde einen Zähler und schreibt die Zahl in eine Datei</p><div class="ulist"><ul><li><p>Die Datei ist auf einen Ordner Ihres lokales Dateisystem gemounted (/aufgabe9-joblog)</p></li></ul></div></li><li><p>Es gibt aber das Problem, dass der No-Op-Operator, der mit Parallelismus 2 arbeitet, für die Verarbeitung von geraden Zahlen deutlich länger braucht als für die ungeraden Zahlen</p></li></ul></div></section>
<section id="aufgabe_9_checkpoints_und_recovery_2"><h2>Aufgabe 9 : Checkpoints und Recovery (2)</h2><div class="olist arabic"><ol class="arabic" start="4"><li><p>Sehen Sie im Log nach, dass in regelmäßigen Abständen Checkpoints erstellt werden und wie viel Zeit dafür benötigt wird</p></li><li><p>Stellen Sie fest, welcher der beiden TaskManager den Job bearbeitet</p></li><li><p>Simulieren Sie einen unerwarteten Ausfall, indem Sie in diesem Taskmanager (z.B. erst "docker exec -it aufgabe-09-taskmanager-1 /bin/bash") den Konsolenbefehl "kill 1" ausführen (beendet Hauptprozess)</p></li><li><p>Überprüfen Sie durch Betrachten der Logs und der Ausgabe des Jobs in der angelegten Datei, ob der andere TaskManager den Job übernommen hat, und nach Laden eines Checkpoints die Konsistenzgarantie
"exactly once" eingehalten wurde</p><div class="ulist"><ul><li><p>Der Job counting-job.jar kann auch mit den Parametern "at-least-once-checkpointing" oder "exactly-once-checkpointing unaligned" ausgeführt werden</p></li></ul></div></li><li><p>Wiederholen Sie in beiden Fällen die gleichen Schritte wie im ersten Fall und vergleichen Sie die Ergebnisse</p></li><li><p>Testen Sie auch noch, manuell einen Savepoint des Jobs zu erstellen und wieder zu laden</p></li></ol></div></section></div></div><script src="reveal.js-3.9.2/lib/js/head.min.js"></script><script src="reveal.js-3.9.2/js/reveal.js"></script><script>// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display controls in the bottom right corner
  controls: true,
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: true,
  // Push each slide change to the browser history
  history: true,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Enable slide navigation via mouse wheel
  mouseWheel: true,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  previewLinks: false,
  // Theme (e.g., beige, black, league, night, serif, simple, sky, solarized, white)
  // NOTE setting the theme in the config no longer works in reveal.js 3.x
  //theme: Reveal.getQueryHash().theme || 'anderscore',
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: Reveal.getQueryHash().transition || 'linear',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1728,
  height: 972,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js-3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'reveal.js-3.9.2/plugin/title-footer/title-footer.js', async: true, callback: function()
          {title_footer.initialize('Schulung Java Data Pipelines mit Apache Flink', 'Jan Lühr', 'anderScore GmbH • Frankenwerft 35 • 50667 Köln');}},
      { src: 'reveal.js-3.9.2/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js-3.9.2/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      
      { src: 'reveal.js-3.9.2/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: 'reveal.js-3.9.2/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
});</script></body></html>